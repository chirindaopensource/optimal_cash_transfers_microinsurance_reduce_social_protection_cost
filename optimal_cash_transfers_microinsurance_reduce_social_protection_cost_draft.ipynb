{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTILbthObJS8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **`README.md`**\n",
        "\n",
        "# **Optimal Social Protection: An Implementation**\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2511.07431-b31b1b.svg)](https://arxiv.org/abs/2511.07431)\n",
        "[![DOI](https://img.shields.io/badge/DOI-10.1257/aer.20251107-gray.svg)](https://www.aeaweb.org/journals/aer)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Actuarial%20Science-00529B)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Development%20Economics-00529B)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Data Source](https://img.shields.io/badge/Data%20Source-Synthetic-003299)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Core Method](https://img.shields.io/badge/Method-Stochastic%20Optimal%20Control-orange)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Analysis](https://img.shields.io/badge/Analysis-Monte%20Carlo%20Simulation-red)](https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type%20checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025577.svg?style=flat&logo=SciPy&logoColor=white)](https://scipy.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-gray?style=flat)](https://pyyaml.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs\"** by:\n",
        "\n",
        "*   Pablo Azcue\n",
        "*   Corina Constantinescu\n",
        "*   José Miguel Flores-Contró\n",
        "*   Nora Muler\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for replicating the paper's findings. It delivers a modular, auditable, and extensible pipeline that executes the entire research workflow: from rigorous configuration validation to core computation (via both analytical and Monte Carlo methods), robustness analysis, and final artifact generation.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: `run_complete_study`](#key-callable-run_complete_study)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the stochastic optimal control framework presented in Azcue et al. (2025). The core of this repository is the iPython Notebook `optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings. The pipeline is designed as a robust and scalable system for determining the cost-minimizing cash transfer policy for a government or NGO, with and without the presence of microinsurance.\n",
        "\n",
        "The paper's central contribution is to frame social protection policy design as a continuous-time optimal control problem and to solve it using both analytical and numerical methods. This codebase operationalizes the paper's framework, allowing users to:\n",
        "-   Rigorously define and validate an entire economic scenario via a single `config.yaml` file.\n",
        "-   Compute the optimal cash transfer policy (the optimal threshold `y*`) that minimizes the expected discounted cost of government interventions.\n",
        "-   Evaluate the cost of this optimal policy and compare it against key baseline policies (e.g., \"inject-to-poverty-line\" and \"perpetual transfers\").\n",
        "-   Analyze the impact of microinsurance (Proportional, Excess-of-Loss, and Total-Loss) on both the optimal policy and the associated government costs.\n",
        "-   Run a complete, end-to-end analysis with a single function call.\n",
        "-   Perform a full suite of sensitivity and robustness analyses to test the model's behavior under different parameterizations.\n",
        "-   Generate a complete, reproducible audit trail for every analysis run.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in stochastic processes, optimal control theory, and actuarial science.\n",
        "\n",
        "**1. Household Capital as a PDMP:**\n",
        "The household's capital, $X_t$, is modeled as a Piecewise-Deterministic Markov Process (PDMP).\n",
        "-   **Deterministic Flow:** Between shocks, capital grows according to the ODE:\n",
        "    $$\n",
        "    dX_t = r[X_t - x^*]^+ dt\n",
        "    $$\n",
        "    where $x^*$ is the poverty line and $r$ is the net growth rate.\n",
        "-   **Stochastic Jumps:** Catastrophic shocks arrive according to a Poisson process with rate $\\lambda$. At a shock time $\\tau_i$, the capital is reduced multiplicatively: $X_{\\tau_i} = X_{\\tau_i^-} \\cdot Z_i$, where $Z_i$ is the remaining capital proportion.\n",
        "\n",
        "**2. Stochastic Optimal Control:**\n",
        "The government's problem is to choose a cumulative transfer process, $S_t$, to minimize the total expected discounted cost, subject to keeping the household's capital above the poverty line. The value function is:\n",
        "$$\n",
        "V(x) = \\inf_{\\pi} \\mathbb{E}_x \\left[ \\int_{0^-}^\\infty e^{-\\delta t} dS_t \\right]\n",
        "$$\n",
        "This problem is characterized by a Hamilton-Jacobi-Bellman (HJB) equation. The paper shows that the optimal policy is a **threshold strategy**: intervene only when capital drops to a specific optimal threshold $y^* \\ge x^*$, and inject just enough to restore it to $y^*$.\n",
        "\n",
        "**3. Solution Methods:**\n",
        "-   **Analytical Solution:** For the specific case where the shock distribution is Beta($\\alpha$, 1) and there is no insurance, the value function $V_y(x)$ can be expressed in closed form using the Gaussian hypergeometric function, ${}_2F_1$.\n",
        "-   **Monte Carlo Simulation:** For the general case (any shock distribution, with or without insurance), the value function is estimated using Monte Carlo simulation. The value at the threshold, $V_y(y)$, is found by solving a renewal equation:\n",
        "    $$\n",
        "    \\hat{V}^{\\pi_y}(y) \\approx \\frac{\\mathbb{E}[J_y e^{-\\delta \\tau_y}]}{1 - \\mathbb{E}[e^{-\\delta \\tau_y}]}\n",
        "    $$\n",
        "    where $(\\tau_y, J_y)$ are the first-passage time and injection amount for paths starting at $y$.\n",
        "\n",
        "**4. Microinsurance Modeling:**\n",
        "Microinsurance is modeled as a transformation of the underlying PDMP. It reduces the growth rate ($r \\to r^R$) and increases the poverty line ($x^* \\to x^{*R}$) due to premium payments, but it favorably alters the shock distribution ($Z \\to W$). The premium $p_R$ is calculated using the expected value principle:\n",
        "$$\n",
        "p_R = (1 + \\gamma)\\lambda \\mathbb{E}[1 - Z - R(1-Z)]\n",
        "$$\n",
        "where $R(\\cdot)$ is the retained loss function for the specific policy type.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Multi-Task Architecture:** The entire pipeline is broken down into 28 distinct, modular tasks, each with its own orchestrator function.\n",
        "-   **Configuration-Driven Design:** All model parameters, computational settings, and analysis stages are controlled by an external `config.yaml` file.\n",
        "-   **Dual-Method Engine:** Seamlessly switches between the high-speed analytical closed-form solver and the general-purpose, high-fidelity Monte Carlo engine based on the configuration.\n",
        "-   **Production-Grade Numerics:** Implements best practices for numerical computation, including robust optimization (Brent's method), accurate numerical integration (`scipy.quad`), stable special function evaluation (`scipy.hyp2f1`), and vectorized operations with `NumPy`.\n",
        "-   **Comprehensive Validation Suite:** Includes a full suite of validation and verification checks, from initial parameter validation to end-to-end sanity checks on the final results (monotonicity, boundary consistency, reproducibility).\n",
        "-   **Rigorous Uncertainty Quantification:** The Monte Carlo engine uses the method of batch means with the t-distribution to provide statistically sound confidence intervals for all estimates.\n",
        "-   **Complete Replication and Robustness:** A single top-level function call can execute the entire study, including a comprehensive suite of sensitivity analyses and convergence diagnostics.\n",
        "-   **Full Provenance:** The pipeline generates a complete, human-readable JSON manifest for each run, containing all inputs, derived parameters, diagnostics, and results for full reproducibility.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Setup (Tasks 1-3):** Ingests, validates, and cleanses the `config.yaml` file into a safe, typed, and immutable object.\n",
        "2.  **Parameter Derivation (Tasks 4-6):** Computes all derived quantities for the base and insured models, and selects the active parameters for the run.\n",
        "3.  **Stochastic Engine (Tasks 7-8):** Constructs and validates the random number samplers and the core PDMP path simulation kernel.\n",
        "4.  **Estimators (Tasks 9-13):** Implements the Monte Carlo and closed-form evaluators for the value functions $V_y(x)$ and $C(x)$.\n",
        "5.  **Optimization (Task 15):** Implements the one-dimensional optimization to find the optimal threshold $y^*$.\n",
        "6.  **Evaluation (Tasks 14, 16, 24):** Computes the final value function curves for the optimal policy and all baseline comparators ($C(x)$, $D(x)$) across a grid.\n",
        "7.  **Robustness & Verification (Tasks 17, 18, 21-23, 25, 28):** Executes the full suite of sensitivity analyses, convergence diagnostics, and final validation checks.\n",
        "8.  **Reporting (Tasks 26-27):** Generates all plots and the final run manifest.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 28 major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: `run_complete_study`\n",
        "\n",
        "The project is designed around a single, top-level user-facing interface function:\n",
        "\n",
        "-   **`run_complete_study`:** This master orchestrator function, located in the final section of the notebook, runs the entire automated research pipeline from end-to-end. A single call to this function reproduces the entire computational portion of the project, controlled by the `analysis_stages` block in the configuration file.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `numpy`, `scipy`, `matplotlib`, `pyyaml`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost.git\n",
        "    cd optimal_cash_transfers_microinsurance_reduce_social_protection_cost\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install numpy scipy matplotlib pyyaml\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The entire pipeline is controlled by a single `config.yaml` file. The structure of this file is detailed in the notebook and the provided example `config.yaml`. It includes sections for metadata, model parameters, computational settings, and run control.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb` notebook provides a complete, self-contained example. The primary workflow is to execute the final cell of the notebook, which demonstrates how to use the top-level `run_complete_study` orchestrator:\n",
        "\n",
        "```python\n",
        "# Final cell of the notebook\n",
        "\n",
        "# This block serves as the main entry point for the entire project.\n",
        "if __name__ == '__main__':\n",
        "    # 1. Define the path to the configuration file.\n",
        "    CONFIG_PATH = \"config.yaml\"\n",
        "    \n",
        "    # 2. Load the configuration from the YAML file into a Python dictionary.\n",
        "    try:\n",
        "        with open(CONFIG_PATH, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        print(\"--- Configuration loaded successfully. ---\")\n",
        "    except Exception as e:\n",
        "        print(f\"--- ERROR: Failed to load configuration. --- \\n{e}\")\n",
        "        # Exit or handle error\n",
        "    \n",
        "    # 3. Execute the entire study.\n",
        "    # The `analysis_stages` block within the config file controls which\n",
        "    # optional analyses (e.g., convergence diagnostics, plots) are run.\n",
        "    if 'config' in locals():\n",
        "        master_results = run_complete_study(full_study_configuration=config)\n",
        "    \n",
        "        # 4. Inspect and save final artifacts.\n",
        "        # For example, save the main plot and the run manifest.\n",
        "        if \"plots\" in master_results:\n",
        "            main_plot = master_results[\"plots\"][\"main_value_function_plot\"]\n",
        "            main_plot.savefig(\"final_value_functions.pdf\")\n",
        "            print(\"\\n--- Main plot saved to 'final_value_functions.pdf' ---\")\n",
        "        \n",
        "        if \"run_manifest_json\" in master_results:\n",
        "            manifest_str = master_results[\"run_manifest_json\"]\n",
        "            with open(\"run_manifest.json\", \"w\") as f:\n",
        "                f.write(manifest_str)\n",
        "            print(\"--- Run manifest saved to 'run_manifest.json' ---\")\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_complete_study` function returns a dictionary containing all generated artifacts. If saved, the primary outputs are:\n",
        "-   **`run_manifest.json`**: A complete JSON file containing all inputs, derived parameters, diagnostics, and numerical results for the run.\n",
        "-   **`*.pdf` / `*.png`**: Plot files generated by the visualization stage.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "optimal_cash_transfers_microinsurance_reduce_social_protection_cost/\n",
        "│\n",
        "├── optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb\n",
        "├── config.yaml\n",
        "├── requirements.txt\n",
        "├── LICENSE\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can modify all study parameters, including economic assumptions, shock distributions, insurance policies, and computational settings, without altering the core Python code. New shock distributions or insurance types can be added by extending the relevant helper functions (e.g., `_create_z_sampler`, `_compute_insurance_premium`).\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "-   **Multi-dimensional Sweeps:** Extend the `run_parameter_sweep` and `plot_sensitivity_sweep` functions to handle and visualize 2D parameter sweeps (e.g., generating a heatmap of `y*` as a function of `λ` and `δ`).\n",
        "-   **Parallelization:** The `run_parameter_sweep` and `_generate_first_passage_samples` functions are embarrassingly parallel. An extension could use `joblib` or `multiprocessing` to significantly accelerate large-scale analyses.\n",
        "-   **Additional Distributions:** Add support for other shock distributions (e.g., Lognormal, Pareto) by implementing their samplers and required properties (PDF, CDF, mean).\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{azcue2025optimal,\n",
        "  title={Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs},\n",
        "  author={Azcue, Pablo and Constantinescu, Corina and Flores-Contr{\\'o}, Jos{\\'e} Miguel and Muler, Nora},\n",
        "  journal={arXiv preprint arXiv:2511.07431},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). An Implementation of \"Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs\".\n",
        "GitHub repository: https://github.com/chirindaopensource/optimal_cash_transfers_microinsurance_reduce_social_protection_cost\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Pablo Azcue, Corina Constantinescu, José Miguel Flores-Contró, and Nora Muler** for the foundational research that forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **NumPy, SciPy, and Matplotlib**.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of the `optimal_cash_transfers_microinsurance_reduce_social_protection_cost_draft.ipynb` notebook and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "dbDlRtKaXpAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs*\"\n",
        "\n",
        "Authors: Pablo Azcue, Corina Constantinescu, José Miguel Flores-Contró, Nora Muler\n",
        "\n",
        "E-Journal Submission Date: 30 October 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2511.07431\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Design and implementation of appropriate social protection strategies is one of the main targets of the United Nation's Sustainable Development Goal (SDG) 1: No Poverty. Cash transfer (CT) programmes are considered one of the main social protection strategies and an instrument for achieving SDG 1. Targeting consists of establishing eligibility criteria for beneficiaries of CT programmes. In low-income countries, where resources are limited, proper targeting of CTs is essential for an efficient use of resources. Given the growing importance of microinsurance as a complementary tool to social protection strategies, this study examines its role as a supplement to CT programmes. In this article, we adopt the piecewise-deterministic Markov process introduced in Kovacevic and Pflug (2011) to model the capital of a household, which when exposed to large proportional capital losses (in contrast to the classical Cramér-Lundberg model) can push them into the poverty area. Striving for cost-effective CT programmes, we optimise the expected discounted cost of keeping the household's capital above the poverty line by means of injection of capital (as a direct capital transfer). Using dynamic programming techniques, we derive the Hamilton-Jacobi-Bellman (HJB) equation associated with the optimal control problem of determining the amount of capital to inject over time. We show that this equation admits a viscosity solution that can be approximated numerically. Moreover, in certain special cases, we obtain closed-form expressions for the solution. Numerical examples show that there is an optimal level of injection above the poverty threshold, suggesting that efficient use of resources is achieved when CTs are preventive rather than reactive, since injecting capital into households when their capital levels are above the poverty line is less costly than to do so only when it falls below the threshold."
      ],
      "metadata": {
        "id": "LD8kCIBMbYXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### **Summary of \"Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs\"**\n",
        "\n",
        "#### **The Core Problem and Objective**\n",
        "\n",
        "The paper addresses a fundamental question in social protection policy: **How can a government or NGO design a cash transfer (CT) program to prevent households from falling into poverty in the most cost-effective way?**\n",
        "\n",
        "The authors frame this not as a static policy choice but as a dynamic **stochastic optimal control problem**. The objective is to minimize the total expected discounted cost of all future capital injections (cash transfers) required to keep a household's capital level perpetually above a predefined poverty line. Their central thesis is that a *preventive* strategy (acting before a household is officially in poverty) is more efficient than a purely *reactive* one.\n",
        "\n",
        "#### **The Mathematical Model of Household Capital**\n",
        "\n",
        "The foundation of their analysis is the **piecewise-deterministic Markov process (PDMP)** for household capital, originally proposed by Kovacevic and Pflug (2011). This model captures two key dynamics:\n",
        "\n",
        "1.  **Deterministic Growth (Between Shocks):** A household's capital, `Xt`, evolves according to the differential equation:\n",
        "    `dXt = r[Xt - x*]+ dt`\n",
        "    *   `x*` is the **poverty line** or critical capital level.\n",
        "    *   If capital `Xt` is above `x*`, the household saves a portion of its income, leading to exponential capital growth at a rate `r`.\n",
        "    *   If capital `Xt` is at or below `x*`, all income is consumed for subsistence, and capital does not grow (`[Xt - x*]+ = 0`). This non-linearity is crucial, as it creates a \"poverty trap\" dynamic.\n",
        "\n",
        "2.  **Stochastic Shocks (Catastrophic Losses):** The household is subject to sudden, large capital losses (e.g., from natural disasters, health emergencies). These events are modeled as:\n",
        "    *   Arrivals of a **Poisson process** with intensity `λ`.\n",
        "    *   At each event, the capital is multiplied by a random factor `Zi` (where `0 < Zi < 1`), representing the *proportion of capital remaining* after the loss.\n",
        "\n",
        "This model provides a mathematically tractable yet realistic framework for a household's financial vulnerability.\n",
        "\n",
        "#### **Formulating the Optimal Control Problem**\n",
        "\n",
        "The government's action is the injection of capital. The control variable, `π = (St)`, represents the cumulative amount of cash transfers up to time `t`. The goal is to find the optimal strategy `π*` that minimizes the **value function** `V(x)`, defined as the infimum of the expected discounted cost:\n",
        "\n",
        "`V(x) = inf E [ ∫₀^∞ e^(-δt) dSt ]`\n",
        "\n",
        "This minimization is subject to the critical constraint that the controlled capital process, `Xπt`, must remain at or above the poverty line `x*` for all time `t`.\n",
        "\n",
        "#### **Deriving and Analyzing the Hamilton-Jacobi-Bellman (HJB) Equation**\n",
        "\n",
        "Using the principles of dynamic programming, the authors associate the value function `V(x)` with its corresponding **Hamilton-Jacobi-Bellman (HJB) equation**. This is a first-order integro-differential equation that characterizes the optimal policy:\n",
        "\n",
        "`min{1 + u'(x), L(u)(x)} = 0`\n",
        "\n",
        "*   `L(u)(x)` is the **infinitesimal generator** of the capital process, which describes its expected change over an infinitesimal time interval, accounting for both deterministic growth and the possibility of a stochastic jump (loss).\n",
        "*   `1 + u'(x) = 0`: This part of the equation defines the **action region**. It implies that the marginal cost of injecting capital is 1, so it is optimal to act.\n",
        "*   `L(u)(x) = 0`: This defines the **inaction region**, where it is optimal for the government to wait and let the household's capital evolve on its own.\n",
        "\n",
        "The authors prove that the value function `V(x)` is the unique **viscosity solution** to this HJB equation, a standard and powerful concept in modern control theory for handling problems where classical smooth solutions may not exist.\n",
        "\n",
        "#### **The Central Result – Optimal Threshold Strategies**\n",
        "\n",
        "The solution to the HJB equation reveals the structure of the optimal policy. It is a **threshold strategy**.\n",
        "\n",
        "*   A simple, reactive strategy would be to wait until a household's capital hits `x*` and then inject just enough to bring it back to `x*`.\n",
        "*   The paper demonstrates that the optimal strategy is often defined by a threshold `y*` which is **strictly greater than the poverty line `x*`**.\n",
        "*   **The Optimal Policy:** Do nothing as long as the household's capital `Xt` is above `y*`. If a shock pushes the capital below `y*`, immediately inject just enough capital to restore it to the level `y*`.\n",
        "\n",
        "This is the paper's key economic insight: it is cheaper in the long run to maintain a \"buffer\" of capital above the poverty line rather than performing emergency interventions at the brink of poverty. The buffer allows the household to leverage its own capital growth (`r > 0`), reducing the future burden on the government.\n",
        "\n",
        "#### **Closed-Form Solutions and Numerical Analysis**\n",
        "\n",
        "To make the problem concrete, the authors analyze a special case where the remaining capital proportion `Zi` follows a **Beta(a, 1) distribution**. This specific choice makes the integral term in the HJB equation solvable, leading to a **closed-form solution** for the value function (expressed using hypergeometric functions).\n",
        "\n",
        "With this analytical solution, they can:\n",
        "1.  Calculate the cost `Vy(x)` for any given threshold `y`.\n",
        "2.  Numerically differentiate `Vy(x)` with respect to `y` to find the optimal threshold `y*` that minimizes the cost.\n",
        "3.  Perform sensitivity analysis to show how `y*` and the minimum cost `V(x)` change with key parameters like the discount rate (`δ`), loss frequency (`λ`), and loss severity (`a`).\n",
        "\n",
        "For more general loss distributions where closed-form solutions are unavailable, they demonstrate the use of **Monte Carlo simulation** to accurately estimate the value function and the optimal threshold.\n",
        "\n",
        "#### **Incorporating Microinsurance as a Complementary Tool**\n",
        "\n",
        "The paper extends the analysis by introducing microinsurance. This is not a control variable for the government but rather a feature of the environment that modifies the underlying capital process. They analyze three standard types of coverage: **Proportional, Excess-of-Loss (XL), and Total-Loss**.\n",
        "\n",
        "Microinsurance changes the problem in three ways:\n",
        "1.  **Reduced Growth:** Premiums reduce the household's disposable income, lowering the capital growth rate `r`.\n",
        "2.  **Higher Effective Poverty Line:** The need to pay premiums increases the income required for subsistence, effectively raising the poverty line `x*`.\n",
        "3.  **Mitigated Losses:** The insurance payout reduces the severity of catastrophic shocks, changing the distribution of the random variable `Zi`.\n",
        "\n",
        "The analysis shows that despite the cost of premiums (which can increase the need for transfers in some states), microinsurance **substantially reduces the government's overall expected cost**. By capping the worst-case losses, it makes the system more stable and predictable, ultimately lowering the fiscal burden of the CT program.\n",
        "\n",
        "#### **Conclusion and Policy Implications**\n",
        "\n",
        "The paper concludes with powerful, model-driven policy recommendations:\n",
        "1.  **CTs should be preventive, not just reactive.** Establishing an optimal injection threshold above the poverty line is more cost-effective.\n",
        "2.  **Microinsurance and cash transfers are complementary, not substitutes.** Integrating microinsurance into a social safety net can significantly reduce the long-term cost of direct government aid by transferring catastrophic risk to the private market.\n",
        "\n",
        "The authors rightly note the limitations, such as the model's theoretical nature and the absence of behavioral responses (e.g., moral hazard). However, the paper provides a rigorous and compelling quantitative framework for thinking about the optimal design of modern social protection systems."
      ],
      "metadata": {
        "id": "DtciDrtoeFIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "0tuJG-pFVBRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "#\n",
        "#  Optimal Control Framework for Social Protection Policy Design\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Optimal Cash Transfers and Microinsurance\n",
        "#  to Reduce Social Protection Costs\" by Azcue, Constantinescu, Flores-Contró,\n",
        "#  and Muler (2025). It delivers a quantitative decision-support system for\n",
        "#  designing, budgeting, and optimizing dynamic social safety net policies under\n",
        "#  uncertainty, enabling policymakers to move from static, reactive aid to\n",
        "#  proactive, cost-effective poverty prevention strategies.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Piecewise-Deterministic Markov Process (PDMP) for household capital dynamics.\n",
        "#  • Stochastic optimal control formulation to minimize discounted government costs.\n",
        "#  • Hamilton-Jacobi-Bellman (HJB) framework for characterizing the value function.\n",
        "#  • Analytical closed-form solutions for the Beta(α, 1) loss distribution case.\n",
        "#  • High-performance Monte Carlo simulation engine for general-case solutions.\n",
        "#  • One-dimensional numerical optimization to find the optimal transfer threshold (y*).\n",
        "#  • Modeling of microinsurance (Proportional, XL, Total-Loss) as a complementary tool.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Robust, configuration-driven pipeline from data ingestion to final analysis.\n",
        "#  • High-fidelity random variate samplers using inverse transform sampling.\n",
        "#  • Professional-grade numerical methods (Brent's method, adaptive quadrature).\n",
        "#  • Implementation of special functions (Gaussian Hypergeometric ₂F₁).\n",
        "#  • Comprehensive diagnostic suite for convergence and cross-validation.\n",
        "#  • Automated generation of a complete, reproducible run manifest.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Azcue, P., Constantinescu, C., Flores-Contró, J. M., & Muler, N. (2025).\n",
        "#  Optimal Cash Transfers and Microinsurance to Reduce Social Protection Costs.\n",
        "#  arXiv preprint arXiv:2511.07431.\n",
        "#  https://arxiv.org/abs/2511.07431\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Fused Imports for the Complete Social Protection Cost Pipeline\n",
        "# ==============================================================================\n",
        "# This block contains all necessary imports to run the entire constellation of\n",
        "# functions developed for the \"Optimal Cash Transfers and Microinsurance\" study.\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import copy\n",
        "import dataclasses\n",
        "import datetime\n",
        "import functools\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import sys\n",
        "import warnings\n",
        "from dataclasses import dataclass, field\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Optional,\n",
        "    Set,\n",
        "    Tuple,\n",
        "    Type,\n",
        "    TypeVar,\n",
        "    Union,\n",
        ")\n",
        "\n",
        "# --- Third-party Library Imports ---\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.figure import Figure\n",
        "from matplotlib.patches import Patch\n",
        "from scipy.optimize import OptimizeResult, minimize_scalar\n",
        "from scipy.special import beta as beta_function\n",
        "from scipy.special import hyp2f1\n",
        "from scipy.stats import linregress\n",
        "import scipy.integrate\n",
        "import scipy.stats\n",
        "\n",
        "# Generic TypeVar for utility functions.\n",
        "T = TypeVar('T')\n"
      ],
      "metadata": {
        "id": "zUqEKWceVF7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "3Y8Faw2iVJlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### **Explication of Final Orchestrator Callables**\n",
        "\n",
        "#### **1. `ingest_and_parse_config` (Task 1)**\n",
        "\n",
        "*   **Inputs:** A raw, nested Python dictionary (`Dict[str, Any]`) representing the `full_study_configuration`.\n",
        "*   **Processes:**\n",
        "    1.  **Structural Validation:** It first calls `_validate_top_level_structure` to confirm the presence of all mandatory top-level keys (e.g., `model_parameters`, `computation_parameters`).\n",
        "    2.  **Recursive Parsing:** It then calls the remediated `_parse_config_to_dataclasses` orchestrator, which systematically traverses the dictionary. Using the `_safe_get_and_cast` helper, it extracts each value, performs an explicit type cast (e.g., to `float`, `int`, `str`), and populates a corresponding field in a hierarchy of nested, immutable dataclasses.\n",
        "*   **Outputs:** A single, top-level `ParsedStudyConfig` dataclass instance.\n",
        "*   **Data Transformation:** This function transforms an untyped, mutable, and potentially unsafe dictionary into a strongly-typed, immutable, and validated object hierarchy. This is a structural and type-safety transformation.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **secure entry point** for the entire pipeline. It implements the crucial first step of data ingestion and validation, ensuring that all subsequent computations are performed on a data structure that is guaranteed to be complete, correctly typed, and consistent with the required schema. It does not directly implement a mathematical equation but is a foundational step in computational best practices for reproducible research.\n",
        "\n",
        "\n",
        "\n",
        "#### **2. `validate_parameters` (Task 2)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig` object from Task 1 and the original `raw_config` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Primitive Validation:** It calls `_validate_economic_primitives` to check that core parameters adhere to their theoretical domains (e.g., $x^* > 0$, $\\lambda > 0$, $0 < a < 1$).\n",
        "    2.  **Distribution Validation:** It calls `_validate_loss_distribution` to verify that the parameters of the shock distribution $G_Z$ are valid and that the pre-computed mean $\\mu$ is consistent with the theoretical mean (e.g., $\\mu = \\alpha / (\\alpha + \\beta)$ for a Beta distribution).\n",
        "    3.  **Insurance Validation:** It calls the remediated `_validate_microinsurance_params`. If insurance is active, this helper re-computes the premium $p_R$ from first principles, using numerical integration where necessary, and enforces the critical feasibility constraint from Section 7: $b > p_R$.\n",
        "*   **Outputs:** None. The function returns `None` if all checks pass or raises a `ValueError` or `NotImplementedError` upon the first failure.\n",
        "*   **Data Transformation:** This function performs no data transformation. It is a pure validation function that acts as a quality gate.\n",
        "*   **Role in Research Pipeline:** This callable implements the **mathematical and economic sanity checks** on the model's parameters. It ensures the configuration corresponds to a well-posed problem. Its most critical role is verifying the feasibility constraint for microinsurance, which is essential for the model's stability:\n",
        "    $$\n",
        "    b > p_R \\quad \\text{where} \\quad p_R = (1 + \\gamma)\\lambda \\mathbb{E}[1 - Z - R(1-Z)]\n",
        "    $$\n",
        "    A failure to meet this condition would lead to a non-positive denominator in the calculation of the transformed poverty line, $x^{*R} = \\left(\\frac{b}{b - p_R}\\right) x^*$, rendering the model meaningless.\n",
        "\n",
        "\n",
        "\n",
        "#### **3. `cleanse_and_normalize_config` (Task 3)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig` object.\n",
        "*   **Processes:**\n",
        "    1.  **Normalization:** It calls `_normalize_enumerations` to convert string identifiers like `insurance_type` and `algorithm_to_use` into their canonical forms (e.g., `\"proportional\"` becomes `\"Proportional\"`).\n",
        "    2.  **Pruning:** It calls `_prune_unused_policy_fields` to set the parameters of inactive insurance policies to `None`, preventing their accidental use.\n",
        "*   **Outputs:** A new, cleansed `ParsedStudyConfig` object.\n",
        "*   **Data Transformation:** This function performs a structural and value-based transformation. It standardizes key string values and nullifies irrelevant parts of the configuration object graph to enforce logical consistency.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **data sanitization** step. It ensures that all downstream logic can rely on a single, canonical representation for categorical parameters, which simplifies conditional checks and makes the entire codebase more robust.\n",
        "\n",
        "\n",
        "\n",
        "#### **4. `compute_base_derived_quantities` (Task 4)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig` object and the `raw_config` dictionary.\n",
        "*   **Processes:**\n",
        "    1.  **Compute Growth Rate:** It calls `_compute_capital_growth_rate` to calculate the definitive value of $r$ from its primitives: $r = (1-a)bc$.\n",
        "    2.  **Compute Shock Mean:** It calls `_compute_expected_remaining_capital` to calculate the definitive value of $\\mu = \\mathbb{E}[Z]$ from the distribution's parameters.\n",
        "    3.  **Compute Boundary:** It calls `_compute_policy_comparator_boundary` to evaluate the diagnostic from Proposition 2.2.\n",
        "*   **Outputs:** A `BaseDerivedQuantities` object containing the computed `r`, `μ`, and the boundary diagnostic.\n",
        "*   **Data Transformation:** This function transforms the primitive model parameters into the first set of essential derived quantities for the baseline (uninsured) model.\n",
        "*   **Role in Research Pipeline:** This callable implements the **computation of fundamental model parameters**. It establishes the definitive, high-precision values of $r$ and $\\mu$ that will be used throughout the rest of the analysis, overriding any potentially stale values from the configuration file. It also implements the check from **Proposition 2.2**:\n",
        "    $$\n",
        "    D(x^*) \\ge C(x^*) \\iff b \\ge \\delta + \\lambda(1-\\mu)\n",
        "    $$\n",
        "    by computing the value $b - (\\delta + \\lambda(1-\\mu))$ and storing its sign.\n",
        "\n",
        "\n",
        "\n",
        "#### **5. `compute_insurance_transforms` (Task 5)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig`, `BaseDerivedQuantities`, and a sampler for the base distribution `Z`.\n",
        "*   **Processes:**\n",
        "    1.  **Conditional Execution:** It first checks if insurance is active. If not, it returns `None`.\n",
        "    2.  **Compute Premium:** It calls `_compute_insurance_premium` to calculate $p_R$.\n",
        "    3.  **Compute Transformed Primitives:** It calls `_compute_transformed_primitives` to calculate the new poverty line $x^{*R}$ and growth rate $r^R$.\n",
        "    4.  **Define New Law:** It calls `_compute_post_insurance_law` to compute the mean of the new shock distribution, $\\mathbb{E}[W]$, and to create a new sampler function for `W`.\n",
        "*   **Outputs:** An `InsuranceTransforms` object containing all the new parameters and the new sampler, or `None`.\n",
        "*   **Data Transformation:** This function transforms the baseline model parameters into a new set of \"active\" parameters that describe the insured model.\n",
        "*   **Role in Research Pipeline:** This callable implements the **parameter transformation for the insured scenario** as described in Section 7. It computes the key quantities that redefine the PDMP:\n",
        "    $$\n",
        "    p_R = (1 + \\gamma)\\lambda \\mathbb{E}[1 - Z - R(1-Z)]\n",
        "    $$\n",
        "    $$\n",
        "    x^{*R} = \\left(\\frac{b}{b - p_R}\\right) x^*\n",
        "    $$\n",
        "    $$\n",
        "    r^R = r\\left(\\frac{b - p_R}{b}\\right)\n",
        "    $$\n",
        "    It also defines the new shock random variable $W = 1 - R(1-Z)$ by providing its mean and a function to sample from it.\n",
        "\n",
        "\n",
        "\n",
        "#### **6. `select_method_and_define_active_parameters` (Task 6)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig`, `BaseDerivedQuantities`, and the optional `InsuranceTransforms` object.\n",
        "*   **Processes:**\n",
        "    1.  **Select Method:** It calls `_select_computation_method` to determine the final algorithm to use (e.g., `\"ClosedForm\"` or `\"MonteCarlo\"`) based on eligibility and user choice.\n",
        "    2.  **Set Active Parameters:** It selects either the base parameters (`x*`, `r`) or the transformed parameters (`x*^R`, `r^R`) based on whether insurance is active.\n",
        "    3.  **Define Optimizer Bracket:** It calculates the search interval `[y_min, y_max]` for the subsequent optimization of `y`.\n",
        "*   **Outputs:** An `ActiveParameters` object containing the final, unambiguous set of parameters for the core computational engine.\n",
        "*   **Data Transformation:** This function is a crucial control point that consolidates all conditional logic. It transforms the full configuration and derived parameters into a small, definitive set of \"active\" parameters that will govern the next stage of the pipeline.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **final setup and control** step before the main computation begins. It makes the strategic decision on which numerical method to use and provides the core simulation and optimization functions with their exact, scenario-specific parameters, abstracting away the complexity of the insured vs. uninsured cases.\n",
        "\n",
        "\n",
        "\n",
        "#### **7. `construct_active_sampler` (Task 7)**\n",
        "\n",
        "*   **Inputs:** The `ParsedStudyConfig`, `BaseDerivedQuantities`, and the optional `InsuranceTransforms` object.\n",
        "*   **Processes:**\n",
        "    1.  **Create RNG:** It initializes a seeded `numpy.random.Generator`.\n",
        "    2.  **Build Base Sampler:** It calls `_create_z_sampler` to create a function that can generate random variates from the base distribution `Z`.\n",
        "    3.  **Select Active Sampler:** If insurance is active, it selects the `post_insurance_loss_sampler` (for `W`) from the `InsuranceTransforms` object. Otherwise, it selects the base `z_sampler`.\n",
        "    4.  **Validate:** It calls `_validate_sampler` to perform a statistical sanity check on the selected active sampler, comparing its empirical mean to the known theoretical mean.\n",
        "*   **Outputs:** A single callable function, the active sampler.\n",
        "*   **Data Transformation:** This function is a factory. It transforms the distributional parameters from the configuration into a concrete, validated computational tool (a function) for generating random numbers.\n",
        "*   **Role in Research Pipeline:** This callable implements the **stochastic engine's core component**. It provides the validated random number generator for the shocks that drive the PDMP. Its use of inverse transform sampling is a direct implementation of the standard method for generating random variates:\n",
        "    $$\n",
        "    Z = F_Z^{-1}(U), \\quad U \\sim \\text{Unif}[0,1]\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **8. `simulate_single_pdmp_path` (Task 8)**\n",
        "\n",
        "*   **Inputs:** An initial capital, a threshold `y`, and all the active parameters, samplers, and RNGs.\n",
        "*   **Processes:** It executes a `while` loop that simulates one full sample path of the PDMP until termination. Each step in the loop involves:\n",
        "    1.  Drawing an inter-arrival time $\\Delta t \\sim \\text{Exp}(\\lambda)$.\n",
        "    2.  Checking for time horizon truncation.\n",
        "    3.  Applying the deterministic flow for duration $\\Delta t$.\n",
        "    4.  Drawing a shock `z` (or `w`) from the active sampler.\n",
        "    5.  Applying the multiplicative jump.\n",
        "    6.  Checking if the new capital has dropped below the threshold `y`.\n",
        "*   **Outputs:** A tuple containing the first-passage time `τ_y` and the required injection amount `J_y`.\n",
        "*   **Data Transformation:** This function transforms the model's parameters into a single realization of the stochastic process's outcome of interest.\n",
        "*   **Role in Research Pipeline:** This callable is the **heart of the Monte Carlo engine**. It is a direct and faithful implementation of the Piecewise-Deterministic Markov Process described in Section 2.1.\n",
        "\n",
        "\n",
        "\n",
        "#### **9. `estimate_vy_at_threshold` (Task 9)**\n",
        "\n",
        "*   **Inputs:** A threshold `y` and all necessary simulation parameters.\n",
        "*   **Processes:**\n",
        "    1.  It calls `_generate_first_passage_samples` (which in turn calls `simulate_single_pdmp_path` `N` times) with the initial capital set to `y`.\n",
        "    2.  It computes the two key statistics, $S_0$ and $S_1$.\n",
        "    3.  It applies the final estimator formula.\n",
        "*   **Outputs:** A single float, the point estimate of $V_y(y)$.\n",
        "*   **Data Transformation:** It transforms `N` simulated path outcomes into a single point estimate of an expected value.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Monte Carlo estimator for the value function at the boundary**, based on the renewal equation argument presented in Section 6.1:\n",
        "    $$\n",
        "    \\hat{V}^{\\pi_y}(y) \\approx \\frac{\\frac{1}{N}\\sum_{i=1}^N J_{y,i} e^{-\\delta \\tau^y_i}}{1 - \\frac{1}{N}\\sum_{i=1}^N e^{-\\delta \\tau^y_i}}\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **10. `estimate_vy_for_continuation_region` (Task 10)**\n",
        "\n",
        "*   **Inputs:** An initial capital `x > y`, the threshold `y`, the pre-computed value `V_y(y)`, and all simulation parameters.\n",
        "*   **Processes:**\n",
        "    1.  It calls `_generate_first_passage_samples` with the initial capital set to `x`.\n",
        "    2.  It applies the continuation estimator formula.\n",
        "*   **Outputs:** A single float, the point estimate of $V_y(x)$.\n",
        "*   **Data Transformation:** It transforms `N` simulated path outcomes and a continuation value into a single point estimate.\n",
        "*   **Role in Research Pipeline:** This callable implements the **Monte Carlo estimator for the value function in the continuation region** ($x>y$), based on the law of total expectation, as discussed near Eq. (6.2):\n",
        "    $$\n",
        "    \\hat{V}^{\\pi_y}(x) \\approx \\frac{1}{N} \\sum_{i=1}^N \\left(J_{y,i} + V^{\\pi_y}(y)\\right) e^{-\\delta \\tau^y_i}\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **11. `with_confidence_interval` (Task 11)**\n",
        "\n",
        "*   **Inputs:** An estimator function, the full set of `N` raw samples, the number of batches `B`, and a confidence level.\n",
        "*   **Processes:**\n",
        "    1.  It calls `_create_batches` to partition the raw samples.\n",
        "    2.  It loops `B` times, calling the provided `estimator_func` on each batch to get `B` batch means.\n",
        "    3.  It computes the overall mean and the sample standard deviation of the batch means.\n",
        "    4.  It uses the t-distribution's percent point function (`scipy.stats.t.ppf`) to find the critical value.\n",
        "    5.  It constructs the confidence interval.\n",
        "*   **Outputs:** A `MonteCarloResult` object containing the point estimate, CI bounds, and truncation fraction.\n",
        "*   **Data Transformation:** This function is a higher-order function that transforms a simple point estimator into a more sophisticated statistical estimator that also quantifies its own uncertainty.\n",
        "*   **Role in Research Pipeline:** This callable implements the **method of batch means**, a standard statistical technique for estimating the variance of a Monte Carlo estimator and constructing a valid confidence interval.\n",
        "\n",
        "\n",
        "\n",
        "#### **12. `evaluate_c_closed_form` (Task 12)**\n",
        "\n",
        "*   **Inputs:** A capital level `x` and the relevant model parameters for the specific Beta($\\alpha$, 1) case.\n",
        "*   **Processes:**\n",
        "    1.  It calls `_compute_hypergeometric_params` to get the parameters `a, b, c`.\n",
        "    2.  It uses `np.where` to apply a piecewise evaluation:\n",
        "        *   If $x \\le x^*$, it applies the linear formula.\n",
        "        *   If $x > x^*$, it applies the formula involving the Gaussian hypergeometric function, `hyp2f1`.\n",
        "*   **Outputs:** The analytical value of $C(x)$.\n",
        "*   **Data Transformation:** It transforms the model parameters and a capital level `x` into the analytical value of the cost function.\n",
        "*   **Role in Research Pipeline:** This callable is a direct and faithful implementation of the **analytical solution for the cost of social protection, $C(x)$**, from **Example 5.1, Equation (5.1)**:\n",
        "    $$\n",
        "    C(x) =\n",
        "    \\begin{cases}\n",
        "    (x^* - x) + \\frac{\\lambda x^*}{(\\alpha + 1)\\delta}, & x \\le x^* \\\\\n",
        "    \\frac{\\lambda x^*}{(\\alpha + 1)\\delta} \\cdot {}_2F_1\\left(b, b-c+1; b-a+1; \\frac{x^*}{x}\\right) \\left(\\frac{x^*}{x}\\right)^b, & x > x^*\n",
        "    \\end{cases}\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **13. `evaluate_vy_closed_form` (Task 13)**\n",
        "\n",
        "*   **Inputs:** A capital level `x`, a threshold `y`, and the relevant model parameters.\n",
        "*   **Processes:**\n",
        "    1.  It calls `_compute_hypergeometric_params`.\n",
        "    2.  It calls the complex helper `_compute_vy_at_y_closed_form` to find the crucial boundary constant $V_y(y)$.\n",
        "    3.  It uses `np.where` to apply a piecewise evaluation:\n",
        "        *   If $x \\le y$, it applies the linear formula $V_y(x) = (y-x) + V_y(y)$.\n",
        "        *   If $x > y$, it applies the robust scaling relationship $V_y(x) = V_y(y) \\cdot \\Psi(x) / \\Psi(y)$, using the `_psi` helper.\n",
        "*   **Outputs:** The analytical value of $V_y(x)$.\n",
        "*   **Data Transformation:** It transforms model parameters, `x`, and `y` into the analytical value of the threshold strategy cost.\n",
        "*   **Role in Research Pipeline:** This callable is a direct and faithful implementation of the **analytical solution for a general threshold strategy, $V_y(x)$**, from **Example 5.2**. The remediated version uses a more robust and theoretically transparent scaling relationship for the continuation region, which is mathematically equivalent to the complex formula presented in the paper.\n",
        "\n",
        "\n",
        "\n",
        "#### **14. `evaluate_d_comparator` (Task 14)**\n",
        "\n",
        "*   **Inputs:** A grid of `x` values and all necessary model and simulation parameters.\n",
        "*   **Processes:**\n",
        "    1.  It partitions the grid into `x <= x*` and `x > x*`.\n",
        "    2.  For the `below` region, it calls the analytical helper `_evaluate_d_below_x_star`.\n",
        "    3.  For the `above` region, it loops through each unique `x`, calls the `_generate_trapping_time_samples` simulator, and computes the recursive Monte Carlo estimate.\n",
        "*   **Outputs:** A NumPy array of $D(x)$ values on the grid.\n",
        "*   **Data Transformation:** It transforms the model parameters into a full curve for the $D(x)$ function.\n",
        "*   **Role in Research Pipeline:** This callable implements the **hybrid evaluation of the perpetual transfers comparator, $D(x)$**, from **Proposition 2.1**. It uses the analytical formula for $x \\le x^*$ and the recursive Monte Carlo definition for $x > x^*$:\n",
        "    $$\n",
        "    D(x) = \\mathbb{E}\\left[D(X_{\\bar{\\tau}}) e^{-\\delta \\bar{\\tau}}\\right], \\quad \\text{for } x > x^*\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **15. `find_optimal_threshold` (Task 15)**\n",
        "\n",
        "*   **Inputs:** All active parameters, configuration objects, samplers, and RNGs.\n",
        "*   **Processes:**\n",
        "    1.  It defines the objective function $f(y) = V_y(y)$, dispatching to either the closed-form or Monte Carlo evaluator. For the Monte Carlo case, it implements Common Random Numbers (CRN).\n",
        "    2.  It calls `scipy.optimize.minimize_scalar` with the bounded Brent method to find the minimum of the objective function.\n",
        "    3.  It performs convergence and boundary checks on the result.\n",
        "*   **Outputs:** An `OptimizationResult` object containing $y^*$ and $V_{y^*}(y^*)$.\n",
        "*   **Data Transformation:** This function transforms the entire model specification into a single optimal policy parameter, $y^*$.\n",
        "*   **Role in Research Pipeline:** This callable is the **core optimization engine**. It numerically solves the main problem of the paper: finding the optimal threshold that minimizes the expected discounted cost.\n",
        "    $$\n",
        "    y^* = \\arg\\min_{y \\ge \\bar{x}^*} V_y(y)\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **16. `evaluate_vy_star_on_grid` (Task 16)**\n",
        "\n",
        "*   **Inputs:** The `OptimizationResult` from Task 15 and all necessary parameters and evaluators.\n",
        "*   **Processes:**\n",
        "    1.  It creates the output grid.\n",
        "    2.  It applies the piecewise evaluation logic for $V_{y^*}(x)$, partitioned at the now-known optimal threshold $y^*$.\n",
        "    3.  For the Monte Carlo branch, it correctly calls the `with_confidence_interval` wrapper and propagates the `truncation_fraction` diagnostic.\n",
        "*   **Outputs:** A `ValueFunctionResult` object containing the full curve for the optimal value function, including CIs and diagnostics.\n",
        "*   **Data Transformation:** It transforms the optimal policy parameter $y^*$ into the full optimal value function curve.\n",
        "*   **Role in Research Pipeline:** This callable **evaluates the final, optimal solution**. It takes the optimal policy, $y^*$, and computes the resulting cost function $V_{y^*}(x)$ for all relevant initial capital levels, producing one of the main outputs of the entire analysis.\n",
        "\n",
        "\n",
        "\n",
        "#### **17. `verify_tail_limits` (Task 17)**\n",
        "\n",
        "*   **Inputs:** A list of `ValueFunctionResult` objects.\n",
        "*   **Processes:** It iterates through the list and calls the `_check_tail_limit` helper for each, which checks if the maximum absolute value in the tail of the grid is below a tolerance.\n",
        "*   **Outputs:** None. It issues `RuntimeWarning`s if checks fail.\n",
        "*   **Data Transformation:** This is a pure validation function.\n",
        "*   **Role in Research Pipeline:** This callable implements a **numerical check of a key theoretical property** from **Lemma 3.1 and Proposition 3.1**:\n",
        "    $$\n",
        "    \\lim_{x\\to\\infty} V(x) = 0 \\quad \\text{and} \\quad \\lim_{x\\to\\infty} C(x) = 0\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **18. `cross_validate_mc_vs_closed_form` (Task 18)**\n",
        "\n",
        "*   **Inputs:** Two `ValueFunctionResult` objects, one from Monte Carlo and one from the closed-form solution.\n",
        "*   **Processes:** It computes a suite of comparison metrics: absolute error, relative error, and confidence interval coverage. It then assesses a pass/fail verdict based on predefined thresholds.\n",
        "*   **Outputs:** A `CrossValidationResult` object.\n",
        "*   **Data Transformation:** It transforms two value function curves into a set of error metrics and a validation verdict.\n",
        "*   **Role in Research Pipeline:** This callable is the **primary validation tool for the entire Monte Carlo engine**. It provides a rigorous, quantitative comparison against a known ground truth, which is the most powerful method for detecting systematic bias or implementation errors in a stochastic simulation framework.\n",
        "\n",
        "\n",
        "\n",
        "#### **19. `solve_vy_fixed_point` (Task 19)**\n",
        "\n",
        "*   **Inputs:** A threshold `y` and all necessary model parameters.\n",
        "*   **Processes:**\n",
        "    1.  It discretizes the domain `[y, X_max]`.\n",
        "    2.  It initializes the value function estimate $W^{(0)}$ to zero.\n",
        "    3.  It iteratively applies the operator $T(W)$ using nested numerical quadrature: $W^{(k+1)} = T(W^{(k)})$.\n",
        "    4.  It checks for convergence using the supremum norm.\n",
        "*   **Outputs:** A `FixedPointResult` object containing the converged value function.\n",
        "*   **Data Transformation:** It transforms the model parameters into a value function curve by finding the fixed point of an operator.\n",
        "*   **Role in Research Pipeline:** This callable implements an **alternative numerical solution method** based on **Proposition 4.3**, which characterizes the value function as the unique fixed point of the operator $T$:\n",
        "    $$\n",
        "    T(W)(x) = \\mathbb{E}\\left[e^{-\\delta \\tau_1} \\left( (y - X_{\\tau_1} + W(y)) \\cdot \\mathbb{I}_{\\{X_{\\tau_1} < y\\}} + W(X_{\\tau_1}) \\cdot \\mathbb{I}_{\\{X_{\\tau_1} \\ge y\\}} \\right) \\right]\n",
        "    $$\n",
        "\n",
        "\n",
        "\n",
        "#### **20. `run_end_to_end_analysis` (Task 20)**\n",
        "\n",
        "*   **Inputs:** The raw `full_study_configuration` dictionary.\n",
        "*   **Processes:** It executes the entire sequence of tasks from 1 to 18 in the correct logical order, managing the flow of data between the various sub-orchestrators.\n",
        "*   **Outputs:** A comprehensive `EndToEndResult` object containing all results and provenance.\n",
        "*   **Data Transformation:** It transforms the single input configuration into the complete set of analytical results.\n",
        "*   **Role in Research Pipeline:** This is the **primary orchestrator for a single analysis run**. It represents the full, end-to-end implementation of the research methodology for a single scenario.\n",
        "\n",
        "\n",
        "\n",
        "#### **21. `run_parameter_sweep` (Task 21)**\n",
        "\n",
        "*   **Inputs:** A base configuration dictionary and a dictionary defining the parameter sweeps.\n",
        "*   **Processes:** It generates all combinations of the sweep parameters. For each combination, it creates a new configuration, calls `run_end_to_end_analysis`, and stores the result, robustly handling any individual failures.\n",
        "*   **Outputs:** A dictionary mapping parameter tuples to `EndToEndResult` objects.\n",
        "*   **Data Transformation:** It transforms a base configuration and a set of parameter ranges into a collection of results across that parameter space.\n",
        "*   **Role in Research Pipeline:** This callable is the engine for **sensitivity and robustness analysis**. It automates the process of re-running the entire study under different assumptions to understand how the results change, which is a critical part of any serious quantitative study.\n",
        "\n",
        "\n",
        "\n",
        "#### **22. `analyze_and_plot_policy_boundary` (Task 22)**\n",
        "\n",
        "*   **Inputs:** The parsed configuration and base derived quantities.\n",
        "*   **Processes:** It creates a 2D grid of $(\\lambda, \\mu)$ values, computes the boundary diagnostic from Proposition 2.2 on this grid, and generates a contour plot showing the preference regions.\n",
        "*   **Outputs:** A `matplotlib.figure.Figure` object.\n",
        "*   **Data Transformation:** It transforms the model's parameters into a visual policy map.\n",
        "*   **Role in Research Pipeline:** This callable provides a **visual representation of the theoretical result from Proposition 2.2**, allowing for an intuitive understanding of how shock frequency and severity influence the choice between lump-sum and perpetual transfer policies.\n",
        "\n",
        "\n",
        "\n",
        "#### **23. `run_convergence_diagnostics` (Task 23)**\n",
        "\n",
        "*   **Inputs:** A base configuration dictionary.\n",
        "*   **Processes:** It runs two parameter sweeps: one over the number of paths `N` and one over the time horizon `T`. It then analyzes the results to check for CI width scaling (for `N`) and truncation bias (for `T`).\n",
        "*   **Outputs:** A `ConvergenceDiagnostics` object containing the results and generated plots.\n",
        "*   **Data Transformation:** It transforms a base configuration into a quantitative and visual report on the stability and convergence of the Monte Carlo engine.\n",
        "*   **Role in Research Pipeline:** This callable is a crucial **diagnostic tool for the Monte Carlo method**. It provides the evidence needed to certify that the chosen simulation parameters (`N` and `T`) are adequate and that the results are not compromised by excessive statistical noise or truncation bias.\n",
        "\n",
        "\n",
        "\n",
        "#### **24. `generate_output_series` (Task 24)**\n",
        "\n",
        "*   **Inputs:** The results of the core computation (`optimization_result`) and all necessary parameters.\n",
        "*   **Processes:** It iterates through the `series_to_plot` list in the configuration and calls the appropriate evaluation engine (the new `evaluate_threshold_strategy` helper or `evaluate_d_comparator`) for each requested series.\n",
        "*   **Outputs:** A dictionary mapping series names to `ValueFunctionResult` objects.\n",
        "*   **Data Transformation:** It transforms the optimal policy and other fixed policies into a complete set of value function curves.\n",
        "*   **Role in Research Pipeline:** This callable is the **central artifact generation engine**. It is responsible for computing all the final curves that will be plotted and reported. Its remediated, modular design ensures all threshold strategies are evaluated with methodological consistency.\n",
        "\n",
        "\n",
        "\n",
        "#### **25. `analyze_insurance_impact` (Task 25)**\n",
        "\n",
        "*   **Inputs:** An insured configuration dictionary.\n",
        "*   **Processes:** It runs the full `run_end_to_end_analysis` pipeline twice: once for the given insured config and once for a programmatically created uninsured counterfactual. It then computes the differences in key outputs.\n",
        "*   **Outputs:** An `InsuranceImpactResult` object containing the comparative metrics.\n",
        "*   **Data Transformation:** It transforms a single insured scenario into a full comparative analysis of the costs and benefits of the insurance policy.\n",
        "*   **Role in Research Pipeline:** This callable directly addresses one of the core research questions of the paper: **quantifying the impact of microinsurance**. It provides the numerical results needed to assess whether and by how much microinsurance reduces the government's social protection costs.\n",
        "\n",
        "\n",
        "\n",
        "#### **26. `plot_value_functions` & `plot_sensitivity_sweep` (Task 26)**\n",
        "\n",
        "*   **Inputs:** The `EndToEndResult` object or the dictionary of sweep results.\n",
        "*   **Processes:** They use `matplotlib` to create professional, publication-quality plots of the computed value functions, including confidence intervals, annotations, and legends.\n",
        "*   **Outputs:** `matplotlib.figure.Figure` objects.\n",
        "*   **Data Transformation:** They transform numerical data arrays into visual representations.\n",
        "*   **Role in Research Pipeline:** These callables are the **primary tools for communicating the results** of the analysis. They translate the complex numerical outputs into intuitive visual formats that align with the figures presented in the research paper.\n",
        "\n",
        "\n",
        "\n",
        "#### **27. `generate_run_manifest` (Task 27)**\n",
        "\n",
        "*   **Inputs:** The `EndToEndResult` object and the original `raw_config`.\n",
        "*   **Processes:** It gathers all inputs, execution metadata, intermediate results, and final outputs. It computes an integrity hash of the inputs and serializes the entire collection into a pretty-printed JSON string.\n",
        "*   **Outputs:** A JSON string.\n",
        "*   **Data Transformation:** It transforms the entire state of a completed pipeline run, including complex Python objects, into a universally readable and persistent text format.\n",
        "*   **Role in Research Pipeline:** This callable is the **cornerstone of reproducibility**. It creates a complete, self-contained, and verifiable audit trail for every single analysis run, which is the highest standard for computational research.\n",
        "\n",
        "\n",
        "\n",
        "#### **28. `run_final_validation_checks` (Task 28)**\n",
        "\n",
        "*   **Inputs:** The `EndToEndResult` object and the `raw_config`.\n",
        "*   **Processes:** It sequentially calls the helper functions to verify monotonicity, boundary consistency, and reproducibility.\n",
        "*   **Outputs:** None. It raises `AssertionError`s if any check fails.\n",
        "*   **Data Transformation:** This is a pure validation function.\n",
        "*   **Role in Research Pipeline:** This callable serves as the **final quality assurance gate**. It performs a suite of end-to-end sanity checks to confirm that the final results are not just computed, but are also consistent with the fundamental theoretical properties of the model. A successful run of this function provides a high degree of confidence in the entire pipeline's correctness.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "### **Example Usage of the End-to-End Pipeline**\n",
        "\n",
        "This example provides a step-by-step guide to using the `run_complete_study` function, which serves as the primary user interface for the entire analytical framework.\n",
        "\n",
        "#### **Step 1: Environment Setup and Configuration Loading**\n",
        "\n",
        "The first step in any analysis is to set up the environment and load the configuration file. We assume all the Python functions and dataclasses we have developed are available in the current session (e.g., in a single script or Jupyter Notebook). We also need to install and import `PyYAML` to parse the configuration file.\n",
        "\n",
        "```python\n",
        "# First, ensure the PyYAML library is installed.\n",
        "# You can run this in your terminal or a notebook cell:\n",
        "# pip install pyyaml\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Define the path to our configuration file.\n",
        "config_path = \"config.yaml\"\n",
        "\n",
        "# Read the YAML file and load its contents into a Python dictionary.\n",
        "# This dictionary, `full_study_configuration`, will be the primary input\n",
        "# for our entire analysis. It contains all model parameters, computational\n",
        "# settings, and run control flags.\n",
        "try:\n",
        "    with open(config_path, 'r') as f:\n",
        "        full_study_configuration = yaml.safe_load(f)\n",
        "    print(\"--- Configuration file 'config.yaml' loaded successfully. ---\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"--- ERROR: Configuration file not found at '{config_path}'. ---\")\n",
        "    # In a real script, you would exit here.\n",
        "    full_study_configuration = None\n",
        "except yaml.YAMLError as e:\n",
        "    print(f\"--- ERROR: Failed to parse YAML file. ---\")\n",
        "    print(e)\n",
        "    full_study_configuration = None\n",
        "\n",
        "# For demonstration, let's inspect a small part of the loaded configuration\n",
        "# to verify it was read correctly.\n",
        "if full_study_configuration:\n",
        "    print(\"\\nSample of loaded configuration:\")\n",
        "    print(f\"  Scenario ID: {full_study_configuration['study_metadata']['scenario_id']}\")\n",
        "    print(f\"  Insurance Active: {full_study_configuration['model_parameters']['microinsurance_parameters']['is_active']}\")\n",
        "    print(f\"  Algorithm Choice: {full_study_configuration['computation_parameters']['method_selection']['algorithm_to_use']}\")\n",
        "```\n",
        "\n",
        "#### **Step 2: Executing the Complete Study**\n",
        "\n",
        "With the configuration loaded, we can now execute the entire analytical pipeline with a single call to the `run_complete_study` master orchestrator. This function will handle everything: validation, parameter computation, optimization, baseline evaluation, robustness checks, and final artifact generation, all driven by the settings in our loaded dictionary.\n",
        "\n",
        "The `run_control` or `analysis_stages` block within the `.yaml` file dictates which optional analyses are performed. For this example, we assume it is configured to run all stages.\n",
        "\n",
        "```python\n",
        "# Check if the configuration was loaded successfully before proceeding.\n",
        "if full_study_configuration:\n",
        "    # Execute the entire end-to-end pipeline.\n",
        "    # This single function call encapsulates the complexity of all 28 tasks.\n",
        "    # It will print progress messages for each major stage of the analysis.\n",
        "    master_results = run_complete_study(full_study_configuration)\n",
        "    \n",
        "    # The `master_results` dictionary now contains all the artifacts from the run.\n",
        "    print(\"\\n--- Pipeline execution finished. Accessing results... ---\")\n",
        "```\n",
        "\n",
        "#### **Step 3: Inspecting the Results**\n",
        "\n",
        "The `master_results` dictionary is the main output. It is a structured container for every piece of information generated during the run, making it easy to inspect the results, debug issues, and generate reports.\n",
        "\n",
        "##### **3.1: Inspecting the Main Analysis Result**\n",
        "\n",
        "The core output is the main analysis, which includes the optimal policy and the corresponding value function.\n",
        "\n",
        "```python\n",
        "if 'main_analysis_result' in master_results:\n",
        "    main_result = master_results['main_analysis_result']\n",
        "    \n",
        "    # Get the key optimization results from the provenance trail.\n",
        "    opt_result = main_result.provenance.optimization_result\n",
        "    y_star = opt_result.optimal_threshold_y_star\n",
        "    v_at_y_star = opt_result.min_value_vy_at_y_star\n",
        "    \n",
        "    print(\"\\n--- Core Optimization Results ---\")\n",
        "    print(f\"  Optimal Transfer Threshold (y*): {y_star:.4f}\")\n",
        "    print(f\"  Minimum Expected Cost (V(y*)): {v_at_y_star:.4f}\")\n",
        "    \n",
        "    # The full optimal value function curve is also available.\n",
        "    optimal_vf_result = main_result.all_value_functions['V_y_star_x_optimal_threshold']\n",
        "    print(f\"\\n  Optimal value function V(x) was computed on a grid of \"\n",
        "          f\"{len(optimal_vf_result.capital_grid)} points.\")\n",
        "    # For example, find the cost for an initial capital of 50.\n",
        "    capital_grid = optimal_vf_result.capital_grid\n",
        "    cost_at_50 = np.interp(50.0, capital_grid, optimal_vf_result.point_estimates)\n",
        "    print(f\"  Estimated cost for an initial capital of x=50: {cost_at_50:.4f}\")\n",
        "```\n",
        "\n",
        "##### **3.2: Inspecting Robustness and Sensitivity Analyses**\n",
        "\n",
        "The results of any optional analyses, like convergence diagnostics or parameter sweeps, are stored under their own keys.\n",
        "\n",
        "```python\n",
        "# Check for and inspect convergence diagnostics results.\n",
        "if 'convergence_diagnostics_result' in master_results:\n",
        "    conv_diag = master_results['convergence_diagnostics_result']\n",
        "    \n",
        "    print(\"\\n--- Convergence Diagnostics Summary ---\")\n",
        "    print(f\"  CI Width vs. N (log-log slope): {conv_diag.ci_scaling_slope:.3f} (Theoretical: -0.5)\")\n",
        "    \n",
        "    # The generated plots are also available as objects.\n",
        "    # You can display them or save them to a file.\n",
        "    # conv_diag.n_convergence_plot.savefig(\"convergence_N_plot.png\")\n",
        "    # conv_diag.t_convergence_plot.savefig(\"convergence_T_plot.png\")\n",
        "    print(\"  Convergence plots have been generated and are available in the result object.\")\n",
        "\n",
        "# Check for and inspect parameter sweep results.\n",
        "if 'parameter_sweep_results' in master_results:\n",
        "    sweep_results = master_results['parameter_sweep_results']\n",
        "    print(\"\\n--- Parameter Sweep Summary ---\")\n",
        "    for sweep_name, results_dict in sweep_results.items():\n",
        "        print(f\"  Sweep '{sweep_name}' completed with {len(results_dict)} successful runs.\")\n",
        "        # Example: Print the optimal y* for each run in the delta sensitivity sweep.\n",
        "        if sweep_name == \"delta_sensitivity_analysis\":\n",
        "            for params, result in results_dict.items():\n",
        "                delta_val = params[0]\n",
        "                y_star = result.provenance.optimization_result.optimal_threshold_y_star\n",
        "                print(f\"    - For δ = {delta_val:.2f}, optimal y* = {y_star:.2f}\")\n",
        "```\n",
        "\n",
        "##### **3.3: Accessing Plots and the Final Manifest**\n",
        "\n",
        "The generated plots and the final JSON manifest are stored directly in the results dictionary.\n",
        "\n",
        "```python\n",
        "# Access the generated plots.\n",
        "if 'plots' in master_results:\n",
        "    print(\"\\n--- Accessing Generated Plots ---\")\n",
        "    main_plot = master_results['plots']['main_value_function_plot']\n",
        "    boundary_plot = master_results['plots']['policy_boundary_plot']\n",
        "    \n",
        "    # In a Jupyter environment, you can simply display the figure.\n",
        "    # display(main_plot)\n",
        "    \n",
        "    # Or save it to a file.\n",
        "    main_plot.savefig(\"optimal_policy_vs_baselines.pdf\")\n",
        "    boundary_plot.savefig(\"policy_preference_boundary.pdf\")\n",
        "    print(\"  Plots have been saved to 'optimal_policy_vs_baselines.pdf' and 'policy_preference_boundary.pdf'.\")\n",
        "\n",
        "# Access and save the final run manifest.\n",
        "if 'run_manifest_json' in master_results:\n",
        "    print(\"\\n--- Accessing Final Run Manifest ---\")\n",
        "    manifest_json_str = master_results['run_manifest_json']\n",
        "    \n",
        "    # Save the manifest to a file for auditing and reproducibility.\n",
        "    manifest_filename = f\"run_manifest_{full_study_configuration['study_metadata']['scenario_id']}.json\"\n",
        "    with open(manifest_filename, 'w') as f:\n",
        "        f.write(manifest_json_str)\n",
        "    print(f\"  Complete audit trail has been saved to '{manifest_filename}'.\")\n",
        "\n",
        "```\n",
        "\n",
        "This concludes the example. It demonstrates a complete workflow: loading a declarative configuration from a file, executing a complex, multi-stage computational pipeline with a single function call, and then accessing the rich, structured results for reporting and further analysis. This represents a professional and reproducible approach to computational research.\n",
        "<br>"
      ],
      "metadata": {
        "id": "5iRJ4S-YVNOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 — Ingest and parse the configuration dictionary\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 1: Ingest and parse the study configuration dictionary\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 1: Helper function to validate top-level structure\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_top_level_structure(\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the presence of all mandatory top-level keys in the configuration.\n",
        "\n",
        "    This function ensures that the provided configuration dictionary contains the\n",
        "    essential high-level sections required for the study's pipeline to operate.\n",
        "    It performs a structural integrity check before any deeper parsing occurs.\n",
        "\n",
        "    Args:\n",
        "        config: The full study configuration dictionary.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the provided input is not a dictionary.\n",
        "        KeyError: If one or more required top-level keys are missing. The\n",
        "                  error message will list all missing keys.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure the configuration is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(\"The study configuration must be a dictionary.\")\n",
        "\n",
        "    # Define the set of mandatory top-level keys for structural validation.\n",
        "    REQUIRED_TOP_LEVEL_KEYS: Set[str] = {\n",
        "        \"study_metadata\",\n",
        "        \"model_parameters\",\n",
        "        \"data_preprocessing_parameters\",\n",
        "        \"computation_parameters\",\n",
        "        \"machine_learning_parameters\",\n",
        "        \"llm_and_generative_ai_parameters\",\n",
        "        \"output_parameters\",\n",
        "        \"methodological_notes\"\n",
        "    }\n",
        "\n",
        "    # Get the set of keys present in the provided configuration.\n",
        "    provided_keys = set(config.keys())\n",
        "\n",
        "    # Check if all required keys are present.\n",
        "    if not REQUIRED_TOP_LEVEL_KEYS.issubset(provided_keys):\n",
        "        # Identify which specific keys are missing for a precise error message.\n",
        "        missing_keys = REQUIRED_TOP_LEVEL_KEYS - provided_keys\n",
        "        # Raise a detailed error, listing all missing keys.\n",
        "        raise KeyError(\n",
        "            \"The study configuration is missing required top-level keys: \"\n",
        "            f\"{sorted(list(missing_keys))}\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2 & 3: Dataclasses for strongly-typed parameter storage\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class StudyMetadata:\n",
        "    \"\"\"\n",
        "    Immutable container for high-level study and scenario metadata.\n",
        "\n",
        "    This class captures descriptive information about the specific simulation\n",
        "    run, ensuring that all outputs can be traced back to a well-defined\n",
        "    scenario. It serves as the provenance record for the computational\n",
        "    experiment, enabling full reproducibility and linking computed results\n",
        "    to research figures and published findings.\n",
        "\n",
        "    The metadata is purely descriptive and does not participate in any\n",
        "    mathematical computations. However, it is essential for organizing\n",
        "    parameter sweeps, managing multiple scenarios, and documenting the\n",
        "    exact configuration that produced a given set of results.\n",
        "\n",
        "    Attributes:\n",
        "        study_name: The descriptive name of the overall research study,\n",
        "                    typically matching the paper title or research project name.\n",
        "        scenario_id: A unique, machine-readable identifier for the specific\n",
        "                     scenario being executed (e.g., \"Figure7a_Proportional_eta_0.80_gamma_0.50\").\n",
        "                     This should uniquely identify the parameter configuration.\n",
        "        description: A human-readable description of the scenario's purpose,\n",
        "                     parameter configuration, or the research question it addresses.\n",
        "        version: The version number of the configuration schema itself, used\n",
        "                 for managing schema updates and ensuring backward compatibility\n",
        "                 across different versions of the research codebase.\n",
        "    \"\"\"\n",
        "    # The descriptive name of the overall research study.\n",
        "    study_name: str\n",
        "\n",
        "    # A unique, machine-readable identifier for this specific scenario.\n",
        "    scenario_id: str\n",
        "\n",
        "    # A human-readable description of the scenario's purpose or configuration.\n",
        "    description: str\n",
        "\n",
        "    # The version number of the configuration schema.\n",
        "    version: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PrimitiveParamsForR:\n",
        "    \"\"\"\n",
        "    Immutable container for primitive economic parameters used to derive growth rate 'r'.\n",
        "\n",
        "    This class holds the three fundamental economic assumptions from which the net\n",
        "    capital growth rate 'r' is calculated according to the household economic model\n",
        "    in Section 2 of the research context. The growth rate formula is:\n",
        "        r = (1 - a) * b * c\n",
        "    where each parameter has a clear economic interpretation.\n",
        "\n",
        "    These primitives represent deep structural assumptions about household behavior:\n",
        "    consumption propensity (a), income generation capacity (b), and the efficiency\n",
        "    of converting savings back into productive capital (c). All three must satisfy\n",
        "    strict positivity and boundedness constraints to ensure a well-posed economic model.\n",
        "\n",
        "    Input validation should ensure: 0 < a < 1, b > 0, c > 0.\n",
        "\n",
        "    Attributes:\n",
        "        marginal_propensity_consume_a: The marginal propensity to consume, 'a'.\n",
        "                                       This parameter governs what fraction of income\n",
        "                                       above the critical level is consumed vs. saved.\n",
        "                                       Economic constraint: 0 < a < 1.\n",
        "        income_generation_rate_b: The rate 'b' at which household capital generates\n",
        "                                  income, representing the productivity of capital.\n",
        "                                  Economic constraint: b > 0.\n",
        "        savings_conversion_rate_c: The rate 'c' at which savings are converted back\n",
        "                                   into productive capital, representing investment\n",
        "                                   efficiency. Economic constraint: c > 0.\n",
        "    \"\"\"\n",
        "    # The marginal propensity to consume 'a'; must be in the interval (0, 1).\n",
        "    marginal_propensity_consume_a: float\n",
        "\n",
        "    # The income generation rate 'b'; must be strictly positive.\n",
        "    income_generation_rate_b: float\n",
        "\n",
        "    # The savings conversion rate 'c'; must be strictly positive.\n",
        "    savings_conversion_rate_c: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class HouseholdEconomicParams:\n",
        "    \"\"\"\n",
        "    Immutable container for all household-level economic parameters.\n",
        "\n",
        "    This class defines the complete economic environment for a household in the\n",
        "    PDMP model described in Section 2 of the research context. It aggregates both\n",
        "    the critical capital threshold (poverty line x*) and the primitives that\n",
        "    determine capital growth dynamics above that threshold.\n",
        "\n",
        "    The poverty line x* is the fundamental policy parameter: households with capital\n",
        "    below x* cannot save (all income is consumed), while those above x* experience\n",
        "    exponential capital growth at rate r. The derived growth rate r is stored in\n",
        "    the configuration for validation purposes, ensuring consistency between the\n",
        "    provided primitives and the expected computational outcome.\n",
        "\n",
        "    Input validation should ensure: x* > 0, and that derived_growth_rate_r matches\n",
        "    the value computed from the primitives within tolerance 1e-10.\n",
        "\n",
        "    Attributes:\n",
        "        poverty_line_x_star: The critical capital level x*, below which households\n",
        "                             cannot save and above which capital grows exponentially.\n",
        "                             This is the fundamental policy threshold. Constraint: x* > 0.\n",
        "        primitive_params_for_r: A nested immutable object containing the three\n",
        "                                primitives (a, b, c) from which the growth rate r\n",
        "                                is derived via r = (1 - a) * b * c.\n",
        "        derived_growth_rate_r: The pre-computed growth rate r = (1 - a) * b * c,\n",
        "                               stored in the configuration for validation purposes.\n",
        "                               This should match the value computed from primitives\n",
        "                               within tolerance 1e-10.\n",
        "    \"\"\"\n",
        "    # The critical capital level x* (poverty line); must be strictly positive.\n",
        "    poverty_line_x_star: float\n",
        "\n",
        "    # A nested immutable object containing the primitives a, b, c.\n",
        "    primitive_params_for_r: PrimitiveParamsForR\n",
        "\n",
        "    # The pre-computed growth rate r = (1 - a) * b * c for validation.\n",
        "    derived_growth_rate_r: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class LossDistributionGZ:\n",
        "    \"\"\"\n",
        "    Immutable container for the loss severity distribution G_Z.\n",
        "\n",
        "    This class specifies the complete probability distribution of Z, the remaining\n",
        "    capital proportion after a catastrophic shock event. In the PDMP model from\n",
        "    Section 2, when a shock occurs at time τ_i, the household's capital is\n",
        "    multiplied by Z_i, where Z_i are i.i.d. random variables from this distribution.\n",
        "\n",
        "    The distribution must be supported on (0, 1), representing the fraction of\n",
        "    capital that remains after the shock. The mean μ = E[Z] is a critical parameter\n",
        "    used in the policy comparator boundary (Proposition 2.2) and in the computation\n",
        "    of insurance premiums (Section 7).\n",
        "\n",
        "    For Beta(α, 1) distributions, the mean is μ = α/(α+1). For Kumaraswamy(p, q)\n",
        "    distributions, the mean involves the gamma function. The stored mean must match\n",
        "    the theoretical mean within tolerance 1e-10.\n",
        "\n",
        "    Input validation should ensure: name is in {\"Beta\", \"Kumaraswamy\", \"Custom\"},\n",
        "    parameters are valid for the specified distribution, and 0 < mean_E_Z_mu < 1.\n",
        "\n",
        "    Attributes:\n",
        "        name: The canonical name of the distribution (e.g., \"Beta\", \"Kumaraswamy\").\n",
        "              This determines which sampling and analytical methods can be applied.\n",
        "        parameters: A dictionary mapping parameter names to values (e.g.,\n",
        "                    {'alpha': 1.25, 'beta': 1.0} for Beta(1.25, 1)). The keys\n",
        "                    and interpretation depend on the distribution name.\n",
        "        mean_E_Z_mu: The pre-computed theoretical mean μ = E[Z], which must lie\n",
        "                     in the interval (0, 1). This is validated against the value\n",
        "                     computed from the distribution parameters.\n",
        "    \"\"\"\n",
        "    # The canonical name of the distribution (e.g., \"Beta\", \"Kumaraswamy\").\n",
        "    name: str\n",
        "\n",
        "    # A dictionary of the distribution's shape parameters (e.g., {'alpha': 1.25, 'beta': 1.0}).\n",
        "    parameters: Dict[str, float]\n",
        "\n",
        "    # The pre-computed theoretical mean μ = E[Z]; must be in the interval (0, 1).\n",
        "    mean_E_Z_mu: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class StochasticShockParams:\n",
        "    \"\"\"\n",
        "    Immutable container for parameters of the stochastic shock process.\n",
        "\n",
        "    This class defines the complete specification of the catastrophic shock process\n",
        "    in the PDMP model from Section 2. Shocks arrive according to a Poisson process\n",
        "    with rate λ, and each shock reduces the household's capital by a random\n",
        "    proportion determined by the loss distribution G_Z.\n",
        "\n",
        "    The shock frequency λ governs how often households face catastrophic events,\n",
        "    while the loss distribution G_Z determines the severity of each event. Together,\n",
        "    these parameters define the stochastic risk environment that the optimal cash\n",
        "    transfer policy must address.\n",
        "\n",
        "    The expected loss rate is λ * (1 - μ), which appears in the policy comparator\n",
        "    boundary from Proposition 2.2 and in the computation of insurance premiums.\n",
        "\n",
        "    Input validation should ensure: λ > 0, and that the nested loss distribution\n",
        "    satisfies all its own constraints.\n",
        "\n",
        "    Attributes:\n",
        "        shock_frequency_lambda: The Poisson arrival rate λ of catastrophic shocks,\n",
        "                                measured in events per unit time. Higher values\n",
        "                                indicate more frequent shocks. Constraint: λ > 0.\n",
        "        loss_distribution_G_Z: A nested immutable object defining the probability\n",
        "                               distribution of the remaining capital proportion Z\n",
        "                               after each shock event.\n",
        "    \"\"\"\n",
        "    # The Poisson arrival rate λ of catastrophic shocks; must be strictly positive.\n",
        "    shock_frequency_lambda: float\n",
        "\n",
        "    # A nested immutable object defining the shock severity distribution G_Z.\n",
        "    loss_distribution_G_Z: LossDistributionGZ\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class SocialPlannerParams:\n",
        "    \"\"\"\n",
        "    Immutable container for the social planner's preference parameters.\n",
        "\n",
        "    This class defines the time preference of the decision-maker (typically a\n",
        "    government or social protection agency) who is optimizing the cash transfer\n",
        "    policy. The discount rate δ determines how future costs are weighted relative\n",
        "    to immediate costs in the objective function.\n",
        "\n",
        "    In the optimization problem defined in Section 2.2, the objective is to minimize\n",
        "    the expected discounted cost:\n",
        "        V(x) = E[ ∫_{0}^∞ e^{-δt} dS_t ]\n",
        "    where S_t is the cumulative capital transfer up to time t. A higher discount\n",
        "    rate δ places less weight on future transfers, potentially favoring policies\n",
        "    that defer costs.\n",
        "\n",
        "    The discount rate also appears in the policy comparator boundary from\n",
        "    Proposition 2.2 and in all value function computations throughout the research.\n",
        "\n",
        "    Input validation should ensure: δ > 0.\n",
        "\n",
        "    Attributes:\n",
        "        discount_rate_delta: The continuous-time discount rate δ, representing\n",
        "                             the social planner's time preference. Higher values\n",
        "                             indicate stronger preference for current over future\n",
        "                             consumption. Constraint: δ > 0.\n",
        "    \"\"\"\n",
        "    # The continuous-time discount rate δ; must be strictly positive.\n",
        "    discount_rate_delta: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MicroinsurancePolicyParams:\n",
        "    \"\"\"\n",
        "    Immutable container for specific microinsurance policy coverage parameters.\n",
        "\n",
        "    This class holds the technical parameters that define the retention and coverage\n",
        "    structure for a specific insurance product type, as described in Section 7 of\n",
        "    the research context. Each insurance type (Proportional, Excess-of-Loss, or\n",
        "    Total-Loss) uses different parameters to specify how losses are shared between\n",
        "    the household and the insurer.\n",
        "\n",
        "    For a valid, cleansed configuration, exactly one of the three attributes will\n",
        "    be non-None, corresponding to the active insurance type:\n",
        "    - Proportional insurance: uses proportional_retention_eta (η in [0, 1])\n",
        "    - Excess-of-Loss (XL) insurance: uses xl_retention_l (l in [0, 1])\n",
        "    - Total-Loss insurance: uses total_loss_L (L in [0, 1])\n",
        "\n",
        "    During the cleansing phase (Task 3, Step 2), unused parameters should be set\n",
        "    to None to prevent accidental access.\n",
        "\n",
        "    Input validation should ensure: exactly one parameter is non-None, and the\n",
        "    non-None parameter lies in [0, 1].\n",
        "\n",
        "    Attributes:\n",
        "        proportional_retention_eta: For 'Proportional' insurance (Section 7.1),\n",
        "                                    the fraction η of each loss retained by the\n",
        "                                    household. The insurer covers (1 - η) of each\n",
        "                                    loss. Constraint: η ∈ [0, 1] when active.\n",
        "        xl_retention_l: For 'Excess-of-Loss' (XL) insurance (Section 7.2), the\n",
        "                        retention limit l per unit of capital. The household bears\n",
        "                        min(loss, l) and the insurer covers max(loss - l, 0).\n",
        "                        Constraint: l ∈ [0, 1] when active.\n",
        "        total_loss_L: For 'Total-Loss' insurance (Section 7.3), the loss trigger\n",
        "                      level L. The insurer covers the entire loss if it exceeds L,\n",
        "                      otherwise the household bears it entirely.\n",
        "                      Constraint: L ∈ [0, 1] when active.\n",
        "    \"\"\"\n",
        "    # For 'Proportional' insurance, the fraction η of loss retained; must be in [0, 1] when active.\n",
        "    proportional_retention_eta: Optional[float]\n",
        "\n",
        "    # For 'XL' insurance, the retention limit l per unit capital; must be in [0, 1] when active.\n",
        "    xl_retention_l: Optional[float]\n",
        "\n",
        "    # For 'Total-Loss' insurance, the loss trigger L; must be in [0, 1] when active.\n",
        "    total_loss_L: Optional[float]\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MicroinsuranceParams:\n",
        "    \"\"\"\n",
        "    Immutable container for the complete microinsurance configuration.\n",
        "\n",
        "    This class aggregates all parameters related to the optional microinsurance\n",
        "    component described in Section 7 of the research context. When microinsurance\n",
        "    is active, it fundamentally transforms the model by modifying the post-shock\n",
        "    capital dynamics and introducing premium payments that reduce the effective\n",
        "    income generation rate.\n",
        "\n",
        "    The insurance premium is computed using the expected value principle with\n",
        "    safety loading γ (Equation 7.1):\n",
        "        p_R = (1 + γ) * λ * E[ceded loss per unit capital]\n",
        "    For proportional insurance, this simplifies to Equation 7.8:\n",
        "        p_R = (1 + γ) * λ * (1 - η) * (1 - μ)\n",
        "\n",
        "    The premium transforms the model primitives via Equations 7.2-7.3:\n",
        "        x*_R = (b / (b - p_R)) * x*\n",
        "        r_R = r * (b - p_R) / b\n",
        "    These transformations are valid only if b > p_R (feasibility constraint).\n",
        "\n",
        "    Input validation should ensure: if is_active is True, then insurance_type is\n",
        "    in {\"Proportional\", \"XL\", \"TotalLoss\"}, γ > 0, and the appropriate policy\n",
        "    parameter is set. The feasibility constraint b > p_R must be verified.\n",
        "\n",
        "    Attributes:\n",
        "        is_active: A boolean flag indicating whether microinsurance is active in\n",
        "                   this scenario. If False, the model reverts to the uninsured\n",
        "                   baseline from Sections 2-6.\n",
        "        insurance_type: The canonical name of the insurance policy type, one of\n",
        "                        \"Proportional\", \"XL\", or \"TotalLoss\". This determines\n",
        "                        which policy parameters are active and which premium and\n",
        "                        transformation formulas apply.\n",
        "        policy_parameters: A nested immutable object containing the specific\n",
        "                           coverage parameters (η, l, or L) for the active\n",
        "                           insurance type.\n",
        "        insurer_safety_loading_gamma: The safety loading factor γ used in the\n",
        "                                      expected value premium principle (Equation 7.1).\n",
        "                                      Higher values increase the premium and account\n",
        "                                      for insurer risk aversion and operational costs.\n",
        "                                      Constraint: γ > 0.\n",
        "    \"\"\"\n",
        "    # A boolean flag indicating whether microinsurance is active in this scenario.\n",
        "    is_active: bool\n",
        "\n",
        "    # The canonical name of the insurance policy type (\"Proportional\", \"XL\", or \"TotalLoss\").\n",
        "    insurance_type: str\n",
        "\n",
        "    # A nested immutable object with the specific coverage parameters for the active type.\n",
        "    policy_parameters: MicroinsurancePolicyParams\n",
        "\n",
        "    # The safety loading factor γ for premium calculation; must be strictly positive.\n",
        "    insurer_safety_loading_gamma: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ModelParameters:\n",
        "    \"\"\"\n",
        "    Immutable container for all core economic and stochastic model parameters.\n",
        "\n",
        "    This class serves as the primary high-level aggregator for the complete\n",
        "    model specification, composing all individual parameter blocks that collectively\n",
        "    define the economic environment, stochastic risk process, policy objectives,\n",
        "    and optional insurance coverage. Its nested structure mirrors the logical\n",
        "    organization of the 'model_parameters' section in the raw configuration\n",
        "    dictionary.\n",
        "\n",
        "    Together, these parameters define the complete universe in which the optimal\n",
        "    cash transfer problem is posed and solved. All mathematical objects in the\n",
        "    research (PDMP dynamics, HJB equation, value functions, boundary comparisons)\n",
        "    are functions of these parameters.\n",
        "\n",
        "    This object represents the definitive, validated, and immutable specification\n",
        "    of the model primitives. It is produced during the ingestion and validation\n",
        "    stage (Tasks 1-2) and consumed by all downstream computational stages. Any\n",
        "    modifications to the model require creating a new instance of this class.\n",
        "\n",
        "    Attributes:\n",
        "        household_economic_parameters: An immutable object containing household-level\n",
        "                                       parameters: poverty line x*, primitives\n",
        "                                       (a, b, c), and derived growth rate r.\n",
        "        stochastic_shock_parameters: An immutable object containing parameters for\n",
        "                                     the Poisson shock process (λ) and the loss\n",
        "                                     severity distribution (G_Z, μ).\n",
        "        social_planner_parameters: An immutable object containing the social\n",
        "                                   planner's time preference (discount rate δ).\n",
        "        microinsurance_parameters: An immutable object containing the complete\n",
        "                                   microinsurance configuration, including activation\n",
        "                                   status, insurance type, coverage parameters,\n",
        "                                   and premium settings.\n",
        "    \"\"\"\n",
        "    # Contains household-level parameters: poverty line, growth primitives, and derived rate.\n",
        "    household_economic_parameters: HouseholdEconomicParams\n",
        "\n",
        "    # Contains parameters for the Poisson shock process and loss severity distribution.\n",
        "    stochastic_shock_parameters: StochasticShockParams\n",
        "\n",
        "    # Contains the social planner's time preference (discount rate δ).\n",
        "    social_planner_parameters: SocialPlannerParams\n",
        "\n",
        "    # Contains the complete microinsurance configuration and coverage parameters.\n",
        "    microinsurance_parameters: MicroinsuranceParams\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ThresholdSearchBracket:\n",
        "    \"\"\"\n",
        "    Immutable container for the optimizer's search bracket definition.\n",
        "\n",
        "    This class specifies the bounds for the one-dimensional search for the\n",
        "    optimal transfer threshold, y*. The bounds are defined relative to the\n",
        "    active poverty line, providing a dynamic and context-aware search space.\n",
        "\n",
        "    Attributes:\n",
        "        y_min_definition: A string describing how the lower bound of the search\n",
        "                          is determined. In this model, it is always anchored to\n",
        "                          the \"active_poverty_line\".\n",
        "        y_max_factor_times_y_min: A factor used to determine the upper bound\n",
        "                                  of the search space as a multiple of the\n",
        "                                  lower bound (y_max = y_min * factor).\n",
        "                                  This value must be strictly greater than 1.0\n",
        "                                  to ensure a valid search interval.\n",
        "    \"\"\"\n",
        "    # A string describing how the lower bound of the search is defined.\n",
        "    y_min_definition: str\n",
        "\n",
        "    # The factor to determine the upper bound of the search. Constraint: must be > 1.0.\n",
        "    y_max_factor_times_y_min: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MethodSelection:\n",
        "    \"\"\"\n",
        "    Immutable container for algorithm selection and objective definition.\n",
        "\n",
        "    This class, includes the search bracket, specifies the\n",
        "    high-level choices that guide the computational strategy for solving the\n",
        "    optimal control problem.\n",
        "\n",
        "    Attributes:\n",
        "        algorithm_to_use: The primary algorithm to use. Must be one of 'Auto',\n",
        "                          'ClosedForm', 'MonteCarlo', or 'FixedPoint'.\n",
        "        auto_mode_rule: A human-readable string describing the logic used when\n",
        "                        `algorithm_to_use` is 'Auto'.\n",
        "        threshold_search_bracket: A nested `ThresholdSearchBracket` object that\n",
        "                                  defines the bounds for the optimization of y*.\n",
        "    \"\"\"\n",
        "    # The primary algorithm to use ('Auto', 'ClosedForm', 'MonteCarlo', 'FixedPoint').\n",
        "    algorithm_to_use: str\n",
        "\n",
        "    # A human-readable string describing the logic for 'Auto' mode.\n",
        "    auto_mode_rule: str\n",
        "\n",
        "    # A nested object defining the bounds for the optimizer search.\n",
        "    threshold_search_bracket: ThresholdSearchBracket\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MonteCarloSettings:\n",
        "    \"\"\"\n",
        "    Immutable container for all Monte Carlo simulation configuration parameters.\n",
        "\n",
        "    This class specifies the complete set of parameters that control the execution,\n",
        "    precision, reproducibility, and diagnostic behavior of the Monte Carlo simulation\n",
        "    engine described in Section 6 of the research context. These settings govern\n",
        "    the fundamental trade-off between computational cost and statistical accuracy.\n",
        "\n",
        "    The number of simulation paths N is the primary determinant of statistical\n",
        "    precision: confidence interval widths scale as 1/√N. The number of batches B\n",
        "    determines the degrees of freedom for confidence interval construction via the\n",
        "    t-distribution. The time horizon T implements the truncation policy, treating\n",
        "    first-passage times exceeding T as infinite (zero discounted contribution).\n",
        "\n",
        "    Reproducibility is ensured through explicit random seed management. All random\n",
        "    number generation must use this seed to guarantee bit-for-bit identical results\n",
        "    across multiple runs with the same configuration.\n",
        "\n",
        "    Input validation should ensure: N > 0, B > 1, T > 0, 0 < confidence_interval_level < 1,\n",
        "    and that random_seed is a valid integer.\n",
        "\n",
        "    Attributes:\n",
        "        num_simulation_paths_N: The total number of independent PDMP paths to\n",
        "                                simulate. Higher values increase precision but\n",
        "                                also computational cost. Typical values: 10³ to\n",
        "                                10⁶. Constraint: N > 0.\n",
        "        num_batches_for_CI: The number of batches to partition the N paths into\n",
        "                            for batch-means confidence interval construction.\n",
        "                            Must be large enough for valid t-distribution inference\n",
        "                            (typically B ≥ 20). Constraint: B > 1.\n",
        "        random_seed: The integer seed for the random number generator, ensuring\n",
        "                     full reproducibility of all simulation results. All random\n",
        "                     sampling (inter-arrival times, shock fractions) must use\n",
        "                     this seed.\n",
        "        simulation_time_horizon_T: The truncation time horizon T. PDMP paths that\n",
        "                                   do not reach the threshold before time T are\n",
        "                                   treated as having infinite first-passage time,\n",
        "                                   contributing zero to the discounted expectation.\n",
        "                                   Should be large relative to 1/δ. Constraint: T > 0.\n",
        "        confidence_interval_level: The desired confidence level for batch-means\n",
        "                                   confidence intervals, typically 0.99 or 0.95.\n",
        "                                   Constraint: 0 < level < 1.\n",
        "    \"\"\"\n",
        "    # The total number of independent simulation paths N. Constraint: N > 0.\n",
        "    num_simulation_paths_N: int\n",
        "\n",
        "    # The number of batches B for confidence interval construction. Constraint: B > 1.\n",
        "    num_batches_for_CI: int\n",
        "\n",
        "    # The integer seed for the random number generator, ensuring reproducibility.\n",
        "    random_seed: int\n",
        "\n",
        "    # The time horizon T for truncating simulation paths. Constraint: T > 0.\n",
        "    simulation_time_horizon_T: float\n",
        "\n",
        "    # The desired confidence level for CIs (e.g., 0.99). Constraint: 0 < level < 1.\n",
        "    confidence_interval_level: float\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class OptimizerSettings:\n",
        "    \"\"\"\n",
        "    Immutable container for one-dimensional threshold optimizer configuration.\n",
        "\n",
        "    This class defines the complete set of parameters for the numerical optimization\n",
        "    routine used to find the optimal cash transfer threshold y* that minimizes the\n",
        "    value function V_y(y). The optimization problem is:\n",
        "        y* = argmin_{y ≥ x̄*} V_y(y)\n",
        "    where x̄* is the active poverty line (x* or x*_R depending on insurance status).\n",
        "\n",
        "    Brent's method is the standard choice for this problem because it combines the\n",
        "    robustness of golden-section search with the speed of inverse quadratic\n",
        "    interpolation, and it is guaranteed to converge for continuous unimodal\n",
        "    functions. The method requires only function evaluations, not derivatives.\n",
        "\n",
        "    The tolerances control the stopping criteria: xtol governs precision in the\n",
        "    threshold location y*, while ftol governs precision in the function value\n",
        "    V_y(y*). The maximum iterations parameter prevents infinite loops and provides\n",
        "    a computational budget constraint.\n",
        "\n",
        "    Input validation should ensure: optimizer_algorithm is \"Brent\" (or another\n",
        "    valid 1-D method), tolerances are positive, and max_iterations > 0.\n",
        "\n",
        "    Attributes:\n",
        "        optimizer_algorithm: The name of the 1-D optimization algorithm to use.\n",
        "                             Standard choice is \"Brent\" for robust, derivative-free\n",
        "                             optimization. This maps to scipy.optimize.minimize_scalar\n",
        "                             with method='bounded'.\n",
        "        y_tolerance_xtol: The absolute tolerance for the optimization variable\n",
        "                          (the threshold y*). Optimization stops when the bracket\n",
        "                          width is smaller than this value. Typical: 1e-3.\n",
        "        f_tolerance_ftol: The relative tolerance for the objective function value\n",
        "                          V_y(y*). Optimization stops when relative changes in the\n",
        "                          function value fall below this threshold. Typical: 1e-6.\n",
        "        max_iterations: The maximum number of iterations allowed before the optimizer\n",
        "                        terminates. Provides a computational budget constraint and\n",
        "                        prevents infinite loops. Typical: 200.\n",
        "    \"\"\"\n",
        "    # The 1-D optimization algorithm name (e.g., \"Brent\").\n",
        "    optimizer_algorithm: str\n",
        "\n",
        "    # The absolute tolerance for the threshold location y*. Typical: 1e-3.\n",
        "    y_tolerance_xtol: float\n",
        "\n",
        "    # The relative tolerance for the objective function value. Typical: 1e-6.\n",
        "    f_tolerance_ftol: float\n",
        "\n",
        "    # The maximum number of optimizer iterations allowed. Typical: 200.\n",
        "    max_iterations: int\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ComputationParameters:\n",
        "    \"\"\"\n",
        "    Immutable container for all computational methodology and algorithm settings.\n",
        "\n",
        "    This class serves as the high-level aggregator for all parameters that control\n",
        "    how the optimal cash transfer problem is solved numerically. It composes the\n",
        "    three main computational subsystems: algorithm selection (which value function\n",
        "    evaluation method to use), Monte Carlo simulation settings (if numerical\n",
        "    simulation is required), and optimization settings (for finding y*).\n",
        "\n",
        "    Together, these parameters define the complete computational strategy. The\n",
        "    method selection determines the fundamental approach (analytical vs. numerical),\n",
        "    the Monte Carlo settings control statistical precision and reproducibility,\n",
        "    and the optimizer settings govern the convergence and accuracy of the threshold\n",
        "    optimization.\n",
        "\n",
        "    This object is produced during the configuration parsing stage and consumed\n",
        "    throughout the computational pipeline. It ensures that all numerical methods\n",
        "    operate with consistent, validated settings and that the computational approach\n",
        "    matches the problem structure (e.g., using closed forms when available).\n",
        "\n",
        "    Attributes:\n",
        "        method_selection: An immutable object defining the core algorithm choice\n",
        "                          for value function evaluation: ClosedForm, MonteCarlo,\n",
        "                          FixedPoint, or Auto.\n",
        "        monte_carlo_settings: An immutable object containing all parameters for\n",
        "                              the Monte Carlo simulation engine: sample sizes,\n",
        "                              batching, truncation, and reproducibility settings.\n",
        "        optimizer_settings: An immutable object containing all parameters for the\n",
        "                            1-D optimizer used to find the optimal threshold y*:\n",
        "                            algorithm choice, tolerances, and iteration limits.\n",
        "    \"\"\"\n",
        "    # Contains the algorithm selection strategy for value function evaluation.\n",
        "    method_selection: MethodSelection\n",
        "\n",
        "    # Contains all Monte Carlo simulation parameters: sample sizes, truncation, and seeding.\n",
        "    monte_carlo_settings: MonteCarloSettings\n",
        "\n",
        "    # Contains all 1-D optimizer parameters: algorithm, tolerances, and iteration limits.\n",
        "    optimizer_settings: OptimizerSettings\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class CapitalGrid:\n",
        "    \"\"\"\n",
        "    Immutable container for the capital grid definition for value function plotting.\n",
        "\n",
        "    This class defines the discrete grid of capital levels (the x-axis) over which\n",
        "    all value functions will be evaluated and plotted. The grid specification\n",
        "    determines the resolution and range of the output visualizations, enabling\n",
        "    comparison of different policies (C(x), D(x), V_y(x), V_{y*}(x)) across the\n",
        "    full capital spectrum from poverty to abundance.\n",
        "\n",
        "    The grid must be sufficiently fine to capture the smooth behavior of value\n",
        "    functions, including the kink at the poverty line x* and any threshold\n",
        "    boundaries. It must also span a wide enough range to demonstrate the tail\n",
        "    decay behavior (V(x) → 0 as x → ∞) required by the verification checks in\n",
        "    Task 17.\n",
        "\n",
        "    A typical configuration is [0, 100] with 201 points, providing resolution\n",
        "    of 0.5 capital units. This is adequate for most scenarios with x* = 20.\n",
        "\n",
        "    Input validation should ensure: start_value ≥ 0, stop_value > start_value,\n",
        "    and num_points > 1 (minimum two points for a meaningful grid).\n",
        "\n",
        "    Attributes:\n",
        "        start_value: The starting value of the capital grid, typically 0 to\n",
        "                     include the full poverty region [0, x*].\n",
        "        stop_value: The ending value of the capital grid, chosen large enough\n",
        "                    to demonstrate tail decay behavior (typically 4-5 times x*).\n",
        "        num_points: The number of discrete, evenly-spaced points in the grid.\n",
        "                    Higher values provide smoother plots but increase computational\n",
        "                    cost. Constraint: num_points > 1.\n",
        "    \"\"\"\n",
        "    # The starting value of the capital grid (x-axis), typically 0.\n",
        "    start_value: float\n",
        "\n",
        "    # The ending value of the capital grid, chosen to capture tail behavior.\n",
        "    stop_value: float\n",
        "\n",
        "    # The number of evenly-spaced points in the grid. Constraint: num_points > 1.\n",
        "    num_points: int\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class OutputParameters:\n",
        "    \"\"\"\n",
        "    Immutable container for all output generation and visualization parameters.\n",
        "\n",
        "    This class specifies what should be computed, evaluated, and made available\n",
        "    for analysis and visualization at the end of the computational pipeline. It\n",
        "    defines both the capital grid structure (where value functions are evaluated)\n",
        "    and which specific value function series should be computed and plotted.\n",
        "\n",
        "    The series_to_plot list contains canonical names that map to specific value\n",
        "    functions or policy comparators:\n",
        "    - \"C_x_inject_to_poverty_line\": Cost function C(x) from Section 2.2\n",
        "    - \"D_x_perpetual_transfers\": Perpetual transfer cost D(x) from Proposition 2.1\n",
        "    - \"V_y_x_at_y_equals_x_star\": Threshold value V_{x*}(x) (inject-to-poverty baseline)\n",
        "    - \"V_y_x_at_y_equals_40\": Threshold value V_{40}(x) (example buffer strategy)\n",
        "    - \"V_y_star_x_optimal_threshold\": Optimal value function V_{y*}(x)\n",
        "\n",
        "    Each series name triggers specific computational procedures and evaluation\n",
        "    methods based on the selected algorithm (closed-form or Monte Carlo).\n",
        "\n",
        "    Attributes:\n",
        "        capital_grid_for_plots: An immutable object defining the discrete capital\n",
        "                                grid (x-axis) over which all value functions are\n",
        "                                evaluated. Determines plot resolution and range.\n",
        "        series_to_plot: A list of canonical string identifiers for the value\n",
        "                        function series that should be computed and made available\n",
        "                        for plotting. Each identifier maps to a specific value\n",
        "                        function or policy comparator from the research.\n",
        "    \"\"\"\n",
        "    # Defines the discrete capital grid (x-axis) for evaluating value functions.\n",
        "    capital_grid_for_plots: CapitalGrid\n",
        "\n",
        "    # A list of canonical names for value function series to compute and plot.\n",
        "    series_to_plot: List[str]\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ParsedStudyConfig:\n",
        "    \"\"\"\n",
        "    The top-level, fully-parsed, strongly-typed, and immutable configuration object.\n",
        "\n",
        "    This class represents the definitive, validated, and immutable specification\n",
        "    of the complete research configuration after successful completion of the\n",
        "    ingestion, validation, and cleansing stages (Tasks 1-3). It serves as the\n",
        "    primary data contract between the configuration processing phase and all\n",
        "    downstream computational stages of the research pipeline.\n",
        "\n",
        "    The structure of this object perfectly mirrors the hierarchical organization\n",
        "    of the raw `full_study_configuration` dictionary, but provides strong typing,\n",
        "    immutability guarantees, and a clean interface that prevents accidental mutation\n",
        "    or inconsistent parameter access. All parameters have been validated for\n",
        "    mathematical consistency, economic feasibility, and numerical soundness.\n",
        "\n",
        "    This object is passed as the primary input to all major pipeline stages:\n",
        "    - Derived quantity computation (Tasks 4-5)\n",
        "    - Method selection and sampler construction (Tasks 6-7)\n",
        "    - PDMP simulation and value function evaluation (Tasks 8-16)\n",
        "    - Verification and output generation (Tasks 17-28)\n",
        "\n",
        "    Its immutability ensures that all computational stages operate on a consistent,\n",
        "    unmodified configuration, enabling full reproducibility and preventing subtle\n",
        "    bugs from parameter mutations.\n",
        "\n",
        "    Attributes:\n",
        "        study_metadata: An immutable object containing descriptive metadata about\n",
        "                        the study and scenario: name, ID, description, and version.\n",
        "        model_parameters: An immutable object containing the complete, nested\n",
        "                          specification of the economic and stochastic model:\n",
        "                          household parameters, shock process, social planner\n",
        "                          preferences, and microinsurance configuration.\n",
        "        computation_parameters: An immutable object containing all settings related\n",
        "                                to computational methods: algorithm selection,\n",
        "                                Monte Carlo settings, and optimizer configuration.\n",
        "        output_parameters: An immutable object containing all settings related to\n",
        "                           output generation: capital grid specification and list\n",
        "                           of value function series to compute and plot.\n",
        "    \"\"\"\n",
        "    # Contains descriptive metadata: study name, scenario ID, description, and version.\n",
        "    study_metadata: StudyMetadata\n",
        "\n",
        "    # Contains the complete economic and stochastic model specification.\n",
        "    model_parameters: ModelParameters\n",
        "\n",
        "    # Contains all computational methodology settings: algorithms and optimization.\n",
        "    computation_parameters: ComputationParameters\n",
        "\n",
        "    # Contains all output generation settings: grid specification and series selection.\n",
        "    output_parameters: OutputParameters\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Step 2 & 3: Helper function to parse dict into typed dataclasses\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Generic TypeVar for the casting function to ensure type hint correctness.\n",
        "T = TypeVar('T')\n",
        "\n",
        "def _safe_get_and_cast(\n",
        "    data: Dict[str, Any],\n",
        "    path: str,\n",
        "    target_type: Type[T],\n",
        "    is_optional: bool = False\n",
        ") -> Optional[T]:\n",
        "    \"\"\"\n",
        "    Safely retrieves and casts a value from a nested dictionary using a path.\n",
        "\n",
        "    This robust utility function is the cornerstone of the parsing logic. It\n",
        "    traverses a dot-separated path (e.g., \"a.b.c\") within a nested dictionary,\n",
        "    retrieves the target value, and casts it to a specified type. Its primary\n",
        "    purpose is to provide exceptionally clear and actionable error messages if\n",
        "    any part of the access or casting process fails, which is critical for\n",
        "    debugging user-provided configuration files.\n",
        "\n",
        "    Args:\n",
        "        data: The nested dictionary from which to extract data.\n",
        "        path: A dot-separated string representing the path to the desired value.\n",
        "        target_type: The Python type to which the value should be cast (e.g.,\n",
        "                     `float`, `int`, `str`, `bool`, `dict`, `list`).\n",
        "        is_optional: If True, the function will return `None` if any key along\n",
        "                     the path is missing, rather than raising a `KeyError`.\n",
        "                     This is used for parsing optional configuration fields.\n",
        "\n",
        "    Returns:\n",
        "        The retrieved and cast value of type `T`, or `None` if the field is\n",
        "        marked as optional and is not found.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required key (when `is_optional` is False) along the\n",
        "                  path is not found. The error message includes the full path.\n",
        "        ValueError: If the retrieved value cannot be successfully cast to the\n",
        "                    `target_type`. The error message includes the path, the\n",
        "                    problematic value, and its original type.\n",
        "    \"\"\"\n",
        "    # Split the dot-separated path into a list of individual keys for traversal.\n",
        "    keys = path.split('.')\n",
        "\n",
        "    # Start the traversal from the top-level of the input dictionary.\n",
        "    current_level = data\n",
        "\n",
        "    # Iterate through the keys to navigate down into the nested structure.\n",
        "    # We stop at the second-to-last key, as the final key is handled separately.\n",
        "    for i, key in enumerate(keys[:-1]):\n",
        "        # At each level, check if the current object is a dictionary and contains the next key.\n",
        "        if not isinstance(current_level, dict) or key not in current_level:\n",
        "            # If an intermediate key is missing, the path is invalid.\n",
        "            # Construct the partial path that caused the failure for a precise error message.\n",
        "            error_path = '.'.join(keys[:i+1])\n",
        "            # If the path was marked as optional, we can simply return None.\n",
        "            if is_optional:\n",
        "                return None\n",
        "            # Otherwise, raise a KeyError indicating the exact point of failure.\n",
        "            raise KeyError(f\"Missing required nested key path: '{error_path}'\")\n",
        "        # Move to the next level in the dictionary.\n",
        "        current_level = current_level[key]\n",
        "\n",
        "    # Get the final key in the path.\n",
        "    final_key = keys[-1]\n",
        "\n",
        "    # Check for the existence of the final key in the current dictionary level.\n",
        "    if final_key not in current_level:\n",
        "        # If the final key is missing, return None if optional.\n",
        "        if is_optional:\n",
        "            return None\n",
        "        # Otherwise, raise a KeyError for the full missing path.\n",
        "        raise KeyError(f\"Missing required configuration key: '{path}'\")\n",
        "\n",
        "    # Retrieve the raw value associated with the final key.\n",
        "    value = current_level[final_key]\n",
        "\n",
        "    # Attempt to cast the retrieved value to the specified target type.\n",
        "    try:\n",
        "        # Optimization: if the value is already of the correct type, return it directly.\n",
        "        if isinstance(value, target_type):\n",
        "            return value\n",
        "        # If not, perform the explicit cast (e.g., float('20.0')).\n",
        "        return target_type(value)\n",
        "    except (ValueError, TypeError) as e:\n",
        "        # If the cast fails (e.g., float('abc')), raise a detailed ValueError.\n",
        "        # The error message is designed to be highly informative for debugging.\n",
        "        raise ValueError(\n",
        "            f\"Invalid value for parameter '{path}'. Expected type \"\n",
        "            f\"'{target_type.__name__}', but got value '{value}' of type \"\n",
        "            f\"'{type(value).__name__}'.\"\n",
        "        ) from e\n",
        "\n",
        "\n",
        "def _parse_study_metadata(config: Dict[str, Any]) -> 'StudyMetadata':\n",
        "    \"\"\"\n",
        "    Parses the 'study_metadata' block of the configuration into its dataclass.\n",
        "\n",
        "    Args:\n",
        "        config: The full raw configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A populated `StudyMetadata` object.\n",
        "    \"\"\"\n",
        "    # Construct the StudyMetadata object by safely getting and casting each field.\n",
        "    return StudyMetadata(\n",
        "        study_name=_safe_get_and_cast(config, \"study_metadata.study_name\", str),\n",
        "        scenario_id=_safe_get_and_cast(config, \"study_metadata.scenario_id\", str),\n",
        "        description=_safe_get_and_cast(config, \"study_metadata.description\", str),\n",
        "        version=_safe_get_and_cast(config, \"study_metadata.version\", float),\n",
        "    )\n",
        "\n",
        "def _parse_model_parameters(config: Dict[str, Any]) -> 'ModelParameters':\n",
        "    \"\"\"\n",
        "    Parses the entire 'model_parameters' block by composing sub-parsers.\n",
        "\n",
        "    This function follows a bottom-up approach, parsing the most deeply nested\n",
        "    structures first and composing them into the final `ModelParameters` object.\n",
        "\n",
        "    Args:\n",
        "        config: The full raw configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A populated `ModelParameters` object.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Parse innermost structures first ---\n",
        "    # Parse the 'primitive_params_for_r' dictionary.\n",
        "    primitives_dict = _safe_get_and_cast(config, \"model_parameters.household_economic_parameters.primitive_params_for_r\", dict)\n",
        "    primitives = PrimitiveParamsForR(\n",
        "        marginal_propensity_consume_a=float(primitives_dict['marginal_propensity_consume_a']),\n",
        "        income_generation_rate_b=float(primitives_dict['income_generation_rate_b']),\n",
        "        savings_conversion_rate_c=float(primitives_dict['savings_conversion_rate_c']),\n",
        "    )\n",
        "\n",
        "    # Parse the 'household_economic_parameters' block.\n",
        "    household_params = HouseholdEconomicParams(\n",
        "        poverty_line_x_star=_safe_get_and_cast(config, \"model_parameters.household_economic_parameters.poverty_line_x_star\", float),\n",
        "        primitive_params_for_r=primitives,\n",
        "        derived_growth_rate_r=_safe_get_and_cast(config, \"model_parameters.household_economic_parameters.derived_growth_rate_r\", float),\n",
        "    )\n",
        "\n",
        "    # Parse the 'loss_distribution_G_Z' block.\n",
        "    loss_dist_dict = _safe_get_and_cast(config, \"model_parameters.stochastic_shock_parameters.loss_distribution_G_Z\", dict)\n",
        "    loss_dist = LossDistributionGZ(\n",
        "        name=str(loss_dist_dict['name']),\n",
        "        parameters=dict(loss_dist_dict['parameters']),\n",
        "        mean_E_Z_mu=float(loss_dist_dict['mean_E_Z_mu']),\n",
        "    )\n",
        "\n",
        "    # Parse the 'stochastic_shock_parameters' block.\n",
        "    shock_params = StochasticShockParams(\n",
        "        shock_frequency_lambda=_safe_get_and_cast(config, \"model_parameters.stochastic_shock_parameters.shock_frequency_lambda\", float),\n",
        "        loss_distribution_G_Z=loss_dist,\n",
        "    )\n",
        "\n",
        "    # Parse the 'social_planner_parameters' block.\n",
        "    planner_params = SocialPlannerParams(\n",
        "        discount_rate_delta=_safe_get_and_cast(config, \"model_parameters.social_planner_parameters.discount_rate_delta\", float),\n",
        "    )\n",
        "\n",
        "    # Parse the 'policy_parameters' block for microinsurance, handling optional fields.\n",
        "    policy_dict = _safe_get_and_cast(config, \"model_parameters.microinsurance_parameters.policy_parameters\", dict)\n",
        "    policy_params = MicroinsurancePolicyParams(\n",
        "        proportional_retention_eta=_safe_get_and_cast(policy_dict, \"proportional_retention_eta\", float, is_optional=True),\n",
        "        xl_retention_l=_safe_get_and_cast(policy_dict, \"xl_retention_l\", float, is_optional=True),\n",
        "        total_loss_L=_safe_get_and_cast(policy_dict, \"total_loss_L\", float, is_optional=True),\n",
        "    )\n",
        "\n",
        "    # Parse the main 'microinsurance_parameters' block.\n",
        "    insurance_params = MicroinsuranceParams(\n",
        "        is_active=_safe_get_and_cast(config, \"model_parameters.microinsurance_parameters.is_active\", bool),\n",
        "        insurance_type=_safe_get_and_cast(config, \"model_parameters.microinsurance_parameters.insurance_type\", str),\n",
        "        policy_parameters=policy_params,\n",
        "        insurer_safety_loading_gamma=_safe_get_and_cast(config, \"model_parameters.microinsurance_parameters.insurer_safety_loading_gamma\", float),\n",
        "    )\n",
        "\n",
        "    # --- Assemble the final ModelParameters object from its components ---\n",
        "    return ModelParameters(\n",
        "        household_economic_parameters=household_params,\n",
        "        stochastic_shock_parameters=shock_params,\n",
        "        social_planner_parameters=planner_params,\n",
        "        microinsurance_parameters=insurance_params,\n",
        "    )\n",
        "\n",
        "def _parse_computation_parameters(config: Dict[str, Any]) -> 'ComputationParameters':\n",
        "    \"\"\"\n",
        "    Parses the 'computation_parameters' block into its dataclass.\n",
        "\n",
        "    Args:\n",
        "        config: The full raw configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A populated `ComputationParameters` object.\n",
        "    \"\"\"\n",
        "    # Parse the 'method_selection' sub-block.\n",
        "    method_selection = MethodSelection(\n",
        "        algorithm_to_use=_safe_get_and_cast(config, \"computation_parameters.method_selection.algorithm_to_use\", str),\n",
        "        auto_mode_rule=_safe_get_and_cast(config, \"computation_parameters.method_selection.auto_mode_rule\", str),\n",
        "    )\n",
        "\n",
        "    # Parse the 'monte_carlo_settings' sub-block.\n",
        "    mc_settings = MonteCarloSettings(\n",
        "        num_simulation_paths_N=_safe_get_and_cast(config, \"computation_parameters.monte_carlo_settings.num_simulation_paths_N\", int),\n",
        "        num_batches_for_CI=_safe_get_and_cast(config, \"computation_parameters.monte_carlo_settings.num_batches_for_CI\", int),\n",
        "        random_seed=_safe_get_and_cast(config, \"computation_parameters.monte_carlo_settings.random_seed\", int),\n",
        "        simulation_time_horizon_T=_safe_get_and_cast(config, \"computation_parameters.monte_carlo_settings.simulation_time_horizon_T\", float),\n",
        "        confidence_interval_level=_safe_get_and_cast(config, \"computation_parameters.monte_carlo_settings.confidence_interval_level\", float),\n",
        "    )\n",
        "\n",
        "    # Parse the 'optimizer_settings' sub-block.\n",
        "    optimizer_settings = OptimizerSettings(\n",
        "        optimizer_algorithm=_safe_get_and_cast(config, \"computation_parameters.optimizer_settings.optimizer_algorithm\", str),\n",
        "        y_tolerance_xtol=_safe_get_and_cast(config, \"computation_parameters.optimizer_settings.y_tolerance_xtol\", float),\n",
        "        f_tolerance_ftol=_safe_get_and_cast(config, \"computation_parameters.optimizer_settings.f_tolerance_ftol\", float),\n",
        "        max_iterations=_safe_get_and_cast(config, \"computation_parameters.optimizer_settings.max_iterations\", int),\n",
        "    )\n",
        "\n",
        "    # Assemble the final ComputationParameters object.\n",
        "    return ComputationParameters(\n",
        "        method_selection=method_selection,\n",
        "        monte_carlo_settings=mc_settings,\n",
        "        optimizer_settings=optimizer_settings,\n",
        "    )\n",
        "\n",
        "def _parse_output_parameters(config: Dict[str, Any]) -> 'OutputParameters':\n",
        "    \"\"\"\n",
        "    Parses the 'output_parameters' block into its dataclass.\n",
        "\n",
        "    Args:\n",
        "        config: The full raw configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A populated `OutputParameters` object.\n",
        "    \"\"\"\n",
        "    # Parse the 'capital_grid_for_plots' sub-block.\n",
        "    grid_params = CapitalGrid(\n",
        "        start_value=_safe_get_and_cast(config, \"output_parameters.capital_grid_for_plots.start_value\", float),\n",
        "        stop_value=_safe_get_and_cast(config, \"output_parameters.capital_grid_for_plots.stop_value\", float),\n",
        "        num_points=_safe_get_and_cast(config, \"output_parameters.capital_grid_for_plots.num_points\", int),\n",
        "    )\n",
        "\n",
        "    # Assemble the final OutputParameters object.\n",
        "    return OutputParameters(\n",
        "        capital_grid_for_plots=grid_params,\n",
        "        series_to_plot=_safe_get_and_cast(config, \"output_parameters.series_to_plot\", list),\n",
        "    )\n",
        "\n",
        "def _parse_config_to_dataclasses(\n",
        "    config: Dict[str, Any]\n",
        ") -> 'ParsedStudyConfig':\n",
        "    \"\"\"\n",
        "    Parses a raw configuration dictionary into a complete, typed dataclass hierarchy.\n",
        "\n",
        "    This function transforms the untyped input dictionary into a\n",
        "    fully populated, immutable `ParsedStudyConfig` object. It employs a\n",
        "    bottom-up parsing strategy with modular helpers and a robust, safe-access\n",
        "    utility to provide granular error handling and ensure structural integrity.\n",
        "    This function is the core of the configuration ingestion stage.\n",
        "\n",
        "    Process:\n",
        "    1.  Calls dedicated sub-parsers for each top-level block of the configuration\n",
        "        (`study_metadata`, `model_parameters`, etc.).\n",
        "    2.  Each sub-parser uses a safe-access utility (`_safe_get_and_cast`) to\n",
        "        retrieve and cast values, providing detailed error messages for missing\n",
        "        keys or type mismatches.\n",
        "    3.  The results from the sub-parsers are composed into the final, top-level\n",
        "        `ParsedStudyConfig` object, creating a complete and validated data\n",
        "        contract for the rest of the pipeline.\n",
        "\n",
        "    Args:\n",
        "        config: The validated, raw, nested dictionary containing the full study\n",
        "                configuration.\n",
        "\n",
        "    Returns:\n",
        "        A `ParsedStudyConfig` object containing the complete, structured, and\n",
        "        validated parameters, ready for downstream use.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If a required key is missing at any level of the nested\n",
        "                  configuration structure, with a detailed path.\n",
        "        ValueError: If a parameter value is of an incorrect type or format that\n",
        "                    cannot be cast.\n",
        "    \"\"\"\n",
        "    # Step 1: Parse the 'study_metadata' block into its dataclass.\n",
        "    metadata = _parse_study_metadata(config)\n",
        "\n",
        "    # Step 2: Parse the entire 'model_parameters' block using its dedicated sub-parser.\n",
        "    model_params = _parse_model_parameters(config)\n",
        "\n",
        "    # Step 3: Parse the entire 'computation_parameters' block.\n",
        "    computation_params = _parse_computation_parameters(config)\n",
        "\n",
        "    # Step 4: Parse the entire 'output_parameters' block.\n",
        "    output_params = _parse_output_parameters(config)\n",
        "\n",
        "    # Step 5: Assemble the final, top-level dataclass object from the parsed components.\n",
        "    # This object is the final, safe-to-use configuration for the entire pipeline.\n",
        "    return ParsedStudyConfig(\n",
        "        study_metadata=metadata,\n",
        "        model_parameters=model_params,\n",
        "        computation_parameters=computation_params,\n",
        "        output_parameters=output_params,\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 1, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def ingest_and_parse_config(\n",
        "    config: Dict[str, Any]\n",
        ") -> ParsedStudyConfig:\n",
        "    \"\"\"\n",
        "    Ingests, validates, and parses the raw study configuration dictionary.\n",
        "\n",
        "    This orchestrator function serves as the entry point for the entire\n",
        "    quantitative pipeline. It performs a series of critical setup steps:\n",
        "    1.  Validates the top-level structure of the input dictionary to ensure\n",
        "        all major sections are present.\n",
        "    2.  Extracts and parses all required parameters from the nested dictionary\n",
        "        structure.\n",
        "    3.  Populates a hierarchy of strongly-typed, immutable dataclasses to\n",
        "        provide a safe and reliable parameter object for all downstream\n",
        "        computational tasks.\n",
        "\n",
        "    Args:\n",
        "        config: The raw, nested dictionary containing the full study\n",
        "                configuration, matching the schema provided in the research\n",
        "                context.\n",
        "\n",
        "    Returns:\n",
        "        A `ParsedStudyConfig` object, which is a deeply nested, immutable\n",
        "        dataclass instance containing all validated and typed parameters\n",
        "        ready for use in the simulation and analysis pipeline.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input `config` is not a dictionary.\n",
        "        KeyError: If a required key is missing at any level of the nested\n",
        "                  configuration structure.\n",
        "        ValueError: If a parameter value is of an incorrect type or format\n",
        "                    (e.g., a string that cannot be converted to a float).\n",
        "    \"\"\"\n",
        "    # Step 1: Validate the presence of all required top-level keys.\n",
        "    # This ensures the basic structure of the configuration is sound before\n",
        "    # attempting to parse its contents.\n",
        "    _validate_top_level_structure(config=config)\n",
        "\n",
        "    # Step 2 & 3: Parse the validated dictionary into a structured, typed,\n",
        "    # and immutable dataclass object. This provides type safety and prevents\n",
        "    # accidental modification of parameters during the pipeline execution.\n",
        "    parsed_config = _parse_config_to_dataclasses(config=config)\n",
        "\n",
        "    # Return the final, safe-to-use configuration object.\n",
        "    return parsed_config\n"
      ],
      "metadata": {
        "id": "OTONsi89_Qyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 — Validate parameter appropriateness and hard constraints\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 2: Validate parameter appropriateness and hard constraints\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 1: Helper function to validate economic primitives\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_economic_primitives(\n",
        "    model_params: ModelParameters,\n",
        "    raw_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the domain constraints of core economic model parameters.\n",
        "\n",
        "    This function ensures that the primitive parameters defining the economic\n",
        "    environment and stochastic process adhere to their theoretical bounds, which\n",
        "    are necessary for the model to be well-posed and economically meaningful.\n",
        "\n",
        "    Args:\n",
        "        model_params: The parsed dataclass containing core model parameters.\n",
        "        raw_config: The original raw configuration dictionary, needed to access\n",
        "                    primitive parameters used to derive `r`.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any parameter violates its required domain constraint.\n",
        "    \"\"\"\n",
        "    # --- Validate strictly positive parameters ---\n",
        "    # These parameters must be > 0 for the model to be non-trivial and stable.\n",
        "    positive_params = {\n",
        "        \"poverty_line_x_star\": model_params.poverty_line_x_star,\n",
        "        \"income_generation_rate_b\": model_params.income_generation_rate_b,\n",
        "        \"shock_frequency_lambda\": model_params.shock_frequency_lambda,\n",
        "        \"discount_rate_delta\": model_params.discount_rate_delta,\n",
        "    }\n",
        "    # Iterate and check each parameter for strict positivity.\n",
        "    for name, value in positive_params.items():\n",
        "        if not value > 0:\n",
        "            raise ValueError(\n",
        "                f\"Parameter '{name}' must be strictly positive, but got {value}.\"\n",
        "            )\n",
        "\n",
        "    # --- Validate parameters from the raw config used for deriving r ---\n",
        "    # These are checked from the raw source to ensure fundamental validity.\n",
        "    r_primitives = raw_config['model_parameters']['household_economic_parameters']['primitive_params_for_r']\n",
        "\n",
        "    # Parameter 'c' (savings_conversion_rate_c) must be strictly positive.\n",
        "    c = r_primitives['savings_conversion_rate_c']\n",
        "    if not c > 0:\n",
        "        raise ValueError(\n",
        "            \"Parameter 'savings_conversion_rate_c' must be strictly positive, \"\n",
        "            f\"but got {c}.\"\n",
        "        )\n",
        "\n",
        "    # Parameter 'a' (marginal_propensity_consume_a) must be in the open interval (0, 1).\n",
        "    # This ensures both consumption and savings occur from marginal income above the poverty line.\n",
        "    a = r_primitives['marginal_propensity_consume_a']\n",
        "    if not (0 < a < 1):\n",
        "        raise ValueError(\n",
        "            \"Parameter 'marginal_propensity_consume_a' must be in the open \"\n",
        "            f\"interval (0, 1), but got {a}.\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 2: Helper function to validate the loss distribution\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_loss_distribution(\n",
        "    loss_dist: LossDistributionParams\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the loss distribution parameters and mean consistency.\n",
        "\n",
        "    This function checks that the specified loss distribution (G_Z) is\n",
        "    supported, its parameters are valid, and the pre-computed mean (μ) is\n",
        "    consistent with the theoretical mean derived from its parameters.\n",
        "\n",
        "    Args:\n",
        "        loss_dist: The dataclass containing loss distribution information.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If the distribution name is not supported.\n",
        "        ValueError: If distribution parameters are invalid or the stored mean\n",
        "                    is inconsistent with the theoretical mean.\n",
        "    \"\"\"\n",
        "    # --- Validate that the mean μ = E[Z] is in (0, 1) ---\n",
        "    # Z is a remaining proportion, so μ cannot be 0 or 1 in this model.\n",
        "    if not (0 < loss_dist.mean_E_Z_mu < 1):\n",
        "        raise ValueError(\n",
        "            \"The mean of the loss distribution 'mean_E_Z_mu' must be in the \"\n",
        "            f\"open interval (0, 1), but got {loss_dist.mean_E_Z_mu}.\"\n",
        "        )\n",
        "\n",
        "    # --- Perform distribution-specific validation ---\n",
        "    if loss_dist.name == \"Beta\":\n",
        "        # For Beta(α, β), require α > 0 and β > 0.\n",
        "        alpha = loss_dist.parameters.get(\"alpha\")\n",
        "        beta = loss_dist.parameters.get(\"beta\")\n",
        "        if alpha is None or beta is None or not (alpha > 0 and beta > 0):\n",
        "            raise ValueError(\n",
        "                \"For a 'Beta' distribution, parameters 'alpha' and 'beta' must \"\n",
        "                f\"be provided and be strictly positive. Got: {loss_dist.parameters}\"\n",
        "            )\n",
        "\n",
        "        # Re-compute the theoretical mean to verify consistency.\n",
        "        # Formula for Beta(α, β) mean: μ = α / (α + β)\n",
        "        theoretical_mu = alpha / (alpha + beta)\n",
        "\n",
        "        # Compare computed mean with the stored mean using a tight tolerance.\n",
        "        if not math.isclose(theoretical_mu, loss_dist.mean_E_Z_mu, rel_tol=1e-9):\n",
        "            raise ValueError(\n",
        "                \"Inconsistency in 'Beta' distribution parameters. The stored \"\n",
        "                f\"'mean_E_Z_mu' is {loss_dist.mean_E_Z_mu}, but the theoretical \"\n",
        "                f\"mean computed from alpha={alpha} and beta={beta} is \"\n",
        "                f\"{theoretical_mu}.\"\n",
        "            )\n",
        "    elif loss_dist.name == \"Kumaraswamy\":\n",
        "        # For Kumaraswamy(p, q), require p > 0 and q > 0.\n",
        "        p = loss_dist.parameters.get(\"p\")\n",
        "        q = loss_dist.parameters.get(\"q\")\n",
        "        if p is None or q is None or not (p > 0 and q > 0):\n",
        "            raise ValueError(\n",
        "                \"For a 'Kumaraswamy' distribution, parameters 'p' and 'q' must \"\n",
        "                f\"be provided and be strictly positive. Got: {loss_dist.parameters}\"\n",
        "            )\n",
        "\n",
        "        # Re-compute the theoretical mean to verify consistency.\n",
        "        # Formula for Kumaraswamy(p, q) mean: μ = q * B(1 + 1/p, q), where B is the Beta function.\n",
        "        theoretical_mu = q * beta_function(1 + 1/p, q)\n",
        "\n",
        "        # Compare computed mean with the stored mean.\n",
        "        if not math.isclose(theoretical_mu, loss_dist.mean_E_Z_mu, rel_tol=1e-9):\n",
        "            raise ValueError(\n",
        "                \"Inconsistency in 'Kumaraswamy' distribution parameters. The stored \"\n",
        "                f\"'mean_E_Z_mu' is {loss_dist.mean_E_Z_mu}, but the theoretical \"\n",
        "                f\"mean computed from p={p} and q={q} is {theoretical_mu}.\"\n",
        "            )\n",
        "    else:\n",
        "        # If the distribution is not recognized, raise an error.\n",
        "        raise NotImplementedError(\n",
        "            f\"Validation for loss distribution '{loss_dist.name}' is not \"\n",
        "            \"implemented. Supported distributions: ['Beta', 'Kumaraswamy'].\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Step 3: Helper function to validate microinsurance parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _get_loss_pdf(\n",
        "    loss_dist: LossDistributionParams\n",
        ") -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Creates and returns a callable probability density function (PDF).\n",
        "\n",
        "    This factory function inspects the provided loss distribution parameters\n",
        "    and returns a callable function that computes the probability density\n",
        "    f_Z(z) for the specified distribution. This is a crucial utility for\n",
        "    numerical integration tasks, such as calculating the insurance premium for\n",
        "    non-proportional coverage types.\n",
        "\n",
        "    Process:\n",
        "    1.  Inspects the `name` attribute of the `loss_dist` object.\n",
        "    2.  Based on the name, it retrieves the necessary parameters (e.g., 'alpha',\n",
        "        'beta' for the Beta distribution).\n",
        "    3.  It constructs and returns a lambda function that wraps a robust PDF\n",
        "        implementation from the `scipy.stats` library, parameterized with the\n",
        "        retrieved values.\n",
        "\n",
        "    Args:\n",
        "        loss_dist: A `LossDistributionParams` dataclass instance containing the\n",
        "                   name and parameters of the loss distribution G_Z.\n",
        "\n",
        "    Returns:\n",
        "        A callable function that takes a single float `z` (a value of the\n",
        "        random variable Z) and returns its probability density as a float.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If the `name` of the distribution in `loss_dist`\n",
        "                             is not one of the supported types (e.g., 'Beta',\n",
        "                             'Kumaraswamy').\n",
        "        KeyError: If the `parameters` dictionary within `loss_dist` is missing\n",
        "                  a required key for the specified distribution name.\n",
        "    \"\"\"\n",
        "    # Check if the specified distribution is the Beta distribution.\n",
        "    if loss_dist.name == \"Beta\":\n",
        "        # Retrieve the 'alpha' shape parameter from the parameters dictionary.\n",
        "        alpha = loss_dist.parameters[\"alpha\"]\n",
        "\n",
        "        # Retrieve the 'beta' shape parameter from the parameters dictionary.\n",
        "        beta = loss_dist.parameters[\"beta\"]\n",
        "\n",
        "        # Return a callable PDF using the numerically stable implementation from scipy.stats.\n",
        "        return lambda z: scipy.stats.beta.pdf(z, a=alpha, b=beta)\n",
        "\n",
        "    # Check if the specified distribution is the Kumaraswamy distribution.\n",
        "    elif loss_dist.name == \"Kumaraswamy\":\n",
        "        # Retrieve the 'p' shape parameter (often denoted 'a' in SciPy) from the parameters.\n",
        "        p = loss_dist.parameters[\"p\"]\n",
        "\n",
        "        # Retrieve the 'q' shape parameter (often denoted 'b' in SciPy) from the parameters.\n",
        "        q = loss_dist.parameters[\"q\"]\n",
        "\n",
        "        # Return a callable PDF using the scipy.stats implementation.\n",
        "        return lambda z: scipy.stats.kumaraswamy.pdf(z, a=p, b=q)\n",
        "\n",
        "    # If the distribution name is not recognized, raise an error.\n",
        "    # This makes the function extensible while ensuring unsupported types fail fast.\n",
        "    raise NotImplementedError(\n",
        "        f\"PDF creation for loss distribution '{loss_dist.name}' is not implemented.\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _validate_microinsurance_params(\n",
        "    microinsurance: 'MicroinsuranceParams',\n",
        "    model_params: 'ModelParameters'\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates microinsurance parameters and the critical feasibility constraint.\n",
        "\n",
        "    This function provides a complete, production-grade validation of the\n",
        "    microinsurance configuration. If insurance is active, it performs a sequence\n",
        "    of rigorous checks:\n",
        "    1.  Validates common parameters (e.g., safety loading `γ`).\n",
        "    2.  Performs policy-specific parameter validation (e.g., `η` for Proportional).\n",
        "    3.  Re-computes the insurance premium `p_R` from first principles, using\n",
        "        analytical formulas or robust numerical integration where necessary.\n",
        "    4.  Checks the accuracy of the numerical integration and issues a warning if\n",
        "        the estimated error is too high.\n",
        "    5.  Enforces the critical model feasibility constraint `b > p_R`, which is\n",
        "        necessary to prevent a non-positive or infinite transformed poverty line.\n",
        "\n",
        "    Args:\n",
        "        microinsurance: The parsed dataclass containing all microinsurance settings.\n",
        "        model_params: The parsed dataclass with core model parameters, needed for\n",
        "                      accessing `b`, `λ`, `μ`, and the loss distribution PDF.\n",
        "\n",
        "    Returns:\n",
        "        None. The function returns successfully if all validations pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any insurance parameter is invalid or if the feasibility\n",
        "                    constraint `b > p_R` is violated.\n",
        "        NotImplementedError: If the configuration specifies an unsupported\n",
        "                             insurance type.\n",
        "        RuntimeWarning: If the numerical integration for premium calculation\n",
        "                        yields a high estimated error.\n",
        "    \"\"\"\n",
        "    # --- Step 0: Conditional Execution ---\n",
        "    # If microinsurance is not active in the configuration, no validation is needed.\n",
        "    if not microinsurance.is_active:\n",
        "        return\n",
        "\n",
        "    # --- Step 1: Common Parameter Validation ---\n",
        "    # The insurer's safety loading gamma (γ) must be strictly positive.\n",
        "    gamma = microinsurance.insurer_safety_loading_gamma\n",
        "    if not gamma > 0:\n",
        "        raise ValueError(\n",
        "            \"Parameter 'insurer_safety_loading_gamma' (γ) must be strictly \"\n",
        "            f\"positive, but got {gamma}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Polymorphic Premium Calculation and Policy Validation ---\n",
        "    # Retrieve shared parameters needed for all premium calculations.\n",
        "    lambda_ = model_params.stochastic_shock_parameters.shock_frequency_lambda\n",
        "    mu = model_params.stochastic_shock_parameters.loss_distribution_G_Z.mean_E_Z_mu\n",
        "    computed_p_R = 0.0\n",
        "\n",
        "    # Select logic based on the canonical insurance type.\n",
        "    if microinsurance.insurance_type == \"Proportional\":\n",
        "        # For Proportional insurance, the retention parameter eta (η) must be in [0, 1].\n",
        "        eta = microinsurance.policy_parameters.proportional_retention_eta\n",
        "        if eta is None or not (0 <= eta <= 1):\n",
        "            raise ValueError(\n",
        "                \"For 'Proportional' insurance, 'proportional_retention_eta' (η) \"\n",
        "                f\"must be a value in the closed interval [0, 1], but got {eta}.\"\n",
        "            )\n",
        "\n",
        "        # Equation (7.8): p_R = (1 + γ) * λ * (1 - η) * (1 - μ)\n",
        "        # This is the analytical formula for the premium in the proportional case.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * (1 - eta) * (1 - mu)\n",
        "\n",
        "    elif microinsurance.insurance_type == \"XL\":\n",
        "        # For Excess-of-Loss (XL) insurance, the retention limit l must be in [0, 1].\n",
        "        l = microinsurance.policy_parameters.xl_retention_l\n",
        "        if l is None or not (0 <= l <= 1):\n",
        "            raise ValueError(\n",
        "                \"For 'XL' insurance, 'xl_retention_l' (l) must be a value \"\n",
        "                f\"in the closed interval [0, 1], but got {l}.\"\n",
        "            )\n",
        "\n",
        "        # The premium requires numerical integration of the expected ceded loss.\n",
        "        # Ceded loss is (u - l) for u > l, where u = 1 - z. This means z < 1 - l.\n",
        "        # The amount is (1 - z - l).\n",
        "        # Equation: p_R = (1 + γ) * λ * E[(1 - Z - l)^+] = (1 + γ) * λ * ∫[0 to 1-l] (1 - z - l) * f_Z(z) dz\n",
        "        pdf = _get_loss_pdf(model_params.stochastic_shock_parameters.loss_distribution_G_Z)\n",
        "        integrand = lambda z: (1 - z - l) * pdf(z)\n",
        "\n",
        "        # Perform the numerical integration using SciPy's robust quad function.\n",
        "        integral, error_est = scipy.integrate.quad(integrand, 0, 1 - l)\n",
        "\n",
        "        # Check the accuracy of the numerical integration.\n",
        "        if error_est > 1e-7:\n",
        "            warnings.warn(\n",
        "                f\"Numerical integration for 'XL' premium calculation yielded a \"\n",
        "                f\"high absolute error estimate of {error_est:.2e}. The resulting \"\n",
        "                f\"feasibility check may be unreliable.\",\n",
        "                RuntimeWarning\n",
        "            )\n",
        "\n",
        "        # Calculate the final premium.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * integral\n",
        "\n",
        "    elif microinsurance.insurance_type == \"TotalLoss\":\n",
        "        # For Total-Loss insurance, the loss trigger L must be in [0, 1].\n",
        "        L = microinsurance.policy_parameters.total_loss_L\n",
        "        if L is None or not (0 <= L <= 1):\n",
        "            raise ValueError(\n",
        "                \"For 'TotalLoss' insurance, 'total_loss_L' (L) must be a value \"\n",
        "                f\"in the closed interval [0, 1], but got {L}.\"\n",
        "            )\n",
        "\n",
        "        # The premium requires numerical integration of the expected ceded loss.\n",
        "        # Ceded loss is u = 1 - z for u > L, which means z < 1 - L.\n",
        "        # Equation: p_R = (1 + γ) * λ * E[(1 - Z) * 1_{Z < 1-L}] = (1 + γ) * λ * ∫[0 to 1-L] (1 - z) * f_Z(z) dz\n",
        "        pdf = _get_loss_pdf(model_params.stochastic_shock_parameters.loss_distribution_G_Z)\n",
        "        integrand = lambda z: (1 - z) * pdf(z)\n",
        "\n",
        "        # Perform the numerical integration.\n",
        "        integral, error_est = scipy.integrate.quad(integrand, 0, 1 - L)\n",
        "\n",
        "        # Check the accuracy of the numerical integration.\n",
        "        if error_est > 1e-7:\n",
        "            warnings.warn(\n",
        "                f\"Numerical integration for 'TotalLoss' premium calculation yielded a \"\n",
        "                f\"high absolute error estimate of {error_est:.2e}. The resulting \"\n",
        "                f\"feasibility check may be unreliable.\",\n",
        "                RuntimeWarning\n",
        "            )\n",
        "\n",
        "        # Calculate the final premium.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * integral\n",
        "\n",
        "    else:\n",
        "        # If the insurance type is not recognized, raise an error.\n",
        "        raise NotImplementedError(\n",
        "            f\"Validation for insurance type '{microinsurance.insurance_type}' \"\n",
        "            \"is not implemented. Supported types: ['Proportional', 'XL', 'TotalLoss'].\"\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Critical Feasibility Check: b > p_R ---\n",
        "    # This constraint is required for the transformed poverty line x*^R to be finite and positive.\n",
        "    # If b <= p_R, the household's net income rate is non-positive, making the model ill-defined.\n",
        "    b = model_params.household_economic_parameters.primitive_params_for_r.income_generation_rate_b\n",
        "    if not b > computed_p_R:\n",
        "        raise ValueError(\n",
        "            f\"Microinsurance feasibility constraint failed for type \"\n",
        "            f\"'{microinsurance.insurance_type}'. The income generation rate 'b' ({b}) \"\n",
        "            f\"must be strictly greater than the re-computed premium 'p_R' \"\n",
        "            f\"({computed_p_R:.6f}). With these parameters, the household's net \"\n",
        "            f\"income is non-positive, and the model is ill-defined.\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 2, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def validate_parameters(\n",
        "    parsed_config: ParsedStudyConfig,\n",
        "    raw_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all model and insurance parameters.\n",
        "\n",
        "    This function serves as the main validator for the parsed configuration.\n",
        "    It executes a sequence of checks on the economic primitives, the loss\n",
        "    distribution specification, and the microinsurance settings to ensure\n",
        "    the entire parameter set is mathematically consistent, economically sound,\n",
        "    and computationally feasible according to the model's theoretical\n",
        "    constraints from the research context. This is a production-grade\n",
        "    implementation that is complete and handles all specified model variations.\n",
        "\n",
        "    Process:\n",
        "    1.  Validates core economic primitives for positivity and domain constraints\n",
        "        (e.g., x* > 0, 0 < a < 1).\n",
        "    2.  Validates the loss distribution G_Z, checking its parameters and\n",
        "        ensuring the stored mean μ is consistent with its theoretical value for\n",
        "        all supported distributions (Beta, Kumaraswamy).\n",
        "    3.  If microinsurance is active, validates its policy parameters and, most\n",
        "        importantly, re-computes the premium p_R from scratch (using numerical\n",
        "        integration where necessary) to verify the critical feasibility\n",
        "        constraint (b > p_R) that ensures the model remains well-posed.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The strongly-typed `ParsedStudyConfig` object from Task 1.\n",
        "        raw_config: The original raw configuration dictionary, required for\n",
        "                    accessing certain primitive parameters for validation.\n",
        "\n",
        "    Returns:\n",
        "        None. The function returns successfully if all validations pass.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any parameter or derived quantity violates its\n",
        "                    required constraints.\n",
        "        NotImplementedError: If the configuration specifies an unsupported\n",
        "                             distribution or insurance type.\n",
        "    \"\"\"\n",
        "    # Extract the relevant dataclass instance for easier access.\n",
        "    model_params = parsed_config.model_params\n",
        "\n",
        "    # Step 1: Validate the fundamental economic parameters.\n",
        "    # This check ensures the basic economic model is well-defined.\n",
        "    _validate_economic_primitives(\n",
        "        model_params=model_params, raw_config=raw_config\n",
        "    )\n",
        "\n",
        "    # Step 2: Validate the stochastic shock (loss) distribution.\n",
        "    # This ensures the loss model is correctly specified and consistent.\n",
        "    _validate_loss_distribution(loss_dist=model_params.loss_distribution)\n",
        "\n",
        "    # Step 3: Validate the microinsurance component, if it is active.\n",
        "    # This includes the critical feasibility check for the premium, now fully\n",
        "    # implemented for all specified insurance types.\n",
        "    _validate_microinsurance_params(\n",
        "        microinsurance=model_params.microinsurance, model_params=model_params\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OtQFDVjD_RAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3 — Cleanse and normalize inputs\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 3: Cleanse and normalize inputs\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 1: Helper function to normalize string enumerations\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _normalize_enumerations(\n",
        "    config: ParsedStudyConfig\n",
        ") -> ParsedStudyConfig:\n",
        "    \"\"\"\n",
        "    Normalizes string-based enumeration fields to their canonical form.\n",
        "\n",
        "    This function ensures that configuration fields representing a choice from a\n",
        "    fixed set (e.g., insurance type) are converted to a standardized,\n",
        "    canonical representation. It performs case-insensitive matching and raises\n",
        "    an error for unrecognized values, preventing ambiguity in downstream logic.\n",
        "\n",
        "    Args:\n",
        "        config: The parsed study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        A new `ParsedStudyConfig` instance with enumeration fields normalized.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a string field contains an unrecognized value.\n",
        "    \"\"\"\n",
        "    # --- Normalize insurance_type ---\n",
        "    # Define the set of canonical, valid insurance types.\n",
        "    VALID_INSURANCE_TYPES: Set[str] = {'Proportional', 'XL', 'TotalLoss'}\n",
        "\n",
        "    # Get the raw insurance type string from the config.\n",
        "    raw_insurance_type = config.model_params.microinsurance.insurance_type\n",
        "\n",
        "    # Normalize the string: remove leading/trailing whitespace and convert to Title Case.\n",
        "    normalized_insurance_type = raw_insurance_type.strip().title()\n",
        "\n",
        "    # Check if the normalized string is in the set of valid types.\n",
        "    if normalized_insurance_type not in VALID_INSURANCE_TYPES:\n",
        "        raise ValueError(\n",
        "            f\"Invalid 'insurance_type' specified: '{raw_insurance_type}'. \"\n",
        "            f\"Must be one of {sorted(list(VALID_INSURANCE_TYPES))}.\"\n",
        "        )\n",
        "\n",
        "    # --- Normalize algorithm_to_use ---\n",
        "    # Define the set of canonical, valid algorithm choices.\n",
        "    VALID_ALGORITHMS: Set[str] = {'Auto', 'ClosedForm', 'MonteCarlo', 'FixedPoint'}\n",
        "\n",
        "    # Get the raw algorithm string from the config.\n",
        "    raw_algorithm = config.computation_params.algorithm_to_use\n",
        "\n",
        "    # Normalize the string.\n",
        "    normalized_algorithm = raw_algorithm.strip().title()\n",
        "\n",
        "    # Check if the normalized string is a valid algorithm choice.\n",
        "    if normalized_algorithm not in VALID_ALGORITHMS:\n",
        "        raise ValueError(\n",
        "            f\"Invalid 'algorithm_to_use' specified: '{raw_algorithm}'. \"\n",
        "            f\"Must be one of {sorted(list(VALID_ALGORITHMS))}.\"\n",
        "        )\n",
        "\n",
        "    # --- Create a new config object with the normalized values ---\n",
        "    # Use dataclasses.replace for safe, immutable updates.\n",
        "\n",
        "    # Create a new MicroinsuranceParams with the normalized type.\n",
        "    updated_microinsurance = replace(\n",
        "        config.model_params.microinsurance,\n",
        "        insurance_type=normalized_insurance_type\n",
        "    )\n",
        "\n",
        "    # Create a new ModelParameters with the updated microinsurance block.\n",
        "    updated_model_params = replace(\n",
        "        config.model_params,\n",
        "        microinsurance=updated_microinsurance\n",
        "    )\n",
        "\n",
        "    # Create a new ComputationParameters with the normalized algorithm.\n",
        "    updated_computation_params = replace(\n",
        "        config.computation_params,\n",
        "        algorithm_to_use=normalized_algorithm\n",
        "    )\n",
        "\n",
        "    # Create the final, top-level config object with all updates.\n",
        "    final_config = replace(\n",
        "        config,\n",
        "        model_params=updated_model_params,\n",
        "        computation_params=updated_computation_params\n",
        "    )\n",
        "\n",
        "    return final_config\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Step 2: Helper function to prune unused optional fields\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _prune_unused_policy_fields(\n",
        "    config: ParsedStudyConfig\n",
        ") -> ParsedStudyConfig:\n",
        "    \"\"\"\n",
        "    Removes unused optional microinsurance fields based on the policy type.\n",
        "\n",
        "    To ensure logical consistency and prevent accidental use of stale parameters,\n",
        "    this function sets the parameters of inactive policy types to `None`. For\n",
        "    example, if the insurance type is 'Proportional', the parameters for 'XL'\n",
        "    and 'TotalLoss' are nulled.\n",
        "\n",
        "    Args:\n",
        "        config: The study configuration object, assumed to have normalized\n",
        "                enumerations.\n",
        "\n",
        "    Returns:\n",
        "        A new `ParsedStudyConfig` instance with irrelevant fields pruned.\n",
        "    \"\"\"\n",
        "    # Get the canonical insurance type.\n",
        "    insurance_type = config.model_params.microinsurance.insurance_type\n",
        "\n",
        "    # Get the current policy parameters.\n",
        "    policy_params = config.model_params.microinsurance.policy_parameters\n",
        "\n",
        "    # Initialize updated parameters with current values.\n",
        "    updated_eta = policy_params.proportional_retention_eta\n",
        "    updated_l = policy_params.xl_retention_l\n",
        "    updated_L = policy_params.total_loss_L\n",
        "\n",
        "    # Conditionally nullify parameters of inactive policy types.\n",
        "    if insurance_type == 'Proportional':\n",
        "        # If policy is Proportional, nullify XL and TotalLoss parameters.\n",
        "        updated_l = None\n",
        "        updated_L = None\n",
        "    elif insurance_type == 'XL':\n",
        "        # If policy is XL, nullify Proportional and TotalLoss parameters.\n",
        "        updated_eta = None\n",
        "        updated_L = None\n",
        "    elif insurance_type == 'TotalLoss':\n",
        "        # If policy is TotalLoss, nullify Proportional and XL parameters.\n",
        "        updated_eta = None\n",
        "        updated_l = None\n",
        "\n",
        "    # Create a new, cleansed policy parameters object.\n",
        "    updated_policy_params = replace(\n",
        "        policy_params,\n",
        "        proportional_retention_eta=updated_eta,\n",
        "        xl_retention_l=updated_l,\n",
        "        total_loss_L=updated_L\n",
        "    )\n",
        "\n",
        "    # Create a new microinsurance object with the cleansed policy.\n",
        "    updated_microinsurance = replace(\n",
        "        config.model_params.microinsurance,\n",
        "        policy_parameters=updated_policy_params\n",
        "    )\n",
        "\n",
        "    # Create a new model parameters object.\n",
        "    updated_model_params = replace(\n",
        "        config.model_params,\n",
        "        microinsurance=updated_microinsurance\n",
        "    )\n",
        "\n",
        "    # Return the final, cleansed configuration object.\n",
        "    return replace(config, model_params=updated_model_params)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 3, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cleanse_and_normalize_config(\n",
        "    parsed_config: ParsedStudyConfig\n",
        ") -> ParsedStudyConfig:\n",
        "    \"\"\"\n",
        "    Orchestrates the cleansing and normalization of a parsed configuration.\n",
        "\n",
        "    This function prepares the configuration for the core computational engine\n",
        "    by performing a sequence of sanitization steps. It ensures that the final\n",
        "    configuration object is unambiguous, logically consistent, and robust\n",
        "    against common input errors like typos or inconsistent optional parameters.\n",
        "\n",
        "    Process:\n",
        "    1.  **Normalize Enumerations:** Converts string fields like `insurance_type`\n",
        "        and `algorithm_to_use` to their canonical, case-sensitive forms (e.g.,\n",
        "        \"proportional\" -> \"Proportional\"). This simplifies all downstream\n",
        "        conditional logic.\n",
        "    2.  **Prune Unused Fields:** Sets optional parameters for inactive policy\n",
        "        types to `None`. This prevents the accidental use of stale or\n",
        "        irrelevant parameter values from the configuration file.\n",
        "\n",
        "    Note on Rounding:\n",
        "    Step 3 from the high-level task list (\"Round derived fields\") is\n",
        "    intentionally omitted. Rounding pre-computed derived fields from the input\n",
        "    is methodologically unsound. The pipeline should instead re-compute all\n",
        "    derived quantities from primitives (in Tasks 4 and 5) to ensure maximum\n",
        "    precision and fidelity. Rounding should only be applied at the final\n",
        "    reporting stage.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The `ParsedStudyConfig` object produced by the initial\n",
        "                       ingestion and parsing (Task 1).\n",
        "\n",
        "    Returns:\n",
        "        A new, cleansed, and normalized `ParsedStudyConfig` instance that is\n",
        "        ready for the core computational stages of the pipeline.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any enumeration string is not a recognized value.\n",
        "    \"\"\"\n",
        "    # Step 1: Normalize all string-based enumeration fields.\n",
        "    # This creates a new config object with canonical string values.\n",
        "    normalized_config = _normalize_enumerations(config=parsed_config)\n",
        "\n",
        "    # Step 2: Prune unused optional parameters from the insurance policy block.\n",
        "    # This ensures logical consistency within the configuration.\n",
        "    cleansed_config = _prune_unused_policy_fields(config=normalized_config)\n",
        "\n",
        "    # Return the fully cleansed and normalized configuration object.\n",
        "    return cleansed_config\n"
      ],
      "metadata": {
        "id": "XHFSBL_EFS0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4 — Compute base derived quantities (no insurance)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 4: Compute base derived quantities (no insurance)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class PolicyComparatorBoundary:\n",
        "    \"\"\"\n",
        "    Immutable container for the policy comparator boundary diagnostic.\n",
        "\n",
        "    This object stores the result of the check from Proposition 2.2 of the\n",
        "    research context. This proposition provides a simple condition to determine\n",
        "    whether lump-sum capital injections (cost function C(x)) are more\n",
        "    cost-effective at the poverty line than a strategy of perpetual regular\n",
        "    transfers (cost function D(x)). The comparison is crucial for understanding\n",
        "    the fundamental trade-offs in the social protection strategy.\n",
        "\n",
        "    The condition is: D(x*) >= C(x*) if and only if b >= δ + λ(1 - μ).\n",
        "\n",
        "    Attributes:\n",
        "        boundary_value: The numerical result of the expression b - (δ + λ(1 - μ)).\n",
        "                        A non-negative value (>= 0) indicates that the condition\n",
        "                        holds and lump-sum transfers are preferred at x*.\n",
        "        lump_sum_preferred: A boolean flag that is True if `boundary_value` is\n",
        "                            non-negative, providing a clear, interpretable\n",
        "                            summary of the policy preference.\n",
        "    \"\"\"\n",
        "    # The numerical result of b - (δ + λ(1 - μ)).\n",
        "    boundary_value: float\n",
        "\n",
        "    # A boolean flag, True if boundary_value >= 0, indicating lump-sum transfers are preferred.\n",
        "    lump_sum_preferred: bool\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 1: Helper function to compute capital growth rate\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_capital_growth_rate(\n",
        "    raw_config: Dict[str, Any]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the capital growth rate 'r' from primitive parameters.\n",
        "\n",
        "    This function implements the formula from Section 2 of the research context,\n",
        "    deriving the net capital growth rate for a household with capital above the\n",
        "    poverty line. It also validates this computed value against the one stored\n",
        "    in the configuration file as a critical consistency check.\n",
        "\n",
        "    Args:\n",
        "        raw_config: The original raw configuration dictionary, used to access\n",
        "                    the primitive parameters directly.\n",
        "\n",
        "    Returns:\n",
        "        The computed capital growth rate 'r' as a high-precision float.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the re-computed 'r' is inconsistent with the value\n",
        "                    stored in the configuration file.\n",
        "        KeyError: If primitive parameters are missing from the config.\n",
        "    \"\"\"\n",
        "    # Access the dictionary containing the primitive parameters for 'r'.\n",
        "    r_primitives = raw_config['model_parameters']['household_economic_parameters']['primitive_params_for_r']\n",
        "\n",
        "    # Extract primitive parameters a, b, and c.\n",
        "    a = r_primitives['marginal_propensity_consume_a']\n",
        "    b = raw_config['model_parameters']['household_economic_parameters']['income_generation_rate_b']\n",
        "    c = r_primitives['savings_conversion_rate_c']\n",
        "\n",
        "    # Equation from Section 2: r = (1 - a) * b * c\n",
        "    # Compute the growth rate 'r' from the primitive economic parameters.\n",
        "    computed_r = (1 - a) * b * c\n",
        "\n",
        "    # --- Validation Step ---\n",
        "    # Retrieve the derived growth rate stored in the configuration for comparison.\n",
        "    stored_r = raw_config['model_parameters']['household_economic_parameters']['derived_growth_rate_r']\n",
        "\n",
        "    # Verify that the newly computed 'r' matches the stored value within a tight tolerance.\n",
        "    # This ensures the configuration file is internally consistent.\n",
        "    if not math.isclose(computed_r, stored_r, rel_tol=1e-9):\n",
        "        raise ValueError(\n",
        "            \"Inconsistency in capital growth rate 'r'. The value stored in the \"\n",
        "            f\"config is {stored_r}, but the value computed from primitives \"\n",
        "            f\"(a, b, c) is {computed_r}. Please correct the configuration file.\"\n",
        "        )\n",
        "\n",
        "    # Return the high-precision, validated growth rate.\n",
        "    return computed_r\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 2: Helper function to compute expected remaining capital\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_expected_remaining_capital(\n",
        "    loss_dist: LossDistributionParams\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the expected remaining capital fraction 'μ' from distribution params.\n",
        "\n",
        "    This function calculates the theoretical mean (μ = E[Z]) of the specified\n",
        "    loss distribution. It is polymorphic, selecting the correct formula based\n",
        "    on the distribution's name. It also validates this computed value against\n",
        "    the one stored in the configuration.\n",
        "\n",
        "    Args:\n",
        "        loss_dist: The dataclass containing the loss distribution specification.\n",
        "\n",
        "    Returns:\n",
        "        The computed expected remaining capital fraction 'μ' as a float.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the re-computed 'μ' is inconsistent with the stored value.\n",
        "        NotImplementedError: If the distribution name is not supported.\n",
        "    \"\"\"\n",
        "    # Initialize the variable for the computed mean.\n",
        "    computed_mu = 0.0\n",
        "\n",
        "    # Select the correct mean formula based on the distribution name.\n",
        "    if loss_dist.name == \"Beta\":\n",
        "        # Extract parameters for the Beta distribution.\n",
        "        alpha = loss_dist.parameters[\"alpha\"]\n",
        "        beta = loss_dist.parameters[\"beta\"]\n",
        "\n",
        "        # Theoretical mean for Beta(α, β): μ = α / (α + β)\n",
        "        computed_mu = alpha / (alpha + beta)\n",
        "\n",
        "    elif loss_dist.name == \"Kumaraswamy\":\n",
        "        # Extract parameters for the Kumaraswamy distribution.\n",
        "        p = loss_dist.parameters[\"p\"]\n",
        "        q = loss_dist.parameters[\"q\"]\n",
        "\n",
        "        # Theoretical mean for Kumaraswamy(p, q): μ = q * B(1 + 1/p, q)\n",
        "        # where B is the complete Beta function.\n",
        "        computed_mu = q * beta_function(1 + 1/p, q)\n",
        "\n",
        "    else:\n",
        "        # If the distribution is not recognized, raise an error.\n",
        "        raise NotImplementedError(\n",
        "            f\"Mean computation for loss distribution '{loss_dist.name}' is not \"\n",
        "            \"implemented. Supported distributions: ['Beta', 'Kumaraswamy'].\"\n",
        "        )\n",
        "\n",
        "    # --- Validation Step ---\n",
        "    # Verify that the newly computed 'μ' matches the stored value.\n",
        "    if not math.isclose(computed_mu, loss_dist.mean_E_Z_mu, rel_tol=1e-9):\n",
        "        raise ValueError(\n",
        "            f\"Inconsistency in loss distribution mean 'μ' for '{loss_dist.name}'. \"\n",
        "            f\"The stored 'mean_E_Z_mu' is {loss_dist.mean_E_Z_mu}, but the \"\n",
        "            f\"theoretical mean computed from its parameters is {computed_mu}.\"\n",
        "        )\n",
        "\n",
        "    # Return the high-precision, validated mean.\n",
        "    return computed_mu\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Step 3: Helper function to compute the policy comparator boundary\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_policy_comparator_boundary(\n",
        "    b: float,\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    mu: float\n",
        ") -> PolicyComparatorBoundary:\n",
        "    \"\"\"\n",
        "    Computes the policy comparator boundary diagnostic from Proposition 2.2.\n",
        "\n",
        "    This function evaluates whether lump-sum transfers are more cost-effective\n",
        "    than perpetual regular transfers at the poverty line by checking the sign of\n",
        "    b - (δ + λ(1 - μ)).\n",
        "\n",
        "    Args:\n",
        "        b: The income generation rate.\n",
        "        delta: The continuous-time discount rate.\n",
        "        lambda_: The Poisson arrival rate of shocks.\n",
        "        mu: The computed expected remaining capital fraction (E[Z]).\n",
        "\n",
        "    Returns:\n",
        "        A `PolicyComparatorBoundary` object containing the numerical boundary\n",
        "        value and a boolean flag indicating the preferred policy.\n",
        "    \"\"\"\n",
        "    # Proposition 2.2: D(x*) - C(x*) >= 0 if and only if b >= δ + λ(1 - μ)\n",
        "    # We compute the difference: b - (δ + λ(1 - μ))\n",
        "    boundary_value = b - (delta + lambda_ * (1 - mu))\n",
        "\n",
        "    # The preference for lump-sum transfers holds if the boundary value is non-negative.\n",
        "    lump_sum_is_preferred = boundary_value >= 0\n",
        "\n",
        "    # Return the result in a structured, immutable dataclass.\n",
        "    return PolicyComparatorBoundary(\n",
        "        boundary_value=boundary_value,\n",
        "        lump_sum_preferred=lump_sum_is_preferred\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 4, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class BaseDerivedQuantities:\n",
        "    \"\"\"\n",
        "    Immutable container for all base derived quantities (uninsured case).\n",
        "\n",
        "    This class aggregates the fundamental parameters of the baseline economic\n",
        "    model, computed directly from the primitive inputs. Storing these values\n",
        "    in a dedicated, immutable object ensures that all downstream functions\n",
        "    operate on a consistent, validated, and high-precision set of parameters,\n",
        "    which is essential for reproducible and accurate results.\n",
        "\n",
        "    Attributes:\n",
        "        capital_growth_rate_r: The computed net capital growth rate 'r' for\n",
        "                               capital above the poverty line, derived from\n",
        "                               r = (1 - a) * b * c. This is a definitive value.\n",
        "        expected_remaining_capital_mu: The computed theoretical mean 'μ' (E[Z])\n",
        "                                       of the loss distribution G_Z. This is the\n",
        "                                       definitive value of μ for the model.\n",
        "        policy_boundary: An instance of `PolicyComparatorBoundary` containing the\n",
        "                         diagnostic result from Proposition 2.2, which informs\n",
        "                         the choice between lump-sum and perpetual transfers.\n",
        "    \"\"\"\n",
        "    # The computed net capital growth rate 'r'.\n",
        "    capital_growth_rate_r: float\n",
        "\n",
        "    # The computed theoretical mean of the loss distribution, 'μ'.\n",
        "    expected_remaining_capital_mu: float\n",
        "\n",
        "    # The computed policy comparator boundary diagnostic object.\n",
        "    policy_boundary: PolicyComparatorBoundary\n",
        "\n",
        "\n",
        "def compute_base_derived_quantities(\n",
        "    parsed_config: ParsedStudyConfig,\n",
        "    raw_config: Dict[str, Any]\n",
        ") -> BaseDerivedQuantities:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of base derived quantities for the model.\n",
        "\n",
        "    This function computes fundamental derived parameters from the primitive\n",
        "    inputs provided in the configuration. It serves as a critical step that\n",
        "    translates the raw configuration into the core mathematical parameters\n",
        "    of the baseline (uninsured) stochastic model. All computations are performed\n",
        "    from scratch to ensure fidelity and are validated against stored values in\n",
        "    the configuration for consistency.\n",
        "\n",
        "    Process:\n",
        "    1.  **Compute Capital Growth Rate (r):** Derives 'r' from primitives a, b, c.\n",
        "    2.  **Compute Expected Remaining Capital (μ):** Calculates the theoretical\n",
        "        mean of the loss distribution G_Z.\n",
        "    3.  **Compute Policy Comparator Boundary:** Evaluates the condition from\n",
        "        Proposition 2.2 to determine the preferred transfer policy type at the\n",
        "        poverty line.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The strongly-typed `ParsedStudyConfig` object.\n",
        "        raw_config: The original raw configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        A `BaseDerivedQuantities` object containing the computed values for\n",
        "        r, μ, and the policy boundary diagnostic.\n",
        "    \"\"\"\n",
        "    # Step 1: Compute the capital growth rate 'r' from primitives.\n",
        "    # This is the definitive value of 'r' to be used in all downstream calculations.\n",
        "    r = _compute_capital_growth_rate(raw_config=raw_config)\n",
        "\n",
        "    # Step 2: Compute the expected remaining capital fraction 'μ'.\n",
        "    # This is the definitive value of 'μ' for the baseline model.\n",
        "    mu = _compute_expected_remaining_capital(\n",
        "        loss_dist=parsed_config.model_params.loss_distribution\n",
        "    )\n",
        "\n",
        "    # Step 3: Compute the policy comparator boundary diagnostic.\n",
        "    # This uses the definitive computed 'μ' and other model parameters.\n",
        "    policy_boundary = _compute_policy_comparator_boundary(\n",
        "        b=parsed_config.model_params.income_generation_rate_b,\n",
        "        delta=parsed_config.model_params.discount_rate_delta,\n",
        "        lambda_=parsed_config.model_params.shock_frequency_lambda,\n",
        "        mu=mu\n",
        "    )\n",
        "\n",
        "    # Package the computed quantities into a structured, immutable dataclass.\n",
        "    return BaseDerivedQuantities(\n",
        "        capital_growth_rate_r=r,\n",
        "        expected_remaining_capital_mu=mu,\n",
        "        policy_boundary=policy_boundary\n",
        "    )\n"
      ],
      "metadata": {
        "id": "wZIXJhHvIWQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5 — Compute microinsurance-induced transforms (if active)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 5: Compute microinsurance-induced transforms (if active)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InsuranceTransforms:\n",
        "    \"\"\"\n",
        "    Immutable container for all microinsurance-induced transformations.\n",
        "\n",
        "    This object holds the complete set of derived quantities that define the\n",
        "    modified economic model when microinsurance is active. It serves as the\n",
        "    definitive source for all insured-case parameters.\n",
        "\n",
        "    Attributes:\n",
        "        premium_p_R: The computed insurance premium per unit of capital, p_R.\n",
        "        transformed_poverty_line_x_star_R: The adjusted poverty line x*^R,\n",
        "                                           reflecting the impact of premium\n",
        "                                           payments on disposable income.\n",
        "        transformed_growth_rate_r_R: The adjusted capital growth rate r^R,\n",
        "                                     reflecting the impact of premium payments.\n",
        "        mean_post_insurance_loss_E_W: The expected value of the post-insurance\n",
        "                                      remaining capital proportion, E[W].\n",
        "        post_insurance_loss_sampler: A callable function that takes an integer\n",
        "                                     `size` and returns `size` random samples\n",
        "                                     from the post-insurance distribution W.\n",
        "    \"\"\"\n",
        "    # The computed insurance premium per unit of capital, p_R.\n",
        "    premium_p_R: float\n",
        "\n",
        "    # The adjusted poverty line x*^R.\n",
        "    transformed_poverty_line_x_star_R: float\n",
        "\n",
        "    # The adjusted capital growth rate r^R.\n",
        "    transformed_growth_rate_r_R: float\n",
        "\n",
        "    # The expected value of the post-insurance remaining capital proportion, E[W].\n",
        "    mean_post_insurance_loss_E_W: float\n",
        "\n",
        "    # A callable function that generates samples from the post-insurance distribution W.\n",
        "    post_insurance_loss_sampler: Callable[[int], np.ndarray]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 1: Helper function to compute the insurance premium\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_insurance_premium(\n",
        "    model_params: 'ModelParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities'\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the insurance premium p_R from first principles for all policy types.\n",
        "\n",
        "    This function implements the expected value principle with a safety loading\n",
        "    to calculate the premium. It uses analytical formulas where available\n",
        "    (Proportional) and robust numerical integration for more complex policies\n",
        "    (Excess-of-Loss, Total-Loss).\n",
        "\n",
        "    Args:\n",
        "        model_params: The parsed dataclass containing all model parameters.\n",
        "        base_quantities: The dataclass containing base derived quantities (μ).\n",
        "\n",
        "    Returns:\n",
        "        The computed insurance premium p_R as a high-precision float.\n",
        "    \"\"\"\n",
        "    # Extract common parameters for convenience and readability.\n",
        "    microinsurance = model_params.microinsurance_parameters\n",
        "    gamma = microinsurance.insurer_safety_loading_gamma\n",
        "    lambda_ = model_params.stochastic_shock_parameters.shock_frequency_lambda\n",
        "    mu = base_quantities.expected_remaining_capital_mu\n",
        "\n",
        "    # Initialize the premium variable.\n",
        "    computed_p_R = 0.0\n",
        "\n",
        "    # Dispatch to the correct calculation based on the canonical insurance type.\n",
        "    if microinsurance.insurance_type == \"Proportional\":\n",
        "        # Retrieve the policy-specific parameter eta (η).\n",
        "        eta = microinsurance.policy_parameters.proportional_retention_eta\n",
        "\n",
        "        # Equation (7.8): p_R = (1 + γ) * λ * (1 - η) * (1 - μ)\n",
        "        # This is the analytical formula for the premium in the proportional case.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * (1 - eta) * (1 - mu)\n",
        "\n",
        "    elif microinsurance.insurance_type == \"XL\":\n",
        "        # Retrieve the policy-specific parameter l.\n",
        "        l = microinsurance.policy_parameters.xl_retention_l\n",
        "\n",
        "        # The premium is based on the expected ceded loss, which requires numerical integration.\n",
        "        # Ceded loss is (u - l) for u > l, where u = 1 - z. This means z < 1 - l.\n",
        "        # The amount of the ceded loss is (1 - z - l).\n",
        "        # Equation from Section 7.2: p_R = (1 + γ)λ * E[(1 - Z - l)^+] = (1 + γ)λ * ∫[0 to 1-l] (1 - z - l) * f_Z(z) dz\n",
        "        # Get the PDF of the base loss distribution Z.\n",
        "        pdf = _get_loss_pdf(model_params.stochastic_shock_parameters.loss_distribution_G_Z)\n",
        "        # Define the integrand: the ceded loss amount multiplied by the probability density.\n",
        "        integrand = lambda z: (1 - z - l) * pdf(z)\n",
        "\n",
        "        # Perform the numerical integration over the region where ceded loss is positive.\n",
        "        integral, error_est = scipy.integrate.quad(integrand, 0, 1 - l)\n",
        "\n",
        "        # Rigorous check on the accuracy of the numerical integration.\n",
        "        if error_est > 1e-8:\n",
        "            warnings.warn(\n",
        "                f\"Numerical integration for 'XL' premium calculation yielded a \"\n",
        "                f\"high absolute error estimate of {error_est:.2e}. The computed \"\n",
        "                f\"premium may be inaccurate.\",\n",
        "                RuntimeWarning\n",
        "            )\n",
        "\n",
        "        # Calculate the final premium using the integrated expected ceded loss.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * integral\n",
        "\n",
        "    elif microinsurance.insurance_type == \"TotalLoss\":\n",
        "        # Retrieve the policy-specific parameter L.\n",
        "        L = microinsurance.policy_parameters.total_loss_L\n",
        "\n",
        "        # The premium is based on the expected ceded loss, requiring numerical integration.\n",
        "        # Ceded loss is u = 1 - z for u > L, which means z < 1 - L.\n",
        "        # Equation from Section 7.3: p_R = (1 + γ)λ * E[(1 - Z) * 1_{Z < 1-L}] = (1 + γ)λ * ∫[0 to 1-L] (1 - z) * f_Z(z) dz\n",
        "        # Get the PDF of the base loss distribution Z.\n",
        "        pdf = _get_loss_pdf(model_params.stochastic_shock_parameters.loss_distribution_G_Z)\n",
        "        # Define the integrand.\n",
        "        integrand = lambda z: (1 - z) * pdf(z)\n",
        "\n",
        "        # Perform the numerical integration.\n",
        "        integral, error_est = scipy.integrate.quad(integrand, 0, 1 - L)\n",
        "\n",
        "        # Rigorous check on the accuracy of the numerical integration.\n",
        "        if error_est > 1e-8:\n",
        "            warnings.warn(\n",
        "                f\"Numerical integration for 'TotalLoss' premium calculation yielded a \"\n",
        "                f\"high absolute error estimate of {error_est:.2e}. The computed \"\n",
        "                f\"premium may be inaccurate.\",\n",
        "                RuntimeWarning\n",
        "            )\n",
        "\n",
        "        # Calculate the final premium.\n",
        "        computed_p_R = (1 + gamma) * lambda_ * integral\n",
        "\n",
        "    # The function returns the computed premium, which has been calculated from\n",
        "    # first principles according to the specified policy type.\n",
        "    return computed_p_R\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 2: Helper function to compute transformed primitives\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_transformed_primitives(\n",
        "    p_R: float,\n",
        "    model_params: 'ModelParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities'\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Computes the transformed poverty line x*^R and growth rate r^R.\n",
        "\n",
        "    These transformations adjust the core parameters of the deterministic part\n",
        "    of the PDMP to account for the cash outflow from premium payments.\n",
        "\n",
        "    Args:\n",
        "        p_R: The computed insurance premium.\n",
        "        model_params: The parsed dataclass containing base model parameters (b, x*).\n",
        "        base_quantities: The dataclass containing the base growth rate 'r'.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing `(transformed_poverty_line_x_star_R, transformed_growth_rate_r_R)`.\n",
        "    \"\"\"\n",
        "    # Extract base parameters needed for the transformation.\n",
        "    b = model_params.household_economic_parameters.primitive_params_for_r.income_generation_rate_b\n",
        "    x_star = model_params.household_economic_parameters.poverty_line_x_star\n",
        "    r = base_quantities.capital_growth_rate_r\n",
        "\n",
        "    # The feasibility constraint b > p_R was already enforced in Task 2.\n",
        "    # This allows for safe computation without division by zero or negative numbers.\n",
        "\n",
        "    # Equation (7.2): x*^R = (b / (b - p_R)) * x*\n",
        "    # This adjusts the poverty line upwards because income 'b' is reduced by the premium 'p_R'.\n",
        "    transformed_poverty_line_x_star_R = (b / (b - p_R)) * x_star\n",
        "\n",
        "    # Equation (7.3): r^R = r * ((b - p_R) / b)\n",
        "    # This adjusts the growth rate downwards, reflecting the reduced net income available for savings.\n",
        "    transformed_growth_rate_r_R = r * ((b - p_R) / b)\n",
        "\n",
        "    # Return the pair of transformed parameters.\n",
        "    return transformed_poverty_line_x_star_R, transformed_growth_rate_r_R\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Step 3: Helper function to define the post-insurance law W\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_post_insurance_law(\n",
        "    model_params: 'ModelParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    z_sampler: Callable[[int], np.ndarray]\n",
        ") -> Tuple[float, Callable[[int], np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Computes E[W] and creates a sampler for the post-insurance loss variable W.\n",
        "\n",
        "    This function defines the new stochastic jump component of the model under\n",
        "    insurance. It computes the new expected remaining capital E[W] and returns\n",
        "    a callable function that can generate random samples of W.\n",
        "\n",
        "    Args:\n",
        "        model_params: The parsed dataclass containing insurance policy details.\n",
        "        base_quantities: The dataclass containing the base mean 'μ'.\n",
        "        z_sampler: A callable function that generates `size` samples from the\n",
        "                   base loss distribution Z.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing `(mean_E_W, w_sampler)`.\n",
        "    \"\"\"\n",
        "    # Extract relevant parameters.\n",
        "    microinsurance = model_params.microinsurance_parameters\n",
        "    loss_dist = model_params.stochastic_shock_parameters.loss_distribution_G_Z\n",
        "\n",
        "    # Initialize outputs.\n",
        "    mean_E_W: float = 0.0\n",
        "    w_sampler: Callable[[int], np.ndarray] = lambda size: np.array([])\n",
        "\n",
        "    # Dispatch logic based on insurance type.\n",
        "    if microinsurance.insurance_type == \"Proportional\":\n",
        "        eta = microinsurance.policy_parameters.proportional_retention_eta\n",
        "        mu = base_quantities.expected_remaining_capital_mu\n",
        "\n",
        "        # Analytical formula for E[W] in the proportional case.\n",
        "        # E[W] = E[1 - η(1-Z)] = 1 - η + η*E[Z] = 1 - η + η*μ\n",
        "        mean_E_W = 1 - eta + eta * mu\n",
        "\n",
        "        # Create the sampler for W = 1 - η(1 - Z). This is a vectorized transformation.\n",
        "        w_sampler = lambda size: 1 - eta * (1 - z_sampler(size))\n",
        "\n",
        "    elif microinsurance.insurance_type == \"XL\":\n",
        "        l = microinsurance.policy_parameters.xl_retention_l\n",
        "\n",
        "        # E[W] = E[max(1-l, Z)] = ∫[0 to 1-l] (1-l)*f_Z(z)dz + ∫[1-l to 1] z*f_Z(z)dz\n",
        "        pdf = _get_loss_pdf(loss_dist)\n",
        "        integral1, _ = scipy.integrate.quad(lambda z: (1 - l) * pdf(z), 0, 1 - l)\n",
        "        integral2, _ = scipy.integrate.quad(lambda z: z * pdf(z), 1 - l, 1)\n",
        "        mean_E_W = integral1 + integral2\n",
        "\n",
        "        # Create the sampler for W = max(1-l, Z) using NumPy's vectorized maximum.\n",
        "        w_sampler = lambda size: np.maximum(1 - l, z_sampler(size))\n",
        "\n",
        "    elif microinsurance.insurance_type == \"TotalLoss\":\n",
        "        L = microinsurance.policy_parameters.total_loss_L\n",
        "\n",
        "        # E[W] = E[Z*1_{Z>=1-L} + 1*1_{Z<1-L}] = ∫[1-L to 1] z*f_Z(z)dz + G_Z(1-L)\n",
        "        pdf = _get_loss_pdf(loss_dist)\n",
        "        # Get the underlying SciPy distribution object to call its CDF method.\n",
        "        dist_scipy = getattr(scipy.stats, loss_dist.name)\n",
        "        # SciPy's parameter names for Beta are 'a', 'b'; for Kumaraswamy they are also 'a', 'b'.\n",
        "        # We must map our parameter names ('alpha', 'p', etc.) to SciPy's names.\n",
        "        scipy_params = {'a': loss_dist.parameters.get('alpha') or loss_dist.parameters.get('p'),\n",
        "                        'b': loss_dist.parameters.get('beta') or loss_dist.parameters.get('q')}\n",
        "\n",
        "        # G_Z(1-L) is the CDF evaluated at 1-L.\n",
        "        cdf_val = dist_scipy.cdf(1 - L, **scipy_params)\n",
        "        # ∫[1-L to 1] z*f_Z(z)dz\n",
        "        integral, _ = scipy.integrate.quad(lambda z: z * pdf(z), 1 - L, 1)\n",
        "        mean_E_W = integral + cdf_val\n",
        "\n",
        "        # Create the sampler for W using NumPy's vectorized `where` function.\n",
        "        def total_loss_sampler(size: int) -> np.ndarray:\n",
        "            z = z_sampler(size)\n",
        "            # If z < 1-L (loss > L), the remaining capital is 1 (total coverage). Otherwise, it's z.\n",
        "            return np.where(z < 1 - L, 1.0, z)\n",
        "        w_sampler = total_loss_sampler\n",
        "\n",
        "    # Return the computed mean and the created sampler function.\n",
        "    return mean_E_W, w_sampler\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 5, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def compute_insurance_transforms(\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    z_sampler: Callable[[int], np.ndarray]\n",
        ") -> Optional[InsuranceTransforms]:\n",
        "    \"\"\"\n",
        "    Orchestrates the computation of all microinsurance-induced transformations.\n",
        "\n",
        "    If microinsurance is inactive, this function does nothing and returns None.\n",
        "    If active, it computes the full set of transformations that redefine the\n",
        "    stochastic process for the insured case, as described in Section 7 of the\n",
        "    research context. This function is a critical step in setting up the\n",
        "    parameters for an insured-case simulation or analysis.\n",
        "\n",
        "    Process:\n",
        "    1.  **Compute Premium (p_R):** Calculates the insurance premium from first\n",
        "        principles, using numerical integration for non-proportional policies.\n",
        "    2.  **Compute Transformed Primitives:** Uses the premium to calculate the\n",
        "        new, effective poverty line (x*^R) and capital growth rate (r^R).\n",
        "    3.  **Define Post-Insurance Law (W):** Determines the new random variable W\n",
        "        for post-insurance shocks, computing its mean E[W] and creating a\n",
        "        callable sampler function for it.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The fully parsed, validated, and cleansed configuration object.\n",
        "        base_quantities: An object containing the derived quantities (r, μ)\n",
        "                         for the baseline uninsured model.\n",
        "        z_sampler: A callable function that generates random samples from the\n",
        "                   base loss distribution Z.\n",
        "\n",
        "    Returns:\n",
        "        An `InsuranceTransforms` object containing all computed transformations\n",
        "        if insurance is active, otherwise `None`.\n",
        "    \"\"\"\n",
        "    # Step 0: Check if insurance is active. If not, no transforms are needed.\n",
        "    if not parsed_config.model_parameters.microinsurance_parameters.is_active:\n",
        "        return None\n",
        "\n",
        "    # Step 1: Compute the insurance premium p_R from first principles.\n",
        "    # This is the definitive premium value for the insured model.\n",
        "    premium_p_R = _compute_insurance_premium(\n",
        "        model_params=parsed_config.model_parameters,\n",
        "        base_quantities=base_quantities\n",
        "    )\n",
        "\n",
        "    # Step 2: Compute the transformed poverty line and growth rate.\n",
        "    # These are the new core parameters for the PDMP's deterministic flow.\n",
        "    x_star_R, r_R = _compute_transformed_primitives(\n",
        "        p_R=premium_p_R,\n",
        "        model_params=parsed_config.model_parameters,\n",
        "        base_quantities=base_quantities\n",
        "    )\n",
        "\n",
        "    # Step 3: Define the post-insurance shock distribution W.\n",
        "    # This yields the new mean E[W] and a sampler for the PDMP's stochastic jumps.\n",
        "    mean_E_W, w_sampler = _compute_post_insurance_law(\n",
        "        model_params=parsed_config.model_parameters,\n",
        "        base_quantities=base_quantities,\n",
        "        z_sampler=z_sampler\n",
        "    )\n",
        "\n",
        "    # Step 4: Package all computed transformations into a single, immutable dataclass.\n",
        "    # This object serves as the complete parameter set for the insured model.\n",
        "    return InsuranceTransforms(\n",
        "        premium_p_R=premium_p_R,\n",
        "        transformed_poverty_line_x_star_R=x_star_R,\n",
        "        transformed_growth_rate_r_R=r_R,\n",
        "        mean_post_insurance_loss_E_W=mean_E_W,\n",
        "        post_insurance_loss_sampler=w_sampler\n",
        "    )\n"
      ],
      "metadata": {
        "id": "2k5Ybv5uJvs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6 — Select computation method and define active parameters\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 6: Select computation method and define active parameters\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ActiveParameters:\n",
        "    \"\"\"\n",
        "    Immutable container for the active parameters governing the simulation.\n",
        "\n",
        "    This object consolidates the definitive parameters to be used by the core\n",
        "    computational engine. It resolves all conditional logic (e.g., insurance)\n",
        "    to provide a single, unambiguous source of truth for the poverty line,\n",
        "    growth rate, and shock distribution.\n",
        "\n",
        "    Attributes:\n",
        "        active_poverty_line: The effective poverty line for the scenario\n",
        "                             (x* or x*^R).\n",
        "        active_growth_rate: The effective capital growth rate (r or r^R).\n",
        "        final_algorithm_choice: The canonical name of the computational method\n",
        "                                to be used ('ClosedForm', 'MonteCarlo', etc.).\n",
        "        optimizer_search_bracket: A tuple (y_min, y_max) defining the bounds\n",
        "                                  for the threshold optimization search.\n",
        "    \"\"\"\n",
        "    # The effective poverty line for the scenario (x* or x*^R).\n",
        "    active_poverty_line: float\n",
        "\n",
        "    # The effective capital growth rate (r or r^R).\n",
        "    active_growth_rate: float\n",
        "\n",
        "    # The canonical name of the computational method to be used.\n",
        "    final_algorithm_choice: str\n",
        "\n",
        "    # A tuple (y_min, y_max) for the threshold optimization search.\n",
        "    optimizer_search_bracket: Tuple[float, float]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Step 1: Helper function to select the computation method\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _select_computation_method(\n",
        "    config: 'ParsedStudyConfig'\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Determines the definitive computational method based on config and eligibility.\n",
        "\n",
        "    This function implements the logic for selecting the appropriate algorithm\n",
        "    ('ClosedForm', 'MonteCarlo', etc.). It checks if the conditions for the\n",
        "    analytical closed-form solution are met and handles the 'Auto' selection\n",
        "    mode.\n",
        "\n",
        "    Args:\n",
        "        config: The fully parsed, validated, and cleansed configuration object.\n",
        "\n",
        "    Returns:\n",
        "        The canonical name of the algorithm to be used for computation.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the user explicitly requests 'ClosedForm' for a\n",
        "                    configuration that is not eligible.\n",
        "    \"\"\"\n",
        "    # Extract relevant parameters for the decision logic.\n",
        "    method_selection = config.computation_parameters.method_selection\n",
        "    loss_dist = config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z\n",
        "    is_insurance_active = config.model_parameters.microinsurance_parameters.is_active\n",
        "    user_choice = method_selection.algorithm_to_use\n",
        "\n",
        "    # --- Step 1a: Determine eligibility for the closed-form solution ---\n",
        "    # The closed-form solution from Section 5 is only valid for the specific\n",
        "    # case of a Beta(α, 1) loss distribution and no microinsurance.\n",
        "    is_closed_form_eligible = (\n",
        "        loss_dist.name == \"Beta\" and\n",
        "        math.isclose(loss_dist.parameters.get(\"beta\", 0.0), 1.0) and\n",
        "        not is_insurance_active\n",
        "    )\n",
        "\n",
        "    # --- Step 1b: Apply selection logic ---\n",
        "    final_choice = \"\"\n",
        "    if user_choice == \"Auto\":\n",
        "        # In 'Auto' mode, use the most efficient valid method.\n",
        "        # If closed-form is eligible, choose it; otherwise, default to MonteCarlo.\n",
        "        final_choice = \"ClosedForm\" if is_closed_form_eligible else \"MonteCarlo\"\n",
        "    elif user_choice == \"ClosedForm\":\n",
        "        # If the user explicitly requests 'ClosedForm', we must verify eligibility.\n",
        "        if not is_closed_form_eligible:\n",
        "            # If not eligible, raise a specific error to prevent incorrect application.\n",
        "            raise ValueError(\n",
        "                \"Explicit request for 'ClosedForm' method is invalid for this \"\n",
        "                \"configuration. The closed-form solution is only available for \"\n",
        "                \"a Beta(alpha, 1) loss distribution with no microinsurance.\"\n",
        "            )\n",
        "        # If eligible, honor the user's choice.\n",
        "        final_choice = \"ClosedForm\"\n",
        "    else:\n",
        "        # For other explicit choices ('MonteCarlo', 'FixedPoint'), honor them directly.\n",
        "        final_choice = user_choice\n",
        "\n",
        "    return final_choice\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 6, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def select_method_and_define_active_parameters(\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    insurance_transforms: Optional['InsuranceTransforms']\n",
        ") -> ActiveParameters:\n",
        "    \"\"\"\n",
        "    Selects the computation method and defines all active model parameters.\n",
        "\n",
        "    This orchestrator function is a critical control point in the\n",
        "    pipeline. It makes final decisions about the computational strategy and\n",
        "    consolidates all conditional parameters (i.e., those dependent on insurance\n",
        "    status) into a single, unambiguous `ActiveParameters` object. This version\n",
        "    is complete and derives all its logic directly from the configuration,\n",
        "    containing no hardcoded values.\n",
        "\n",
        "    Process:\n",
        "    1.  **Select Method:** Determines the final algorithm to use ('ClosedForm'\n",
        "        or 'MonteCarlo') based on model eligibility and user configuration.\n",
        "    2.  **Set Active Parameters:** Chooses between the base (uninsured) and\n",
        "        transformed (insured) poverty line and growth rate.\n",
        "    3.  **Define Optimizer Bracket:** Calculates the search bounds (y_min, y_max)\n",
        "        for the subsequent optimization of the transfer threshold y*, sourcing\n",
        "        all parameters from the configuration object.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The fully parsed, validated, and cleansed configuration.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        insurance_transforms: The computed insurance transformations, or None if\n",
        "                              insurance is inactive.\n",
        "\n",
        "    Returns:\n",
        "        An `ActiveParameters` object containing the final, consolidated set of\n",
        "        parameters for the core computational engine.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Determine the final computational method ---\n",
        "    final_algorithm = _select_computation_method(config=parsed_config)\n",
        "\n",
        "    # --- Step 2: Set the active poverty line and growth rate ---\n",
        "    # This logic resolves the insurance-dependent parameters into a single set.\n",
        "    if insurance_transforms is None:\n",
        "        # If insurance is inactive, use the base parameters from the uninsured model.\n",
        "        active_poverty_line = parsed_config.model_parameters.household_economic_parameters.poverty_line_x_star\n",
        "        active_growth_rate = base_quantities.capital_growth_rate_r\n",
        "    else:\n",
        "        # If insurance is active, use the transformed parameters computed in Task 5.\n",
        "        active_poverty_line = insurance_transforms.transformed_poverty_line_x_star_R\n",
        "        active_growth_rate = insurance_transforms.transformed_growth_rate_r_R\n",
        "\n",
        "    # --- Step 3: Define the optimization search bracket ---\n",
        "    # The lower bound for the optimal threshold y* is always the active poverty line.\n",
        "    # This is a fundamental constraint of the model.\n",
        "    y_min = active_poverty_line\n",
        "\n",
        "    # Step 3a: Access the y_max_factor from the parsed config.\n",
        "    y_max_factor = parsed_config.computation_parameters.method_selection.threshold_search_bracket.y_max_factor_times_y_min\n",
        "\n",
        "    # Step 3b Validate the factor to ensure a valid search interval.\n",
        "    # A factor <= 1.0 would result in an invalid or zero-width search bracket.\n",
        "    if y_max_factor <= 1.0:\n",
        "        raise ValueError(\n",
        "            f\"Configuration error: 'y_max_factor_times_y_min' must be > 1.0, \"\n",
        "            f\"but got {y_max_factor}.\"\n",
        "        )\n",
        "\n",
        "    # Step 3c: Calculate the upper bound of the search bracket.\n",
        "    y_max = y_min * y_max_factor\n",
        "\n",
        "    # --- Final Assembly ---\n",
        "    # Assemble the final, consolidated ActiveParameters object. This object\n",
        "    # now contains all necessary, scenario-specific parameters for the core engine.\n",
        "    return ActiveParameters(\n",
        "        active_poverty_line=active_poverty_line,\n",
        "        active_growth_rate=active_growth_rate,\n",
        "        final_algorithm_choice=final_algorithm,\n",
        "        optimizer_search_bracket=(y_min, y_max)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PHULJgggpr_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7 — Construct sampler for Z (and W, if insured)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 7: Construct sampler for Z (and W, if insured)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 1: Helper factory to build the base sampler for Z\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_z_sampler(\n",
        "    loss_dist: 'LossDistributionGZ',\n",
        "    rng: np.random.Generator\n",
        ") -> Callable[[int], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Factory function to create a vectorized sampler for the base loss distribution Z.\n",
        "\n",
        "    This function uses the inverse transform sampling method to generate random\n",
        "    variates. It inspects the distribution's name and parameters and returns a\n",
        "    highly efficient, vectorized sampler function.\n",
        "\n",
        "    Args:\n",
        "        loss_dist: The dataclass specifying the loss distribution.\n",
        "        rng: An instance of `numpy.random.Generator` for reproducible random\n",
        "             number generation.\n",
        "\n",
        "    Returns:\n",
        "        A callable function that takes an integer `size` and returns a NumPy\n",
        "        array of `size` random variates from the distribution Z.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If the distribution name is not supported.\n",
        "    \"\"\"\n",
        "    # Dispatch to the correct inverse CDF formula based on the distribution name.\n",
        "    if loss_dist.name == \"Beta\":\n",
        "        # For a Beta(α, 1) distribution, the inverse CDF is F_Z⁻¹(u) = u^(1/α).\n",
        "        alpha = loss_dist.parameters[\"alpha\"]\n",
        "\n",
        "        def beta_sampler(size: int) -> np.ndarray:\n",
        "            # Generate 'size' uniform random numbers in [0, 1).\n",
        "            uniform_variates = rng.random(size)\n",
        "            # Apply the inverse transform formula in a vectorized manner.\n",
        "            return np.power(uniform_variates, 1.0 / alpha)\n",
        "\n",
        "        return beta_sampler\n",
        "\n",
        "    elif loss_dist.name == \"Kumaraswamy\":\n",
        "        # For Kumaraswamy(p, q), the inverse CDF is F_Z⁻¹(u) = (1 - (1 - u)^(1/q))^(1/p).\n",
        "        p = loss_dist.parameters[\"p\"]\n",
        "        q = loss_dist.parameters[\"q\"]\n",
        "\n",
        "        def kumaraswamy_sampler(size: int) -> np.ndarray:\n",
        "            # Generate 'size' uniform random numbers.\n",
        "            uniform_variates = rng.random(size)\n",
        "            # Apply the inverse transform formula in a vectorized manner.\n",
        "            term1 = np.power(1.0 - uniform_variates, 1.0 / q)\n",
        "            term2 = 1.0 - term1\n",
        "            return np.power(term2, 1.0 / p)\n",
        "\n",
        "        return kumaraswamy_sampler\n",
        "\n",
        "    else:\n",
        "        # If the distribution is not recognized, raise an error.\n",
        "        raise NotImplementedError(\n",
        "            f\"Sampler for loss distribution '{loss_dist.name}' is not implemented. \"\n",
        "            \"Supported distributions: ['Beta', 'Kumaraswamy'].\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Step 3: Helper function to validate a sampler's output\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _validate_sampler(\n",
        "    sampler: Callable[[int], np.ndarray],\n",
        "    theoretical_mean: float,\n",
        "    sampler_name: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs a sanity check on a sampler by comparing its empirical mean to the theoretical mean.\n",
        "\n",
        "    Args:\n",
        "        sampler: The sampler function to validate.\n",
        "        theoretical_mean: The known theoretical mean of the distribution.\n",
        "        sampler_name: The name of the sampler ('Z' or 'W') for error messages.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the empirical mean is not close to the theoretical\n",
        "                        mean or if samples fall outside the valid (0, 1] domain.\n",
        "    \"\"\"\n",
        "    # Define a large sample size for a statistically meaningful validation.\n",
        "    validation_size = 100_000\n",
        "\n",
        "    # Generate a large batch of samples.\n",
        "    samples = sampler(validation_size)\n",
        "\n",
        "    # Check 1: Domain validation. All samples must be in the interval (0, 1].\n",
        "    # Z=0 represents total loss of capital, which can be problematic in some models.\n",
        "    # We ensure samples are strictly positive.\n",
        "    if not (np.all(samples > 0) and np.all(samples <= 1.0)):\n",
        "        min_sample, max_sample = np.min(samples), np.max(samples)\n",
        "        raise AssertionError(\n",
        "            f\"Sampler validation failed for '{sampler_name}'. Samples fall \"\n",
        "            f\"outside the valid domain (0, 1]. \"\n",
        "            f\"Min: {min_sample}, Max: {max_sample}.\"\n",
        "        )\n",
        "\n",
        "    # Check 2: Mean validation. The empirical mean should be close to the theoretical mean.\n",
        "    empirical_mean = np.mean(samples)\n",
        "\n",
        "    # Use a statistical tolerance (e.g., 2%) for this check, as it's subject to sampling error.\n",
        "    if not math.isclose(empirical_mean, theoretical_mean, rel_tol=0.02):\n",
        "        raise AssertionError(\n",
        "            f\"Sampler validation failed for '{sampler_name}'. The empirical \"\n",
        "            f\"mean ({empirical_mean:.6f}) is not close to the theoretical \"\n",
        "            f\"mean ({theoretical_mean:.6f}). Check the sampler implementation.\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 7, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def construct_active_sampler(\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    insurance_transforms: Optional['InsuranceTransforms']\n",
        ") -> Callable[[int], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs, validates, and returns the active sampler for the simulation.\n",
        "\n",
        "    This function is the definitive factory for the random shock generator used\n",
        "    in the Monte Carlo engine. It determines whether the scenario is insured or\n",
        "    not and returns the appropriate, validated sampler.\n",
        "\n",
        "    Process:\n",
        "    1.  **Create RNG:** Initializes a NumPy random number generator with the\n",
        "        seed from the configuration to ensure reproducibility.\n",
        "    2.  **Build Base Sampler (Z):** Creates a sampler for the underlying loss\n",
        "        distribution Z using the inverse transform method.\n",
        "    3.  **Select Active Sampler:** If insurance is active, it uses the pre-computed\n",
        "        `post_insurance_loss_sampler` for W from the `insurance_transforms` object.\n",
        "        Otherwise, it uses the base sampler for Z.\n",
        "    4.  **Validate:** Performs a crucial sanity check on the selected active\n",
        "        sampler, comparing its empirical mean against the known theoretical mean\n",
        "        to catch potential implementation errors before the main simulation begins.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The fully parsed and validated configuration object.\n",
        "        base_quantities: An object containing the derived quantities (μ) for the\n",
        "                         baseline uninsured model.\n",
        "        insurance_transforms: The computed insurance transformations, or None if\n",
        "                              insurance is inactive. Contains the sampler for W.\n",
        "\n",
        "    Returns:\n",
        "        The active, validated sampler function. This function takes an integer\n",
        "        `size` and returns a NumPy array of random shock proportions.\n",
        "    \"\"\"\n",
        "    # Step 1: Create a dedicated, seeded random number generator.\n",
        "    # This ensures that all stochastic operations are reproducible.\n",
        "    rng = np.random.default_rng(\n",
        "        seed=parsed_config.computation_parameters.monte_carlo_settings.random_seed\n",
        "    )\n",
        "\n",
        "    # Step 2: Build the sampler for the base distribution Z.\n",
        "    # This sampler is always created, as the sampler for W depends on it.\n",
        "    z_sampler = _create_z_sampler(\n",
        "        loss_dist=parsed_config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z,\n",
        "        rng=rng\n",
        "    )\n",
        "\n",
        "    # Step 3: Select the active sampler and its corresponding theoretical mean.\n",
        "    if insurance_transforms is not None:\n",
        "        # If insurance is active, the active sampler is the one for W.\n",
        "        active_sampler = insurance_transforms.post_insurance_loss_sampler\n",
        "        # The corresponding theoretical mean is E[W].\n",
        "        theoretical_mean = insurance_transforms.mean_post_insurance_loss_E_W\n",
        "        sampler_name = \"W (post-insurance)\"\n",
        "    else:\n",
        "        # If insurance is not active, the active sampler is the one for Z.\n",
        "        active_sampler = z_sampler\n",
        "        # The corresponding theoretical mean is E[Z] = μ.\n",
        "        theoretical_mean = base_quantities.expected_remaining_capital_mu\n",
        "        sampler_name = \"Z (base)\"\n",
        "\n",
        "    # Step 4: Perform a critical validation of the active sampler.\n",
        "    # This sanity check helps prevent subtle bugs in the random variate generation.\n",
        "    _validate_sampler(\n",
        "        sampler=active_sampler,\n",
        "        theoretical_mean=theoretical_mean,\n",
        "        sampler_name=sampler_name\n",
        "    )\n",
        "\n",
        "    # Return the final, validated sampler to be used by the simulation engine.\n",
        "    return active_sampler\n"
      ],
      "metadata": {
        "id": "_hG0JpL3sj1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8 — Implement PDMP path simulation kernel\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 8: Implement PDMP path simulation kernel\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Step 1: Helper function for the deterministic flow\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_deterministic_flow(\n",
        "    capital: float,\n",
        "    time_delta: float,\n",
        "    active_poverty_line: float,\n",
        "    active_growth_rate: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies the deterministic capital flow over a given time interval.\n",
        "\n",
        "    This function implements the solution to the piecewise ordinary differential\n",
        "    equation governing the household's capital between stochastic shock events.\n",
        "\n",
        "    Equation from Section 2:\n",
        "    X(t + Δ) = (X(t) - x*) * exp(r*Δ) + x*  , if X(t) > x*\n",
        "    X(t + Δ) = X(t)                         , if X(t) <= x*\n",
        "\n",
        "    Args:\n",
        "        capital: The capital level at the beginning of the interval.\n",
        "        time_delta: The duration of the interval (Δ).\n",
        "        active_poverty_line: The effective poverty line (x* or x*^R).\n",
        "        active_growth_rate: The effective capital growth rate (r or r^R).\n",
        "\n",
        "    Returns:\n",
        "        The new capital level after the deterministic flow.\n",
        "    \"\"\"\n",
        "    # If capital is above the active poverty line, it grows exponentially.\n",
        "    if capital > active_poverty_line:\n",
        "        # Calculate the capital surplus above the poverty line.\n",
        "        surplus = capital - active_poverty_line\n",
        "        # Apply exponential growth to the surplus.\n",
        "        grown_surplus = surplus * np.exp(active_growth_rate * time_delta)\n",
        "        # The new capital is the grown surplus plus the poverty line.\n",
        "        return active_poverty_line + grown_surplus\n",
        "    else:\n",
        "        # If capital is at or below the poverty line, it stagnates.\n",
        "        return capital\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 8, Orchestrator Function (The Simulation Kernel)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def simulate_single_pdmp_path(\n",
        "    initial_capital: float,\n",
        "    threshold_y: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator,\n",
        "    shock_arrival_rate: float,\n",
        "    time_horizon: float\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Simulates a single sample path of the PDMP to find a first-passage time.\n",
        "\n",
        "    This function is the core simulation kernel. It generates one complete\n",
        "    trajectory of a household's capital, starting from `initial_capital`, until\n",
        "    it first drops below the specified `threshold_y` or the simulation time\n",
        "    exceeds the `time_horizon`.\n",
        "\n",
        "    Process for each step in the path:\n",
        "    1.  **Draw Inter-arrival Time:** A random time `Δt` until the next shock is\n",
        "        drawn from an Exponential distribution with rate `λ`.\n",
        "    2.  **Check Truncation:** The cumulative time is checked against the horizon `T`.\n",
        "        If `t + Δt >= T`, the path is truncated.\n",
        "    3.  **Apply Deterministic Flow:** The capital evolves deterministically for\n",
        "        the duration `Δt` according to the piecewise growth dynamics.\n",
        "    4.  **Apply Stochastic Jump:** A random shock is drawn from the active\n",
        "        sampler, and the capital is reduced multiplicatively.\n",
        "    5.  **Check Threshold Crossing:** The new capital level is checked against\n",
        "        the threshold `y`. If it has dropped below, the first-passage time and\n",
        "        required injection are recorded, and the simulation for this path ends.\n",
        "\n",
        "    Args:\n",
        "        initial_capital: The starting capital level for the simulation path.\n",
        "        threshold_y: The threshold `y` below which we are measuring the first passage.\n",
        "        active_params: The dataclass containing the active poverty line and growth rate.\n",
        "        shock_sampler: The validated sampler for the active shock distribution (Z or W).\n",
        "        rng: The seeded NumPy random number generator for drawing inter-arrival times.\n",
        "        shock_arrival_rate: The Poisson arrival rate `λ` for shocks.\n",
        "        time_horizon: The maximum simulation time `T` before truncation.\n",
        "\n",
        "    Returns:\n",
        "        A tuple `(first_passage_time, injection_amount)`:\n",
        "        - `first_passage_time`: The time `τ_y` at which capital first dropped\n",
        "          below `y`. Returns `np.inf` if the path is truncated.\n",
        "        - `injection_amount`: The capital `J_y = y - X(τ_y)` needed to restore\n",
        "          the threshold. Returns `0.0` if the path is truncated.\n",
        "    \"\"\"\n",
        "    # Initialize the state variables for the simulation path.\n",
        "    current_capital = initial_capital\n",
        "    cumulative_time = 0.0\n",
        "\n",
        "    # The main simulation loop continues until the path terminates.\n",
        "    while True:\n",
        "        # --- Step 1: Draw Inter-arrival Time ---\n",
        "        # The time until the next Poisson event follows an Exponential distribution.\n",
        "        # Δt ~ Exponential(λ)\n",
        "        time_to_next_shock = rng.exponential(1.0 / shock_arrival_rate)\n",
        "\n",
        "        # --- Step 2: Implement Truncation Policy ---\n",
        "        # Check if the next event would occur beyond the simulation time horizon.\n",
        "        if cumulative_time + time_to_next_shock >= time_horizon:\n",
        "            # If so, the path is truncated. Return values corresponding to zero cost.\n",
        "            # First-passage time is infinite.\n",
        "            first_passage_time = np.inf\n",
        "            # Injection amount is zero.\n",
        "            injection_amount = 0.0\n",
        "            return first_passage_time, injection_amount\n",
        "\n",
        "        # --- Step 3: Apply Deterministic Flow ---\n",
        "        # Evolve the capital deterministically over the inter-arrival period.\n",
        "        capital_after_flow = _apply_deterministic_flow(\n",
        "            capital=current_capital,\n",
        "            time_delta=time_to_next_shock,\n",
        "            active_poverty_line=active_params.active_poverty_line,\n",
        "            active_growth_rate=active_params.active_growth_rate\n",
        "        )\n",
        "\n",
        "        # Update the cumulative time.\n",
        "        cumulative_time += time_to_next_shock\n",
        "\n",
        "        # --- Step 4: Apply Stochastic Jump ---\n",
        "        # Draw a single random shock representing the remaining capital proportion.\n",
        "        shock = shock_sampler(1)[0]\n",
        "        # Apply the shock multiplicatively to the capital.\n",
        "        capital_after_shock = capital_after_flow * shock\n",
        "\n",
        "        # Update the current capital for the next iteration.\n",
        "        current_capital = capital_after_shock\n",
        "\n",
        "        # --- Step 5: Check for Threshold Crossing ---\n",
        "        # If the capital has dropped below the threshold y, the path terminates.\n",
        "        if current_capital < threshold_y:\n",
        "            # The first-passage time is the current cumulative time.\n",
        "            first_passage_time = cumulative_time\n",
        "            # The required injection is the amount needed to return to the threshold.\n",
        "            # J_y = y - X(τ_y)\n",
        "            injection_amount = threshold_y - current_capital\n",
        "            # Return the results for this completed path.\n",
        "            return first_passage_time, injection_amount\n"
      ],
      "metadata": {
        "id": "7sBPSf2WuDyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9 — Implement Monte Carlo estimator for V<sup>π_y</sup>(y)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 9: Implement Monte Carlo estimator for V^π_y(y)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Step 1: Helper function to generate N first-passage samples\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_first_passage_samples(\n",
        "    num_paths: int,\n",
        "    initial_capital: float,\n",
        "    threshold_y: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator,\n",
        "    shock_arrival_rate: float,\n",
        "    time_horizon: float\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates N first-passage time and injection amount samples via simulation.\n",
        "\n",
        "    This function orchestrates a Monte Carlo campaign by repeatedly calling the\n",
        "    single-path simulation kernel. It is designed for efficiency by collecting\n",
        "    results in pre-allocated NumPy arrays.\n",
        "\n",
        "    Args:\n",
        "        num_paths: The number of simulation paths (N) to generate.\n",
        "        initial_capital: The starting capital for all paths.\n",
        "        threshold_y: The threshold `y` for defining first passage.\n",
        "        active_params: The active model parameters (poverty line, growth rate).\n",
        "        shock_sampler: The validated sampler for the shock distribution.\n",
        "        rng: The seeded NumPy random number generator.\n",
        "        shock_arrival_rate: The Poisson arrival rate `λ`.\n",
        "        time_horizon: The simulation truncation time `T`.\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two NumPy arrays:\n",
        "        - `tau_samples`: An array of `N` first-passage times.\n",
        "        - `injection_samples`: An array of `N` required injection amounts.\n",
        "    \"\"\"\n",
        "    # Pre-allocate NumPy arrays for storing the results for efficiency.\n",
        "    # This avoids the overhead of appending to Python lists in a tight loop.\n",
        "    tau_samples = np.zeros(num_paths, dtype=np.float64)\n",
        "    injection_samples = np.zeros(num_paths, dtype=np.float64)\n",
        "\n",
        "    # Loop N times to generate N independent sample paths.\n",
        "    for i in range(num_paths):\n",
        "        # Call the single-path simulation kernel from Task 8.\n",
        "        tau, injection = simulate_single_pdmp_path(\n",
        "            initial_capital=initial_capital,\n",
        "            threshold_y=threshold_y,\n",
        "            active_params=active_params,\n",
        "            shock_sampler=shock_sampler,\n",
        "            rng=rng,\n",
        "            shock_arrival_rate=shock_arrival_rate,\n",
        "            time_horizon=time_horizon\n",
        "        )\n",
        "        # Store the results in the pre-allocated arrays.\n",
        "        tau_samples[i] = tau\n",
        "        injection_samples[i] = injection\n",
        "\n",
        "    # Return the collected samples.\n",
        "    return tau_samples, injection_samples\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 9, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def estimate_vy_at_threshold(\n",
        "    threshold_y: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator,\n",
        "    shock_arrival_rate: float,\n",
        "    discount_rate: float,\n",
        "    num_paths: int,\n",
        "    time_horizon: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Estimates the value function V^π_y(y) at a given threshold `y`.\n",
        "\n",
        "    This function implements the Monte Carlo estimator for the expected\n",
        "    discounted cost of the threshold strategy, evaluated at the threshold\n",
        "    itself. This value is a critical component for both the optimization routine\n",
        "    (finding y*) and for evaluating the value function at other capital levels.\n",
        "\n",
        "    Process:\n",
        "    1.  **Simulate Paths:** Generates `N` sample paths of the PDMP, all starting\n",
        "        at `threshold_y`, and records the first-passage time `τ_y` and the\n",
        "        required injection `J_y` for each path.\n",
        "    2.  **Compute Statistics:** Calculates the sample means of the discounted\n",
        "        injection (`S1`) and the discount factors (`S0`) across all paths.\n",
        "    3.  **Apply Estimator:** Uses the renewal equation estimator from Section 6.1\n",
        "        to compute the final point estimate for V^π_y(y).\n",
        "\n",
        "    Args:\n",
        "        threshold_y: The threshold `y` at which to evaluate the value function.\n",
        "        active_params: The active model parameters for the simulation.\n",
        "        shock_sampler: The validated sampler for the active shock distribution.\n",
        "        rng: The seeded NumPy random number generator.\n",
        "        shock_arrival_rate: The Poisson arrival rate `λ`.\n",
        "        discount_rate: The continuous-time discount rate `δ`.\n",
        "        num_paths: The number of Monte Carlo paths `N` to simulate.\n",
        "        time_horizon: The simulation truncation time `T`.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the point estimate of V^π_y(y).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the denominator of the estimator is non-positive,\n",
        "                    indicating a pathological model configuration.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Generate N paths from the threshold y ---\n",
        "    # For this estimator, the simulation starts at the threshold itself.\n",
        "    tau_samples, injection_samples = _generate_first_passage_samples(\n",
        "        num_paths=num_paths,\n",
        "        initial_capital=threshold_y,\n",
        "        threshold_y=threshold_y,\n",
        "        active_params=active_params,\n",
        "        shock_sampler=shock_sampler,\n",
        "        rng=rng,\n",
        "        shock_arrival_rate=shock_arrival_rate,\n",
        "        time_horizon=time_horizon\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Compute discounted statistics in a vectorized manner ---\n",
        "    # Calculate the discount factor e^(-δ*τ) for each path.\n",
        "    # np.exp handles τ = np.inf correctly, resulting in 0.0.\n",
        "    discount_factors = np.exp(-discount_rate * tau_samples)\n",
        "\n",
        "    # Equation for S1: S1 = (1/N) * Σ[ J_y,i * exp(-δ*τ_y,i) ]\n",
        "    # This is the sample mean of the discounted injection amounts.\n",
        "    s1 = np.mean(injection_samples * discount_factors)\n",
        "\n",
        "    # Equation for S0: S0 = (1/N) * Σ[ exp(-δ*τ_y,i) ]\n",
        "    # This is the sample mean of the discount factors, representing the\n",
        "    # expected discounted probability of returning to the threshold.\n",
        "    s0 = np.mean(discount_factors)\n",
        "\n",
        "    # --- Step 3: Apply the fixed-point (renewal equation) estimator ---\n",
        "\n",
        "    # Calculate the denominator of the estimator.\n",
        "    denominator = 1.0 - s0\n",
        "\n",
        "    # Critical safeguard: The denominator must be strictly positive.\n",
        "    # If denominator <= 0, it implies E[e^(-δ*τ)] >= 1, which should not happen\n",
        "    # for δ > 0, as it suggests a divergent cost.\n",
        "    if denominator <= 1e-12: # Use a small tolerance for floating point safety\n",
        "        raise ValueError(\n",
        "            f\"Estimator denominator (1 - E[e^(-δτ)]) is non-positive ({denominator:.4e}). \"\n",
        "            \"This indicates a divergent cost, possibly due to a discount rate δ \"\n",
        "            \"that is too low or other pathological model parameters.\"\n",
        "        )\n",
        "\n",
        "    # Equation (6.1): V^π_y(y) ≈ S1 / (1 - S0)\n",
        "    # This formula arises from the Bellman equation at state y:\n",
        "    # V(y) = E[ e^(-δτ) * (J_y + V(y)) ], which solves to V(y) = E[e^(-δτ)J_y] / (1 - E[e^(-δτ)]).\n",
        "    estimated_value = s1 / denominator\n",
        "\n",
        "    return estimated_value\n"
      ],
      "metadata": {
        "id": "tYoVEc-xvX2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10 — Implement Monte Carlo estimator for V<sup>π_y</sup>(x) for x > y\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 10: Implement Monte Carlo estimator for V^π_y(x) for x > y\n",
        "# ==============================================================================\n",
        "\n",
        "def estimate_vy_for_continuation_region(\n",
        "    initial_capital_x: float,\n",
        "    threshold_y: float,\n",
        "    vy_at_y: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator,\n",
        "    shock_arrival_rate: float,\n",
        "    discount_rate: float,\n",
        "    num_paths: int,\n",
        "    time_horizon: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Estimates the value function V^π_y(x) for a starting capital `x` > `y`.\n",
        "\n",
        "    This function implements the Monte Carlo estimator for the \"continuation\n",
        "    region,\" where no immediate intervention is made. The value is derived from\n",
        "    the expected discounted cost incurred at the first time the capital process\n",
        "    drops below the threshold `y`. This cost consists of the immediate injection\n",
        "    plus the continuation value at the threshold, `vy_at_y`.\n",
        "\n",
        "    Process:\n",
        "    1.  **Simulate Paths:** Generates `N` sample paths of the PDMP, all starting\n",
        "        from `initial_capital_x`, and records the first-passage time `τ_y` and\n",
        "        the required injection `J_y` for each path.\n",
        "    2.  **Apply Estimator:** Calculates the sample mean of the total discounted\n",
        "        cost, `(J_y + vy_at_y) * exp(-δ * τ_y)`, across all `N` paths.\n",
        "\n",
        "    Args:\n",
        "        initial_capital_x: The starting capital `x`, where `x` > `y`.\n",
        "        threshold_y: The threshold `y` defining the intervention boundary.\n",
        "        vy_at_y: The pre-computed value of the function at the threshold, V^π_y(y).\n",
        "        active_params: The active model parameters for the simulation.\n",
        "        shock_sampler: The validated sampler for the active shock distribution.\n",
        "        rng: The seeded NumPy random number generator.\n",
        "        shock_arrival_rate: The Poisson arrival rate `λ`.\n",
        "        discount_rate: The continuous-time discount rate `δ`.\n",
        "        num_paths: The number of Monte Carlo paths `N` to simulate.\n",
        "        time_horizon: The simulation truncation time `T`.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the point estimate of V^π_y(x).\n",
        "    \"\"\"\n",
        "    # Input validation: This estimator is only valid for x > y.\n",
        "    if initial_capital_x <= threshold_y:\n",
        "        raise ValueError(\n",
        "            f\"This estimator is only for the continuation region (x > y), but \"\n",
        "            f\"received initial_capital_x={initial_capital_x} and \"\n",
        "            f\"threshold_y={threshold_y}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 1: Generate N paths from the initial capital x ---\n",
        "    # We reuse the exact same simulation helper from Task 9, changing only the\n",
        "    # starting point. This ensures methodological consistency.\n",
        "    tau_samples, injection_samples = _generate_first_passage_samples(\n",
        "        num_paths=num_paths,\n",
        "        initial_capital=initial_capital_x,\n",
        "        threshold_y=threshold_y,\n",
        "        active_params=active_params,\n",
        "        shock_sampler=shock_sampler,\n",
        "        rng=rng,\n",
        "        shock_arrival_rate=shock_arrival_rate,\n",
        "        time_horizon=time_horizon\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Retrieve the pre-computed threshold value V^π_y(y) ---\n",
        "    # The value `vy_at_y` is passed as an argument, making the dependency explicit.\n",
        "    # This is the continuation value of the process once it hits the threshold.\n",
        "\n",
        "    # --- Step 3: Apply the continuation estimator ---\n",
        "    # This estimator is a direct application of the law of total expectation.\n",
        "    # V(x) = E[ e^(-δ*τ_y) * (Cost at τ_y) ]\n",
        "    # Cost at τ_y = (Injection J_y) + (Continuation Value V(y))\n",
        "    # Equation (approximating Eq. 6.2): V^π_y(x) ≈ (1/N) * Σ[ (J_y,i + V^π_y(y)) * exp(-δ*τ_y,i) ]\n",
        "\n",
        "    # Calculate the discount factor e^(-δ*τ) for each path.\n",
        "    # This correctly handles truncated paths where τ = inf, as exp(-inf) = 0.\n",
        "    discount_factors = np.exp(-discount_rate * tau_samples)\n",
        "\n",
        "    # For each path, calculate the total cost incurred at the first-passage time.\n",
        "    # This is the sum of the immediate injection and the continuation value.\n",
        "    total_cost_at_passage = injection_samples + vy_at_y\n",
        "\n",
        "    # Calculate the discounted cost for each path.\n",
        "    discounted_costs = total_cost_at_passage * discount_factors\n",
        "\n",
        "    # The final estimate is the sample mean of these discounted costs.\n",
        "    estimated_value = np.mean(discounted_costs)\n",
        "\n",
        "    return estimated_value\n"
      ],
      "metadata": {
        "id": "mL201hmNwynt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11 — Implement batching and confidence interval computation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 11: Implement batching and confidence interval computation\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class MonteCarloResult:\n",
        "    \"\"\"\n",
        "    Immutable container for a complete Monte Carlo estimation result.\n",
        "\n",
        "    This object stores not only the point estimate but also its statistical\n",
        "    precision (the confidence interval) and a key diagnostic metric (the\n",
        "    truncation fraction).\n",
        "\n",
        "    Attributes:\n",
        "        point_estimate: The final aggregated point estimate for the quantity\n",
        "                        of interest (e.g., V(y)).\n",
        "        ci_lower: The lower bound of the confidence interval.\n",
        "        ci_upper: The upper bound of the confidence interval.\n",
        "        confidence_level: The confidence level (e.g., 0.99) for the interval.\n",
        "        truncation_fraction: The fraction of simulation paths that were\n",
        "                             truncated because they exceeded the time horizon.\n",
        "                             A high value may indicate simulation bias.\n",
        "    \"\"\"\n",
        "    # The final aggregated point estimate.\n",
        "    point_estimate: float\n",
        "\n",
        "    # The lower bound of the confidence interval.\n",
        "    ci_lower: float\n",
        "\n",
        "    # The upper bound of the confidence interval.\n",
        "    ci_upper: float\n",
        "\n",
        "    # The confidence level (e.g., 0.99) for the interval.\n",
        "    confidence_level: float\n",
        "\n",
        "    # The fraction of paths that were truncated.\n",
        "    truncation_fraction: float\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Step 1: Helper function to partition samples into batches\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _create_batches(\n",
        "    num_paths: int,\n",
        "    num_batches: int,\n",
        "    *sample_arrays: np.ndarray\n",
        ") -> Iterator[Tuple[np.ndarray, ...]]:\n",
        "    \"\"\"\n",
        "    Partitions large sample arrays into smaller, equal-sized batches.\n",
        "\n",
        "    This generator function yields tuples of array slices, where each tuple\n",
        "    represents one batch of data. It handles the partitioning logic and\n",
        "    validates that the resulting batches are large enough for statistical\n",
        "    inference.\n",
        "\n",
        "    Args:\n",
        "        num_paths: The total number of samples (N).\n",
        "        num_batches: The desired number of batches (B).\n",
        "        *sample_arrays: A variable number of NumPy arrays to be batched. All\n",
        "                        arrays must have length `num_paths`.\n",
        "\n",
        "    Yields:\n",
        "        A tuple of NumPy array slices, one for each input `sample_array`.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the configuration would result in batches that are too\n",
        "                    small for reliable statistical estimation.\n",
        "    \"\"\"\n",
        "    # --- Validation ---\n",
        "    # Ensure there are enough paths to create meaningful batches.\n",
        "    if num_paths < num_batches or num_batches <= 1:\n",
        "        raise ValueError(\n",
        "            f\"Invalid batch configuration: num_paths ({num_paths}) must be \"\n",
        "            f\"greater than num_batches ({num_batches}), and num_batches must be > 1.\"\n",
        "        )\n",
        "\n",
        "    # Calculate the size of each batch using integer division.\n",
        "    batch_size = num_paths // num_batches\n",
        "\n",
        "    # A common rule of thumb for the Central Limit Theorem to apply to batch means.\n",
        "    if batch_size < 30:\n",
        "        raise ValueError(\n",
        "            f\"Batch size ({batch_size}) is too small for reliable confidence \"\n",
        "            \"intervals. Increase num_paths or decrease num_batches.\"\n",
        "        )\n",
        "\n",
        "    # The total number of samples that will be used after discarding the remainder.\n",
        "    num_used_paths = batch_size * num_batches\n",
        "\n",
        "    # Iterate B times to yield B batches.\n",
        "    for i in range(num_batches):\n",
        "        # Calculate the start and end index for the current batch slice.\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        # Create a tuple of slices, one for each of the input sample arrays.\n",
        "        yield tuple(arr[start_idx:end_idx] for arr in sample_arrays)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 11, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def with_confidence_interval(\n",
        "    estimator_func: Callable[..., float],\n",
        "    tau_samples: np.ndarray,\n",
        "    injection_samples: np.ndarray,\n",
        "    num_batches: int,\n",
        "    confidence_level: float,\n",
        "    **estimator_kwargs\n",
        ") -> MonteCarloResult:\n",
        "    \"\"\"\n",
        "    A higher-order function that adds confidence intervals to a Monte Carlo estimator.\n",
        "\n",
        "    This function takes a point estimator and raw simulation data, and using\n",
        "    the method of batch means, calculates the point estimate, a confidence\n",
        "    interval for that estimate, and key diagnostic metrics.\n",
        "\n",
        "    Process:\n",
        "    1.  **Partition Samples:** The `N` total simulation paths are divided into\n",
        "        `B` smaller batches.\n",
        "    2.  **Compute Batch Means:** The provided `estimator_func` is applied to\n",
        "        each batch independently, yielding `B` separate estimates (the batch means).\n",
        "    3.  **Compute Statistics:** The overall mean and the sample standard deviation\n",
        "        of these `B` batch means are calculated.\n",
        "    4.  **Construct Interval:** A two-sided confidence interval is constructed\n",
        "        using the critical value from a t-distribution with `B-1` degrees of freedom.\n",
        "    5.  **Calculate Diagnostics:** The fraction of truncated paths is calculated.\n",
        "\n",
        "    Args:\n",
        "        estimator_func: The underlying Monte Carlo point estimator function\n",
        "                        (e.g., a wrapper around the logic from Task 9 or 10).\n",
        "                        It must accept `tau_samples` and `injection_samples`\n",
        "                        as its first two arguments.\n",
        "        tau_samples: An array of `N` first-passage times.\n",
        "        injection_samples: An array of `N` required injection amounts.\n",
        "        num_batches: The number of batches `B` to use.\n",
        "        confidence_level: The desired confidence level for the interval (e.g., 0.99).\n",
        "        **estimator_kwargs: Additional keyword arguments to pass to the\n",
        "                            `estimator_func`.\n",
        "\n",
        "    Returns:\n",
        "        A `MonteCarloResult` object containing the point estimate, confidence\n",
        "        interval bounds, and diagnostic information.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Partition the full sample into B batches ---\n",
        "    num_paths = len(tau_samples)\n",
        "    batches = _create_batches(\n",
        "        num_paths, num_batches, tau_samples, injection_samples\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Compute the point estimate for each batch ---\n",
        "    batch_means = np.array([\n",
        "        estimator_func(tau_batch, injection_batch, **estimator_kwargs)\n",
        "        for tau_batch, injection_batch in batches\n",
        "    ])\n",
        "\n",
        "    # --- Step 3: Compute overall statistics from the batch means ---\n",
        "    # The final point estimate is the mean of the batch means.\n",
        "    point_estimate = np.mean(batch_means)\n",
        "\n",
        "    # If there's no variation, the CI is just the point estimate.\n",
        "    if np.std(batch_means) == 0:\n",
        "        return MonteCarloResult(\n",
        "            point_estimate=point_estimate,\n",
        "            ci_lower=point_estimate,\n",
        "            ci_upper=point_estimate,\n",
        "            confidence_level=confidence_level,\n",
        "            truncation_fraction=np.mean(np.isinf(tau_samples))\n",
        "        )\n",
        "\n",
        "    # Calculate the sample standard deviation of the batch means.\n",
        "    # `ddof=1` provides the unbiased estimator s.\n",
        "    sample_std_dev = np.std(batch_means, ddof=1)\n",
        "\n",
        "    # --- Step 4: Construct the confidence interval ---\n",
        "    # Degrees of freedom for the t-distribution is B - 1.\n",
        "    df = num_batches - 1\n",
        "\n",
        "    # Calculate the two-sided critical t-value.\n",
        "    # For a 99% CI, we need the value at the 99.5th percentile.\n",
        "    alpha = 1 - confidence_level\n",
        "    t_critical = scipy.stats.t.ppf(1 - alpha / 2, df=df)\n",
        "\n",
        "    # The standard error of the mean of the batch means.\n",
        "    standard_error = sample_std_dev / np.sqrt(num_batches)\n",
        "\n",
        "    # The half-width (margin of error) of the confidence interval.\n",
        "    half_width = t_critical * standard_error\n",
        "\n",
        "    # The lower and upper bounds of the confidence interval.\n",
        "    ci_lower = point_estimate - half_width\n",
        "    ci_upper = point_estimate + half_width\n",
        "\n",
        "    # --- Step 5: Calculate Diagnostic Metrics ---\n",
        "    # The truncation fraction is the proportion of paths where τ was infinite.\n",
        "    truncation_fraction = np.mean(np.isinf(tau_samples))\n",
        "\n",
        "    # Package all results into the structured result object.\n",
        "    return MonteCarloResult(\n",
        "        point_estimate=point_estimate,\n",
        "        ci_lower=ci_lower,\n",
        "        ci_upper=ci_upper,\n",
        "        confidence_level=confidence_level,\n",
        "        truncation_fraction=truncation_fraction\n",
        "    )\n"
      ],
      "metadata": {
        "id": "X3YdS1XuyNFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12 — Implement closed-form evaluator for C(x) (Beta(α,1), no insurance)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 12: Implement closed-form evaluator for C(x)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class HypergeometricParams:\n",
        "    \"\"\"\n",
        "    Immutable container for the parameters of the Gaussian hypergeometric function.\n",
        "\n",
        "    These parameters (a, b, c) are derived from the core model parameters and\n",
        "    are used in the closed-form solution for the value functions.\n",
        "\n",
        "    Attributes:\n",
        "        a: The first parameter of the hypergeometric function, related to one\n",
        "           of the roots of the characteristic equation.\n",
        "        b: The second parameter of the hypergeometric function, related to the\n",
        "           other root of the characteristic equation.\n",
        "        c: The third parameter of the hypergeometric function, equal to α.\n",
        "    \"\"\"\n",
        "    # The first parameter of the hypergeometric function.\n",
        "    a: float\n",
        "\n",
        "    # The second parameter of the hypergeometric function.\n",
        "    b: float\n",
        "\n",
        "    # The third parameter of the hypergeometric function.\n",
        "    c: float\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Step 1: Helper function to compute hypergeometric parameters\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_hypergeometric_params(\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    alpha: float,\n",
        "    r: float\n",
        ") -> HypergeometricParams:\n",
        "    \"\"\"\n",
        "    Computes the parameters a, b, c for the hypergeometric function solution.\n",
        "\n",
        "    These parameters are derived from the model's core parameters (δ, λ, α, r)\n",
        "    and form the basis of the closed-form solution presented in Section 5 of\n",
        "    the research context.\n",
        "\n",
        "    Args:\n",
        "        delta: The continuous-time discount rate δ.\n",
        "        lambda_: The Poisson arrival rate of shocks λ.\n",
        "        alpha: The shape parameter α of the Beta(α, 1) distribution.\n",
        "        r: The capital growth rate.\n",
        "\n",
        "    Returns:\n",
        "        A `HypergeometricParams` object containing the computed a, b, and c.\n",
        "    \"\"\"\n",
        "    # Define the intermediate term k = δ + λ - αr for clarity.\n",
        "    k = delta + lambda_ - alpha * r\n",
        "\n",
        "    # The discriminant of the characteristic quadratic equation.\n",
        "    # This is guaranteed to be non-negative for positive model parameters.\n",
        "    discriminant = np.sqrt(k**2 + 4 * r * alpha * delta)\n",
        "\n",
        "    # Equation from Example 5.1: a and b are the roots of r*x^2 + k*x - α*δ = 0\n",
        "    # Parameter 'a' corresponds to the negative root.\n",
        "    param_a = (-k - discriminant) / (2 * r)\n",
        "\n",
        "    # Parameter 'b' corresponds to the positive root.\n",
        "    param_b = (-k + discriminant) / (2 * r)\n",
        "\n",
        "    # Parameter 'c' is simply the shape parameter α.\n",
        "    param_c = alpha\n",
        "\n",
        "    # Return the parameters in a structured, immutable dataclass.\n",
        "    return HypergeometricParams(a=param_a, b=param_b, c=param_c)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 12, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_c_closed_form(\n",
        "    x: Union[float, np.ndarray],\n",
        "    x_star: float,\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    alpha: float,\n",
        "    r: float\n",
        ") -> Union[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluates the closed-form solution for the cost of social protection, C(x).\n",
        "\n",
        "    This function implements the analytical solution for C(x) from Section 5,\n",
        "    Example 5.1, which is valid only for the specific case of a Beta(α, 1)\n",
        "    loss distribution and no microinsurance. It serves as a crucial benchmark\n",
        "    for validating the general-purpose Monte Carlo engine.\n",
        "\n",
        "    The function is piecewise:\n",
        "    1.  For `x <= x*`, it applies a linear formula representing the immediate\n",
        "        cost to reach the poverty line plus the expected future cost from there.\n",
        "    2.  For `x > x*`, it applies a more complex formula involving the Gaussian\n",
        "        hypergeometric function, which describes the expected cost until the\n",
        "        first time capital drops to the poverty line.\n",
        "\n",
        "    Args:\n",
        "        x: A scalar or NumPy array of capital levels at which to evaluate C(x).\n",
        "        x_star: The poverty line x*.\n",
        "        delta: The discount rate δ.\n",
        "        lambda_: The shock arrival rate λ.\n",
        "        alpha: The shape parameter α of the Beta(α, 1) distribution.\n",
        "        r: The capital growth rate.\n",
        "\n",
        "    Returns:\n",
        "        A scalar or NumPy array of the same shape as `x`, containing the\n",
        "        computed values of C(x).\n",
        "    \"\"\"\n",
        "    # --- Step 1: Compute the hypergeometric parameters ---\n",
        "    # These are required for the x > x* region.\n",
        "    hg_params = _compute_hypergeometric_params(delta, lambda_, alpha, r)\n",
        "\n",
        "    # --- Step 2: Evaluate C(x) for the region x <= x* ---\n",
        "    # First, calculate the constant value C(x*).\n",
        "    # Equation (2.2) adapted for Beta(α,1): C(x*) = λ(1-μ)x*/δ = λx*/((α+1)δ)\n",
        "    c_at_x_star = (lambda_ * x_star) / ((alpha + 1) * delta)\n",
        "\n",
        "    # Equation (2.3): C(x) = (x* - x) + C(x*) for x <= x*\n",
        "    # This represents the immediate injection (x* - x) plus the continuation value.\n",
        "    c_below_x_star = (x_star - x) + c_at_x_star\n",
        "\n",
        "    # --- Step 3: Evaluate C(x) for the region x > x* ---\n",
        "    # This region requires the hypergeometric function.\n",
        "\n",
        "    # The argument for the hypergeometric function is the ratio z = x*/x.\n",
        "    # We need to handle the case x=0 to avoid division by zero, though the\n",
        "    # logic below only applies this to x > x*, so x is never zero.\n",
        "    # A small epsilon is added for numerical stability if x can be very close to 0.\n",
        "    ratio_z = x_star / np.maximum(x, 1e-12)\n",
        "\n",
        "    # Evaluate the Gaussian hypergeometric function ₂F₁(a, b; c; z).\n",
        "    # The parameters are unpacked from the pre-computed dataclass.\n",
        "    hg_value = hyp2f1(\n",
        "        hg_params.b,\n",
        "        hg_params.b - hg_params.c + 1,\n",
        "        hg_params.b - hg_params.a + 1,\n",
        "        ratio_z\n",
        "    )\n",
        "\n",
        "    # Equation (5.1): C(x) = C(x*) * ₂F₁(...) * (x*/x)^b\n",
        "    # This formula gives the value in the continuation region.\n",
        "    c_above_x_star = c_at_x_star * hg_value * np.power(ratio_z, hg_params.b)\n",
        "\n",
        "    # --- Final Assembly using np.where for vectorization ---\n",
        "    # `np.where` is a highly efficient way to apply a piecewise function.\n",
        "    # If the condition (x <= x*) is true, it takes the value from `c_below_x_star`.\n",
        "    # Otherwise, it takes the value from `c_above_x_star`.\n",
        "    result = np.where(x <= x_star, c_below_x_star, c_above_x_star)\n",
        "\n",
        "    # If the input was a scalar float, return a scalar float.\n",
        "    return result.item() if isinstance(x, float) else result\n"
      ],
      "metadata": {
        "id": "XceIGf8Vz7CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13 — Implement closed-form evaluator for V_y(x) (Beta(α,1), no insurance)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 13: Implementation of closed-form evaluator for V_y(x)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Helper to compute V_y(y)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_vy_at_y_closed_form(\n",
        "    y: float,\n",
        "    x_star: float,\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    alpha: float,\n",
        "    r: float,\n",
        "    hg_params: 'HypergeometricParams'\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the closed-form value of V_y(y) based on the paper's Example 5.2.\n",
        "\n",
        "    This function implements the complex formula for V_y(y) derived from the\n",
        "    paper's formula for the auxiliary constant A(y). While the formula is\n",
        "    convoluted, this implementation is a high-fidelity translation of the source\n",
        "    material, used to determine the crucial value at the boundary.\n",
        "\n",
        "    Args:\n",
        "        y: The transfer threshold.\n",
        "        x_star: The poverty line.\n",
        "        delta: The discount rate.\n",
        "        lambda_: The shock arrival rate.\n",
        "        alpha: The shape parameter α of the Beta(α, 1) distribution.\n",
        "        r: The capital growth rate.\n",
        "        hg_params: The pre-computed hypergeometric parameters (a, b, c).\n",
        "\n",
        "    Returns:\n",
        "        The computed value of V_y(y). Returns np.inf if y is at the poverty line.\n",
        "    \"\"\"\n",
        "    # The formula is singular at y = x*. The cost is infinite for a zero buffer.\n",
        "    if np.isclose(y, x_star):\n",
        "        return np.inf\n",
        "\n",
        "    # Unpack hypergeometric parameters for clarity.\n",
        "    a, b, c = hg_params.a, hg_params.b, hg_params.c\n",
        "    ratio_z_y = x_star / y\n",
        "\n",
        "    # --- Compute Auxiliary Function A(y) from Paper's Example 5.2 ---\n",
        "    hyp2f1_val1 = hyp2f1(b, b - c + 1, b - a + 1, ratio_z_y)\n",
        "    hyp2f1_val2 = hyp2f1(b + 1, b - c + 1, b - a + 1, ratio_z_y)\n",
        "\n",
        "    # The formula for A(y) is:\n",
        "    # A(y) = (λy / (α+1)) * [ 1/(δ*₂F₁(b,..)) + 1/(r(y-x*)(b/y)*₂F₁(b+1,..)) ]\n",
        "    term1_denom = delta * hyp2f1_val1\n",
        "    if np.isclose(term1_denom, 0): return np.inf\n",
        "\n",
        "    term2_denom = r * (y - x_star) * (b / y) * hyp2f1_val2\n",
        "    if np.isclose(term2_denom, 0): return np.inf\n",
        "\n",
        "    const_A_y = (lambda_ * y / (alpha + 1)) * (1/term1_denom + 1/term2_denom)\n",
        "\n",
        "    # --- Compute V_y(y) ---\n",
        "    # From the paper's structure for x>y, V_y(x) = ₂F₁(b, ..., x*/x) * A(y) * (y/x)^b.\n",
        "    # Setting x=y gives V_y(y) = ₂F₁(b, ..., x*/y) * A(y).\n",
        "    vy_at_y = hyp2f1_val1 * const_A_y\n",
        "\n",
        "    return vy_at_y\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Step 2: Helper for the fundamental solution Psi(x)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _psi(\n",
        "    x: Union[float, np.ndarray],\n",
        "    x_star: float,\n",
        "    hg_params: 'HypergeometricParams'\n",
        ") -> Union[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes the fundamental decaying solution Ψ(x) of the IDE.\n",
        "\n",
        "    This function encapsulates the core mathematical structure of the value\n",
        "    function in the continuation region (x > y).\n",
        "\n",
        "    Equation: Ψ(x) = ₂F₁(b, b-c+1; b-a+1; x*/x) * (x*/x)^b\n",
        "\n",
        "    Args:\n",
        "        x: A scalar or NumPy array of capital levels.\n",
        "        x_star: The poverty line.\n",
        "        hg_params: The pre-computed hypergeometric parameters (a, b, c).\n",
        "\n",
        "    Returns:\n",
        "        The value(s) of the fundamental solution Ψ(x).\n",
        "    \"\"\"\n",
        "    # Unpack hypergeometric parameters.\n",
        "    a, b, c = hg_params.a, hg_params.b, hg_params.c\n",
        "\n",
        "    # The argument for the function is the ratio z = x*/x.\n",
        "    # Add a small epsilon to prevent division by zero if x contains 0.\n",
        "    ratio_z = x_star / np.maximum(x, 1e-12)\n",
        "\n",
        "    # Evaluate the Gaussian hypergeometric function part of the solution.\n",
        "    hg_val = hyp2f1(b, b - c + 1, b - a + 1, ratio_z)\n",
        "\n",
        "    # Evaluate the power law part of the solution.\n",
        "    power_val = np.power(ratio_z, b)\n",
        "\n",
        "    # The full solution is the product of the two parts.\n",
        "    return hg_val * power_val\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 13, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_vy_closed_form(\n",
        "    x: Union[float, np.ndarray],\n",
        "    y: float,\n",
        "    x_star: float,\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    alpha: float,\n",
        "    r: float\n",
        ") -> Union[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluates the closed-form solution for the threshold value function, V_y(x).\n",
        "\n",
        "    This function implements the analytical solution from Section 5,\n",
        "    Example 5.2, using a theoretically sound and numerically stable scaling\n",
        "    relationship. It is valid for a Beta(α, 1) loss distribution and no\n",
        "    microinsurance.\n",
        "\n",
        "    The function is piecewise:\n",
        "    1.  For `x <= y` (intervention region), it applies the linear formula from\n",
        "        Lemma 4.1: V_y(x) = (y - x) + V_y(y).\n",
        "    2.  For `x > y` (continuation region), it uses the scaling property of the\n",
        "        fundamental solution Ψ(x): V_y(x) = V_y(y) * Ψ(x) / Ψ(y).\n",
        "\n",
        "    Args:\n",
        "        x: A scalar or NumPy array of capital levels at which to evaluate V_y(x).\n",
        "        y: The transfer threshold y. Must be >= x*.\n",
        "        x_star: The poverty line x*.\n",
        "        delta: The discount rate δ.\n",
        "        lambda_: The shock arrival rate λ.\n",
        "        alpha: The shape parameter α of the Beta(α, 1) distribution.\n",
        "        r: The capital growth rate.\n",
        "\n",
        "    Returns:\n",
        "        A scalar or NumPy array of the same shape as `x`, containing the\n",
        "        computed values of V_y(x).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if y < x_star:\n",
        "        raise ValueError(f\"Threshold y ({y}) must be >= poverty line x* ({x_star}).\")\n",
        "    if np.any(np.asarray(x) < 0):\n",
        "        raise ValueError(\"Capital x cannot be negative.\")\n",
        "\n",
        "    # --- Step 1: Compute shared parameters ---\n",
        "    hg_params = _compute_hypergeometric_params(delta, lambda_, alpha, r)\n",
        "\n",
        "    # --- Step 2: Compute the crucial constant V_y(y) ---\n",
        "    # This value is determined by the boundary conditions at y. We use the\n",
        "    # helper function which implements the paper's complex formula for this constant.\n",
        "    vy_at_y = _compute_vy_at_y_closed_form(y, x_star, delta, lambda_, alpha, r, hg_params)\n",
        "\n",
        "    # --- Step 3: Evaluate V_y(x) for the region x <= y ---\n",
        "    # Equation from Lemma 4.1: V_y(x) = (y - x) + V_y(y)\n",
        "    vy_below_y = (y - x) + vy_at_y\n",
        "\n",
        "    # --- Step 4: Evaluate V_y(x) for the region x > y using scaling ---\n",
        "    # The solution in this region scales relative to its value at y via the\n",
        "    # fundamental solution Ψ(x). This is more numerically stable than using A(y).\n",
        "    # Calculate the value of the fundamental solution at the boundary y.\n",
        "    psi_at_y = _psi(y, x_star, hg_params)\n",
        "    if np.isclose(psi_at_y, 0):\n",
        "        # This case is unlikely but would lead to division by zero.\n",
        "        vy_above_y = np.inf\n",
        "    else:\n",
        "        # Calculate the value of the fundamental solution at the point(s) x.\n",
        "        psi_at_x = _psi(x, x_star, hg_params)\n",
        "        # Apply the scaling relationship: V_y(x) = V_y(y) * Ψ(x) / Ψ(y)\n",
        "        vy_above_y = vy_at_y * (psi_at_x / psi_at_y)\n",
        "\n",
        "    # --- Final Assembly using np.where for vectorization ---\n",
        "    result = np.where(x <= y, vy_below_y, vy_above_y)\n",
        "\n",
        "    # Ensure the output type matches the input type (scalar vs. array).\n",
        "    return result.item() if isinstance(x, float) else result\n"
      ],
      "metadata": {
        "id": "TKElyaGV1hsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14 — Implement baseline comparator D(x) (perpetual transfers)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 14: Implement baseline comparator D(x) (perpetual transfers)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 1: Helper for the analytical solution for D(x) where x <= x*\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _evaluate_d_below_x_star(\n",
        "    x: Union[float, np.ndarray],\n",
        "    active_x_star: float,\n",
        "    b: float,\n",
        "    delta: float,\n",
        "    lambda_: float,\n",
        "    base_mu: float\n",
        ") -> Union[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluates the closed-form solution for D(x) for capital at or below x*.\n",
        "\n",
        "    Args:\n",
        "        x: A scalar or NumPy array of capital levels.\n",
        "        active_x_star: The active poverty line (x* or x*^R).\n",
        "        b: The income generation rate.\n",
        "        delta: The discount rate.\n",
        "        lambda_: The shock arrival rate.\n",
        "        base_mu: The mean of the base loss distribution Z, E[Z].\n",
        "\n",
        "    Returns:\n",
        "        The computed value(s) of D(x).\n",
        "    \"\"\"\n",
        "    # Common denominator term from Proposition 2.1.\n",
        "    denominator = delta + lambda_ * (1 - base_mu)\n",
        "    if np.isclose(denominator, 0):\n",
        "        # This case should be prevented by delta > 0.\n",
        "        return np.inf\n",
        "\n",
        "    # Equation from Proposition 2.1: D(x*) = (b*x*/δ) * (λ(1-μ))/(δ+λ(1-μ))\n",
        "    d_at_x_star = (b * active_x_star / delta) * (lambda_ * (1 - base_mu) / denominator)\n",
        "\n",
        "    # Equation from Proposition 2.1: D(x) = D(x*) + (b/(δ+λ(1-μ))) * (x* - x)\n",
        "    d_below_x_star = d_at_x_star + (b / denominator) * (active_x_star - x)\n",
        "\n",
        "    return d_below_x_star\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Step 2: Helper to generate trapping time samples for x > x*\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_trapping_time_samples(\n",
        "    num_paths: int,\n",
        "    initial_capital: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator,\n",
        "    shock_arrival_rate: float,\n",
        "    time_horizon: float\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Generates N samples of trapping times and capital at trapping.\n",
        "\n",
        "    This is a specialized version of the simulation loop where the stopping\n",
        "    condition is the first time capital drops to or below the poverty line.\n",
        "\n",
        "    Args:\n",
        "        (Arguments are identical to _generate_first_passage_samples)\n",
        "\n",
        "    Returns:\n",
        "        A tuple of two NumPy arrays:\n",
        "        - `trapping_times`: An array of `N` first-passage times to x*.\n",
        "        - `capital_at_trapping`: An array of `N` capital levels at those times.\n",
        "    \"\"\"\n",
        "    # Pre-allocate arrays for results.\n",
        "    trapping_times = np.zeros(num_paths, dtype=np.float64)\n",
        "    capital_at_trapping = np.zeros(num_paths, dtype=np.float64)\n",
        "\n",
        "    # The trapping threshold is the active poverty line.\n",
        "    trapping_threshold = active_params.active_poverty_line\n",
        "\n",
        "    # Loop N times to generate N independent sample paths.\n",
        "    for i in range(num_paths):\n",
        "        # We reuse the generic single-path simulator from Task 8.\n",
        "        # The 'threshold_y' argument is now set to the poverty line.\n",
        "        tau, injection = simulate_single_pdmp_path(\n",
        "            initial_capital=initial_capital,\n",
        "            threshold_y=trapping_threshold,\n",
        "            active_params=active_params,\n",
        "            shock_sampler=shock_sampler,\n",
        "            rng=rng,\n",
        "            shock_arrival_rate=shock_arrival_rate,\n",
        "            time_horizon=time_horizon\n",
        "        )\n",
        "        # Store the trapping time.\n",
        "        trapping_times[i] = tau\n",
        "        # If the path was not truncated, calculate the capital at trapping.\n",
        "        if np.isfinite(tau):\n",
        "            # Capital at trapping X(τ) = y - J_y = x* - injection\n",
        "            capital_at_trapping[i] = trapping_threshold - injection\n",
        "        else:\n",
        "            # For truncated paths, the capital at trapping is irrelevant (cost is 0).\n",
        "            capital_at_trapping[i] = 0.0\n",
        "\n",
        "    return trapping_times, capital_at_trapping\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 14, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_d_comparator(\n",
        "    x_grid: np.ndarray,\n",
        "    active_params: 'ActiveParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluates the baseline comparator D(x) over a grid of capital levels.\n",
        "\n",
        "    This function computes the expected discounted cost of a policy of perpetual\n",
        "    regular transfers. It serves as an economic benchmark for the optimized\n",
        "    lump-sum transfer strategies. The function is hybrid:\n",
        "    1.  For `x <= x*`, it uses a closed-form analytical solution.\n",
        "    2.  For `x > x*`, it uses a recursive Monte Carlo method.\n",
        "\n",
        "    Args:\n",
        "        x_grid: A NumPy array of capital levels at which to evaluate D(x).\n",
        "        active_params: The active model parameters for the simulation.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        parsed_config: The fully parsed configuration object.\n",
        "        shock_sampler: The validated sampler for the active shock distribution.\n",
        "        rng: The seeded NumPy random number generator.\n",
        "\n",
        "    Returns:\n",
        "        A NumPy array of the same shape as `x_grid` containing the values of D(x).\n",
        "    \"\"\"\n",
        "    # Extract necessary parameters for clarity.\n",
        "    active_x_star = active_params.active_poverty_line\n",
        "    b = parsed_config.model_parameters.household_economic_parameters.primitive_params_for_r.income_generation_rate_b\n",
        "    delta = parsed_config.model_parameters.social_planner_parameters.discount_rate_delta\n",
        "    lambda_ = parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda\n",
        "    base_mu = base_quantities.expected_remaining_capital_mu\n",
        "    mc_settings = parsed_config.computation_parameters.monte_carlo_settings\n",
        "\n",
        "    # Pre-allocate the output array.\n",
        "    d_values = np.zeros_like(x_grid, dtype=np.float64)\n",
        "\n",
        "    # --- Step 1: Evaluate D(x) for x <= x* using the analytical formula ---\n",
        "    # Create a boolean mask for the part of the grid at or below the poverty line.\n",
        "    below_mask = x_grid <= active_x_star\n",
        "    # Apply the closed-form solution to this part of the grid.\n",
        "    d_values[below_mask] = _evaluate_d_below_x_star(\n",
        "        x=x_grid[below_mask],\n",
        "        active_x_star=active_x_star,\n",
        "        b=b, delta=delta, lambda_=lambda_, base_mu=base_mu\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Evaluate D(x) for x > x* using Monte Carlo recursion ---\n",
        "    # Create a mask for the part of the grid above the poverty line.\n",
        "    above_mask = x_grid > active_x_star\n",
        "\n",
        "    # Iterate through each unique capital level in the \"above\" region.\n",
        "    # Using unique values avoids re-computing for duplicate grid points.\n",
        "    for x_val in np.unique(x_grid[above_mask]):\n",
        "        # Generate N samples of trapping times and capital levels at trapping.\n",
        "        trapping_times, capital_at_trapping = _generate_trapping_time_samples(\n",
        "            num_paths=mc_settings.num_simulation_paths_N,\n",
        "            initial_capital=x_val,\n",
        "            active_params=active_params,\n",
        "            shock_sampler=shock_sampler,\n",
        "            rng=rng,\n",
        "            shock_arrival_rate=lambda_,\n",
        "            time_horizon=mc_settings.simulation_time_horizon_T\n",
        "        )\n",
        "\n",
        "        # For each path, calculate the terminal value D(X_τ) using the analytical formula.\n",
        "        terminal_d_values = _evaluate_d_below_x_star(\n",
        "            x=capital_at_trapping,\n",
        "            active_x_star=active_x_star,\n",
        "            b=b, delta=delta, lambda_=lambda_, base_mu=base_mu\n",
        "        )\n",
        "\n",
        "        # Calculate the discount factors for each path.\n",
        "        discount_factors = np.exp(-delta * trapping_times)\n",
        "\n",
        "        # The estimate for D(x) is the mean of the discounted terminal values.\n",
        "        # Equation: D(x) = E[ D(X_τ) * exp(-δ*τ) ]\n",
        "        d_estimate = np.mean(terminal_d_values * discount_factors)\n",
        "\n",
        "        # Assign the computed estimate to all corresponding points in the output grid.\n",
        "        d_values[x_grid == x_val] = d_estimate\n",
        "\n",
        "    return d_values\n"
      ],
      "metadata": {
        "id": "T_xh7VpQCOeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15 — Implement one-dimensional threshold optimizer\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 15: Implement one-dimensional threshold optimizer\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class OptimizationResult:\n",
        "    \"\"\"\n",
        "    Immutable container for the results of the threshold optimization.\n",
        "\n",
        "    Attributes:\n",
        "        optimal_threshold_y_star: The optimal transfer threshold y* that\n",
        "                                  minimizes the cost V_y(y).\n",
        "        min_value_vy_at_y_star: The value of the cost function at the optimum,\n",
        "                                V_{y*}(y*).\n",
        "        optimizer_result: The full result object returned by the SciPy optimizer,\n",
        "                          containing detailed convergence information.\n",
        "    \"\"\"\n",
        "    # The optimal transfer threshold y*.\n",
        "    optimal_threshold_y_star: float\n",
        "\n",
        "    # The value of the cost function at the optimum, V_{y*}(y*).\n",
        "    min_value_vy_at_y_star: float\n",
        "\n",
        "    # The full result object from the optimizer for diagnostics.\n",
        "    optimizer_result: OptimizeResult\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 15, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def find_optimal_threshold(\n",
        "    active_params: 'ActiveParameters',\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator\n",
        ") -> OptimizationResult:\n",
        "    \"\"\"\n",
        "    Finds the optimal transfer threshold y* by minimizing the value function V_y(y).\n",
        "\n",
        "    This function orchestrates the one-dimensional optimization process. It\n",
        "    constructs the objective function, `f(y) = V_y(y)`, based on the selected\n",
        "    computational method (Closed-Form or Monte Carlo) and then uses a robust\n",
        "    numerical optimizer (Brent's method) to find the minimum within a specified\n",
        "    bracket.\n",
        "\n",
        "    Process:\n",
        "    1.  **Define Objective Function:** Creates a callable `objective_func(y)` that\n",
        "        evaluates `V_y(y)` using the appropriate engine. For Monte Carlo, it\n",
        "        implements Common Random Numbers (CRN) by resetting the RNG seed for\n",
        "        each evaluation to ensure a smoother objective for the optimizer.\n",
        "    2.  **Invoke Optimizer:** Calls `scipy.optimize.minimize_scalar` with the\n",
        "        'bounded' Brent method, providing the objective function, search\n",
        "        bracket, and tolerances from the configuration.\n",
        "    3.  **Process and Validate Results:** Checks the optimizer's success flag and\n",
        "        warns if the solution is found at the boundary of the search bracket.\n",
        "        It then packages the optimal threshold `y*` and the minimum value\n",
        "        `V_{y*}(y*)` into a structured result object.\n",
        "\n",
        "    Args:\n",
        "        active_params: The active model parameters for the simulation.\n",
        "        parsed_config: The fully parsed configuration object.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        shock_sampler: The validated sampler for the active shock distribution.\n",
        "        rng: The main seeded NumPy random number generator for the pipeline.\n",
        "\n",
        "    Returns:\n",
        "        An `OptimizationResult` object containing the optimal threshold, the\n",
        "        minimum value, and detailed optimizer diagnostics.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the numerical optimizer fails to converge.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define the Objective Function f(y) = V_y(y) ---\n",
        "\n",
        "    # Extract parameters for the objective function.\n",
        "    mc_settings = parsed_config.computation_parameters.monte_carlo_settings\n",
        "    model_params = parsed_config.model_parameters\n",
        "\n",
        "    objective_func: Callable[[float], float]\n",
        "\n",
        "    if active_params.final_algorithm_choice == \"ClosedForm\":\n",
        "        # If using the closed-form solution, the objective function is deterministic.\n",
        "        # We use functools.partial to create a function of `y` only.\n",
        "        objective_func = functools.partial(\n",
        "            evaluate_vy_closed_form,\n",
        "            x=None, # x will be set to y inside the lambda\n",
        "            x_star=active_params.active_poverty_line,\n",
        "            delta=model_params.social_planner_parameters.discount_rate_delta,\n",
        "            lambda_=model_params.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "            alpha=model_params.stochastic_shock_parameters.loss_distribution_G_Z.parameters['alpha'],\n",
        "            r=active_params.active_growth_rate\n",
        "        )\n",
        "        # The final objective function sets x=y.\n",
        "        objective_func = lambda y: objective_func(x=y, y=y)\n",
        "\n",
        "    elif active_params.final_algorithm_choice == \"MonteCarlo\":\n",
        "        # If using Monte Carlo, the objective is stochastic.\n",
        "        # We must handle the random number generator carefully.\n",
        "\n",
        "        def mc_objective(y: float) -> float:\n",
        "            # --- Common Random Numbers (CRN) Implementation ---\n",
        "            # To stabilize the optimization of a stochastic objective, we use the\n",
        "            # same sequence of random numbers for each evaluation of f(y).\n",
        "            # We achieve this by creating a new, temporary RNG with a fixed seed\n",
        "            # for each call to the objective function.\n",
        "            crn_rng = np.random.default_rng(seed=mc_settings.random_seed)\n",
        "\n",
        "            # Call the estimator from Task 9.\n",
        "            return estimate_vy_at_threshold(\n",
        "                threshold_y=y,\n",
        "                active_params=active_params,\n",
        "                shock_sampler=shock_sampler,\n",
        "                rng=crn_rng, # Use the CRN-specific RNG\n",
        "                shock_arrival_rate=model_params.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "                discount_rate=model_params.social_planner_parameters.discount_rate_delta,\n",
        "                num_paths=mc_settings.num_simulation_paths_N,\n",
        "                time_horizon=mc_settings.simulation_time_horizon_T\n",
        "            )\n",
        "        objective_func = mc_objective\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"Optimizer not implemented for method '{active_params.final_algorithm_choice}'\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Invoke the Numerical Optimizer ---\n",
        "\n",
        "    # Extract optimizer settings from the configuration.\n",
        "    opt_settings = parsed_config.computation_parameters.optimizer_settings\n",
        "    search_bracket = active_params.optimizer_search_bracket\n",
        "\n",
        "    # Call SciPy's bounded scalar minimizer (Brent's method).\n",
        "    optimizer_result = minimize_scalar(\n",
        "        fun=objective_func,\n",
        "        method='bounded',\n",
        "        bounds=search_bracket,\n",
        "        options={\n",
        "            'xatol': opt_settings.y_tolerance_xtol,\n",
        "            'maxiter': opt_settings.max_iterations\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Process and Validate Optimization Results ---\n",
        "\n",
        "    # Check if the optimizer reported successful convergence.\n",
        "    if not optimizer_result.success:\n",
        "        raise RuntimeError(\n",
        "            \"Optimal threshold search failed to converge. \"\n",
        "            f\"Message: {optimizer_result.message}\"\n",
        "        )\n",
        "\n",
        "    # Extract the optimal threshold and the minimum value found.\n",
        "    optimal_y = optimizer_result.x\n",
        "    min_value = optimizer_result.fun\n",
        "\n",
        "    # Issue a warning if the optimum is found at the boundary of the search bracket,\n",
        "    # as this may indicate the true minimum is outside the bracket.\n",
        "    if np.isclose(optimal_y, search_bracket[0], atol=opt_settings.y_tolerance_xtol) or \\\n",
        "       np.isclose(optimal_y, search_bracket[1], atol=opt_settings.y_tolerance_xtol):\n",
        "        warnings.warn(\n",
        "            f\"Optimal threshold y* ({optimal_y:.4f}) was found at the boundary \"\n",
        "            f\"of the search bracket {search_bracket}. The true optimum may lie \"\n",
        "            \"outside this range. Consider widening the 'y_max_factor_times_y_min'.\",\n",
        "            RuntimeWarning\n",
        "        )\n",
        "\n",
        "    # Package the results into a structured, immutable dataclass.\n",
        "    return OptimizationResult(\n",
        "        optimal_threshold_y_star=optimal_y,\n",
        "        min_value_vy_at_y_star=min_value,\n",
        "        optimizer_result=optimizer_result\n",
        "    )\n"
      ],
      "metadata": {
        "id": "xmheGTs8FxHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16 — Compute V<sub>y</sub>(x) over the capital grid\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 16: Compute V_{y*}(x) over the capital grid\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1: Dataclass Schema to propagate diagnostics\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ValueFunctionResult:\n",
        "    \"\"\"\n",
        "    Immutable container for a value function evaluated over a capital grid.\n",
        "\n",
        "    This object stores the complete results of a value function evaluation across\n",
        "    a discrete grid of capital levels. This version is updated to\n",
        "    include the `truncation_fraction` diagnostic, which is critical for assessing\n",
        "    the quality and potential bias of Monte Carlo-based estimates.\n",
        "\n",
        "    Attributes:\n",
        "        capital_grid: The 1D NumPy array of capital levels (the x-axis) over\n",
        "                      which the function was evaluated.\n",
        "        point_estimates: A 1D NumPy array of the value function's point\n",
        "                         estimates, V(x), corresponding to each point in the\n",
        "                         `capital_grid`.\n",
        "        ci_lower: An optional 1D NumPy array for the lower bound of the\n",
        "                  confidence interval at each grid point. This is populated\n",
        "                  for Monte Carlo estimates and is `None` for deterministic\n",
        "                  (closed-form) calculations.\n",
        "        ci_upper: An optional 1D NumPy array for the upper bound of the\n",
        "                  confidence interval at each grid point.\n",
        "        confidence_level: The confidence level (e.g., 0.99) used to construct\n",
        "                          the interval. `None` for deterministic calculations.\n",
        "        truncation_fraction: An optional float representing the fraction of\n",
        "                             simulation paths that were truncated because they\n",
        "                             exceeded the time horizon `T`. A high value may\n",
        "                             indicate simulation bias. `None` for deterministic\n",
        "                             calculations.\n",
        "    \"\"\"\n",
        "    # The 1D NumPy array of capital levels (x-axis).\n",
        "    capital_grid: np.ndarray\n",
        "\n",
        "    # A 1D NumPy array of the value function's point estimates, V(x).\n",
        "    point_estimates: np.ndarray\n",
        "\n",
        "    # An optional 1D NumPy array for the lower bound of the confidence interval.\n",
        "    ci_lower: Optional[np.ndarray]\n",
        "\n",
        "    # An optional 1D NumPy array for the upper bound of the confidence interval.\n",
        "    ci_upper: Optional[np.ndarray]\n",
        "\n",
        "    # The confidence level (e.g., 0.99) used for the interval.\n",
        "    confidence_level: Optional[float]\n",
        "\n",
        "    # The fraction of simulation paths that were truncated.\n",
        "    truncation_fraction: Optional[float]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 16, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_vy_star_on_grid(\n",
        "    opt_result: 'OptimizationResult',\n",
        "    active_params: 'ActiveParameters',\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator\n",
        ") -> ValueFunctionResult:\n",
        "    \"\"\"\n",
        "    Evaluates the optimal value function V_{y*}(x) over a capital grid (Amended).\n",
        "\n",
        "    This amended function takes the result of the optimization (the optimal\n",
        "    threshold y*) and computes the value function for that policy across a\n",
        "    predefined grid. It now correctly propagates the `truncation_fraction`\n",
        "    diagnostic from the Monte Carlo engine to the final result object for each\n",
        "    point on the grid, enabling more granular convergence analysis.\n",
        "\n",
        "    Process:\n",
        "    1.  **Grid Creation:** Creates the capital grid based on configuration.\n",
        "    2.  **Piecewise Evaluation:**\n",
        "        a.  For `x <= y*`, it applies the deterministic linear formula.\n",
        "        b.  For `x > y*`, it dispatches to the correct evaluation engine.\n",
        "            - **Monte Carlo:** For each grid point, it runs a full simulation,\n",
        "              captures the `truncation_fraction` from the `MonteCarloResult`,\n",
        "              and stores it in an array.\n",
        "    3.  **Result Aggregation:** Assembles the point estimates, confidence\n",
        "        intervals, and point-wise truncation fractions into a `ValueFunctionResult`.\n",
        "\n",
        "    Args:\n",
        "        opt_result: The result from the `find_optimal_threshold` function.\n",
        "        active_params: The active model parameters for the simulation.\n",
        "        parsed_config: The fully parsed configuration object.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        shock_sampler: The validated sampler for the active shock distribution.\n",
        "        rng: The seeded NumPy random number generator.\n",
        "\n",
        "    Returns:\n",
        "        A `ValueFunctionResult` object containing the evaluated optimal value\n",
        "        function, its confidence interval, and the truncation diagnostic.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Extract parameters and create the evaluation grid ---\n",
        "    # Get the optimal threshold y* and the value at the optimum V(y*) from the optimization result.\n",
        "    y_star = opt_result.optimal_threshold_y_star\n",
        "    vy_at_y_star = opt_result.min_value_vy_at_y_star\n",
        "\n",
        "    # Define the capital grid (x-axis) for the evaluation based on the output configuration.\n",
        "    grid_cfg = parsed_config.output_parameters.capital_grid_for_plots\n",
        "    x_grid = np.linspace(grid_cfg.start_value, grid_cfg.stop_value, grid_cfg.num_points)\n",
        "\n",
        "    # Pre-allocate result arrays. Initialize with appropriate defaults (0 or NaN).\n",
        "    point_estimates = np.zeros_like(x_grid)\n",
        "    ci_lower = np.full_like(x_grid, np.nan)\n",
        "    ci_upper = np.full_like(x_grid, np.nan)\n",
        "    truncation_fractions = np.full_like(x_grid, np.nan) # New array for diagnostics\n",
        "\n",
        "    # --- Step 2a: Evaluate the intervention region (x <= y*) ---\n",
        "    # Identify the part of the grid at or below the optimal threshold y*.\n",
        "    below_mask = x_grid <= y_star\n",
        "\n",
        "    # Equation from Lemma 4.1: V_y*(x) = (y* - x) + V_y*(y*)\n",
        "    # Apply the linear formula for the cost in the intervention region.\n",
        "    point_estimates[below_mask] = (y_star - x_grid[below_mask]) + vy_at_y_star\n",
        "\n",
        "    # This region is deterministic, so the confidence interval is a single point.\n",
        "    ci_lower[below_mask] = point_estimates[below_mask]\n",
        "    ci_upper[below_mask] = point_estimates[below_mask]\n",
        "\n",
        "    # Truncation is not applicable to the deterministic part, so set fraction to 0.\n",
        "    truncation_fractions[below_mask] = 0.0\n",
        "\n",
        "    # --- Step 2b: Evaluate the continuation region (x > y*) ---\n",
        "    # Identify the part of the grid above the optimal threshold y*.\n",
        "    above_mask = x_grid > y_star\n",
        "    # Get the unique capital levels in this region to avoid redundant computations.\n",
        "    unique_x_above = np.unique(x_grid[above_mask])\n",
        "\n",
        "    # Dispatch to the appropriate evaluation method based on the active configuration.\n",
        "    if active_params.final_algorithm_choice == \"ClosedForm\":\n",
        "        # --- Closed-Form Branch ---\n",
        "        model_params = parsed_config.model_parameters\n",
        "        # Call the vectorized closed-form evaluator from Task 13.\n",
        "        vy_star_above = evaluate_vy_closed_form(\n",
        "            x=unique_x_above, y=y_star,\n",
        "            x_star=active_params.active_poverty_line,\n",
        "            delta=model_params.social_planner_parameters.discount_rate_delta,\n",
        "            lambda_=model_params.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "            alpha=model_params.stochastic_shock_parameters.loss_distribution_G_Z.parameters['alpha'],\n",
        "            r=active_params.active_growth_rate\n",
        "        )\n",
        "        # Map the results from the unique points back to the full grid structure.\n",
        "        for i, x_val in enumerate(unique_x_above):\n",
        "            indices = x_grid == x_val\n",
        "            point_estimates[indices] = vy_star_above[i]\n",
        "            ci_lower[indices] = vy_star_above[i]\n",
        "            ci_upper[indices] = vy_star_above[i]\n",
        "            truncation_fractions[indices] = 0.0 # No truncation in closed-form.\n",
        "\n",
        "    elif active_params.final_algorithm_choice == \"MonteCarlo\":\n",
        "        # --- Monte Carlo Branch ---\n",
        "        mc_settings = parsed_config.computation_parameters.monte_carlo_settings\n",
        "        model_params = parsed_config.model_parameters\n",
        "        delta = model_params.social_planner_parameters.discount_rate_delta\n",
        "\n",
        "        # Define a simple, pure function to estimate the value from a *batch* of samples.\n",
        "        def batch_estimator(\n",
        "            tau_batch: np.ndarray, injection_batch: np.ndarray,\n",
        "            continuation_value: float, discount_rate: float\n",
        "        ) -> float:\n",
        "            # This implements the core logic of the Task 10 estimator on a batch.\n",
        "            discount_factors = np.exp(-discount_rate * tau_batch)\n",
        "            total_cost_at_passage = injection_batch + continuation_value\n",
        "            return np.mean(total_cost_at_passage * discount_factors)\n",
        "\n",
        "        # Loop through each unique capital level that needs a Monte Carlo estimation.\n",
        "        for x_val in unique_x_above:\n",
        "            # Generate N samples ONCE for the current grid point x_val.\n",
        "            tau_samples, injection_samples = _generate_first_passage_samples(\n",
        "                num_paths=mc_settings.num_simulation_paths_N,\n",
        "                initial_capital=x_val, threshold_y=y_star,\n",
        "                active_params=active_params, shock_sampler=shock_sampler, rng=rng,\n",
        "                shock_arrival_rate=model_params.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "                time_horizon=mc_settings.simulation_time_horizon_T\n",
        "            )\n",
        "\n",
        "            # Create the specific batch estimator for this context using functools.partial.\n",
        "            specific_batch_estimator = functools.partial(\n",
        "                batch_estimator,\n",
        "                continuation_value=vy_at_y_star,\n",
        "                discount_rate=delta\n",
        "            )\n",
        "\n",
        "            # Call the CI wrapper, which returns the full MonteCarloResult object.\n",
        "            mc_result = with_confidence_interval(\n",
        "                estimator_func=specific_batch_estimator,\n",
        "                tau_samples=tau_samples,\n",
        "                injection_samples=injection_samples,\n",
        "                num_batches=mc_settings.num_batches_for_CI,\n",
        "                confidence_level=mc_settings.confidence_level\n",
        "            )\n",
        "\n",
        "            # Store all results, now including the point-wise truncation fraction.\n",
        "            indices = x_grid == x_val\n",
        "            point_estimates[indices] = mc_result.point_estimate\n",
        "            ci_lower[indices] = mc_result.ci_lower\n",
        "            ci_upper[indices] = mc_result.ci_upper\n",
        "            truncation_fractions[indices] = mc_result.truncation_fraction # Store the diagnostic\n",
        "\n",
        "    # --- Step 3: Assemble and return the final result object ---\n",
        "    # The final object now contains the complete set of results and diagnostics.\n",
        "    return ValueFunctionResult(\n",
        "        capital_grid=x_grid,\n",
        "        point_estimates=point_estimates,\n",
        "        ci_lower=ci_lower if active_params.final_algorithm_choice == \"MonteCarlo\" else None,\n",
        "        ci_upper=ci_upper if active_params.final_algorithm_choice == \"MonteCarlo\" else None,\n",
        "        confidence_level=mc_settings.confidence_level if active_params.final_algorithm_choice == \"MonteCarlo\" else None,\n",
        "        truncation_fraction=truncation_fractions if active_params.final_algorithm_choice == \"MonteCarlo\" else None\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "xAJebAU7HyAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17 — Verification: check tail limits\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 17: Verification: check tail limits\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Step 1 & 2: Generic helper to check the tail limit of a value function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _check_tail_limit(\n",
        "    result: 'ValueFunctionResult',\n",
        "    function_name: str,\n",
        "    tail_fraction: float = 0.2,\n",
        "    tolerance: float = 1e-3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Verifies that a computed value function decays to zero for large capital.\n",
        "\n",
        "    This function checks a fundamental theoretical property of the value\n",
        "    functions (V(x) and C(x)): they must approach zero as capital x approaches\n",
        "    infinity. This is verified numerically by checking that the function's\n",
        "    values in the upper tail of the capital grid are close to zero.\n",
        "\n",
        "    Args:\n",
        "        result: The `ValueFunctionResult` object containing the grid and estimates.\n",
        "        function_name: The name of the function (e.g., \"V_y_star(x)\") for use\n",
        "                       in warning messages.\n",
        "        tail_fraction: The fraction of the grid to consider as the \"tail\"\n",
        "                       (e.g., 0.2 means the top 20%).\n",
        "        tolerance: The maximum absolute value allowed in the tail region.\n",
        "\n",
        "    Returns:\n",
        "        None. Issues a `RuntimeWarning` if the check fails.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define the tail region of the grid ---\n",
        "    # Calculate the number of points in the grid.\n",
        "    num_points = len(result.capital_grid)\n",
        "    # Determine the starting index of the tail region.\n",
        "    tail_start_index = int(num_points * (1 - tail_fraction))\n",
        "\n",
        "    # Ensure there's at least one point in the tail to check.\n",
        "    if tail_start_index >= num_points - 1:\n",
        "        warnings.warn(\n",
        "            f\"Tail limit check for '{function_name}' skipped: the grid is too \"\n",
        "            f\"small or the tail_fraction ({tail_fraction}) is too large to \"\n",
        "            \"define a meaningful tail region.\",\n",
        "            RuntimeWarning\n",
        "        )\n",
        "        return\n",
        "\n",
        "    # --- Step 2: Extract tail values and check the condition ---\n",
        "    # Get the point estimates corresponding to the tail of the grid.\n",
        "    tail_values = result.point_estimates[tail_start_index:]\n",
        "\n",
        "    # Find the maximum absolute value in the tail. This should be close to zero.\n",
        "    max_abs_tail_value = np.max(np.abs(tail_values))\n",
        "\n",
        "    # --- Step 3: Issue a warning if the check fails ---\n",
        "    # Compare the maximum tail value against the specified tolerance.\n",
        "    if max_abs_tail_value > tolerance:\n",
        "        # If the value is too large, it suggests a potential issue.\n",
        "        # The warning message is constructed to be highly informative and actionable.\n",
        "        warnings.warn(\n",
        "            f\"Tail limit verification failed for '{function_name}'. \"\n",
        "            f\"The maximum absolute value in the top {tail_fraction*100:.0f}% of the \"\n",
        "            f\"capital grid was {max_abs_tail_value:.4f}, which exceeds the \"\n",
        "            f\"tolerance of {tolerance:.4f}. This may indicate that the grid \"\n",
        "            \"range ('stop_value') or the simulation horizon ('time_horizon') \"\n",
        "            \"is too small for the function to decay properly.\",\n",
        "            RuntimeWarning\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 17, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def verify_tail_limits(\n",
        "    results_to_check: List[Tuple['ValueFunctionResult', str]],\n",
        "    tail_fraction: float = 0.2,\n",
        "    tolerance: float = 1e-3\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the tail limit verification for multiple value functions.\n",
        "\n",
        "    This function iterates through a list of computed value function results\n",
        "    and applies a verification check to each one, ensuring they all satisfy\n",
        "    the theoretical property of decaying to zero for large capital levels.\n",
        "\n",
        "    This verification is a crucial sanity check on the correctness of both the\n",
        "    closed-form and Monte Carlo implementations. A failure suggests that the\n",
        "    computational domain (either the capital grid or the time horizon) may be\n",
        "    insufficient, or that there is a more fundamental error in the model\n",
        "    implementation.\n",
        "\n",
        "    Args:\n",
        "        results_to_check: A list of tuples, where each tuple contains a\n",
        "                          `ValueFunctionResult` object and a string with the\n",
        "                          function's name for logging purposes.\n",
        "        tail_fraction: The fraction of the grid to consider as the \"tail\".\n",
        "        tolerance: The maximum absolute value allowed in the tail region.\n",
        "\n",
        "    Returns:\n",
        "        None. Issues `RuntimeWarning`s via the `_check_tail_limit` helper if\n",
        "        any of the verification checks fail.\n",
        "    \"\"\"\n",
        "    # Input validation.\n",
        "    if not (0 < tail_fraction < 1):\n",
        "        raise ValueError(f\"tail_fraction must be in (0, 1), but got {tail_fraction}.\")\n",
        "    if not tolerance > 0:\n",
        "        raise ValueError(f\"tolerance must be > 0, but got {tolerance}.\")\n",
        "\n",
        "    # Iterate through the list of value function results provided.\n",
        "    for result, function_name in results_to_check:\n",
        "        # Check if the result object is valid before proceeding.\n",
        "        if result is None or result.point_estimates is None:\n",
        "            warnings.warn(\n",
        "                f\"Skipping tail limit check for '{function_name}' because no \"\n",
        "                \"valid result object was provided.\",\n",
        "                RuntimeWarning\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Call the generic helper function to perform the check for each function.\n",
        "        _check_tail_limit(\n",
        "            result=result,\n",
        "            function_name=function_name,\n",
        "            tail_fraction=tail_fraction,\n",
        "            tolerance=tolerance\n",
        "        )\n"
      ],
      "metadata": {
        "id": "3JDka1qJJlxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18 — Verification: cross-validate Monte Carlo vs closed form (if applicable)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 18: Verification: cross-validate Monte Carlo vs closed form\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class CrossValidationResult:\n",
        "    \"\"\"\n",
        "    Immutable container for the results of the cross-validation check.\n",
        "\n",
        "    This object stores a comprehensive set of metrics comparing the Monte Carlo\n",
        "    estimates against the analytical closed-form solution, providing a powerful\n",
        "    diagnostic on the health and accuracy of the simulation engine.\n",
        "\n",
        "    Attributes:\n",
        "        passed: A boolean flag indicating if the validation passed based on\n",
        "                pre-defined criteria (CI coverage and mean error).\n",
        "        ci_coverage: The fraction of grid points where the analytical value\n",
        "                     lies within the Monte Carlo confidence interval.\n",
        "        mean_absolute_error: The mean absolute error (MAE) between the two methods.\n",
        "        mean_absolute_percentage_error: The mean absolute percentage error (MAPE).\n",
        "        max_absolute_error: The maximum absolute error observed on the grid.\n",
        "    \"\"\"\n",
        "    # A boolean flag indicating if the validation passed.\n",
        "    passed: bool\n",
        "\n",
        "    # The fraction of grid points where the analytical value is within the CI.\n",
        "    ci_coverage: float\n",
        "\n",
        "    # The mean absolute error (MAE) between the two methods.\n",
        "    mean_absolute_error: float\n",
        "\n",
        "    # The mean absolute percentage error (MAPE).\n",
        "    mean_absolute_percentage_error: float\n",
        "\n",
        "    # The maximum absolute error observed on the grid.\n",
        "    max_absolute_error: float\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 18, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def cross_validate_mc_vs_closed_form(\n",
        "    mc_result: 'ValueFunctionResult',\n",
        "    cf_result: 'ValueFunctionResult',\n",
        "    coverage_threshold: float = 0.95,\n",
        "    mape_threshold: float = 0.05\n",
        ") -> CrossValidationResult:\n",
        "    \"\"\"\n",
        "    Performs a rigorous cross-validation of Monte Carlo vs. closed-form results.\n",
        "\n",
        "    This function is a critical verification step. It compares the results from\n",
        "    the general-purpose Monte Carlo engine against the known analytical solution\n",
        "    (when available) to detect potential systematic biases or implementation\n",
        "    errors in the simulation code.\n",
        "\n",
        "    Process:\n",
        "    1.  **Error Calculation:** Computes point-wise absolute and relative errors\n",
        "        between the Monte Carlo point estimates and the closed-form values.\n",
        "    2.  **CI Coverage Check:** Determines the fraction of grid points where the\n",
        "        analytical \"ground truth\" value falls within the computed Monte Carlo\n",
        "        confidence interval.\n",
        "    3.  **Summary Metrics:** Aggregates the errors into summary statistics like\n",
        "        Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE).\n",
        "    4.  **Pass/Fail Assessment:** Compares the CI coverage and MAPE against\n",
        "        pre-defined thresholds to issue a final pass/fail verdict.\n",
        "\n",
        "    Args:\n",
        "        mc_result: The `ValueFunctionResult` object from the Monte Carlo evaluation.\n",
        "        cf_result: The `ValueFunctionResult` object from the closed-form evaluation.\n",
        "        coverage_threshold: The minimum acceptable CI coverage fraction for the\n",
        "                            validation to pass.\n",
        "        mape_threshold: The maximum acceptable Mean Absolute Percentage Error\n",
        "                        for the validation to pass.\n",
        "\n",
        "    Returns:\n",
        "        A `CrossValidationResult` object containing detailed comparison metrics\n",
        "        and a final pass/fail assessment.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure both results are based on the same capital grid.\n",
        "    if not np.array_equal(mc_result.capital_grid, cf_result.capital_grid):\n",
        "        raise ValueError(\"Cannot cross-validate results on different capital grids.\")\n",
        "    if mc_result.ci_lower is None or mc_result.ci_upper is None:\n",
        "        raise ValueError(\"Monte Carlo result must include confidence intervals for validation.\")\n",
        "\n",
        "    # Extract the core data arrays for comparison.\n",
        "    mc_estimates = mc_result.point_estimates\n",
        "    cf_values = cf_result.point_estimates\n",
        "\n",
        "    # --- Step 1: Error Calculation ---\n",
        "    # Calculate the absolute error at each point on the grid.\n",
        "    absolute_error = np.abs(mc_estimates - cf_values)\n",
        "\n",
        "    # Calculate the relative error. Add a small epsilon to the denominator to\n",
        "    # avoid division by zero where the true value is zero.\n",
        "    relative_error = absolute_error / (np.abs(cf_values) + 1e-9)\n",
        "\n",
        "    # --- Step 2: CI Coverage Check ---\n",
        "    # Check where the closed-form value is within the MC confidence interval.\n",
        "    # This is the most important check for unbiasedness.\n",
        "    is_covered = (cf_values >= mc_result.ci_lower) & (cf_values <= mc_result.ci_upper)\n",
        "\n",
        "    # Calculate the fraction of points that are covered.\n",
        "    ci_coverage = np.mean(is_covered)\n",
        "\n",
        "    # --- Step 3: Summary Metrics ---\n",
        "    # Calculate Mean Absolute Error (MAE).\n",
        "    mean_absolute_error = np.mean(absolute_error)\n",
        "\n",
        "    # Calculate Mean Absolute Percentage Error (MAPE).\n",
        "    mean_absolute_percentage_error = np.mean(relative_error)\n",
        "\n",
        "    # Find the maximum absolute error.\n",
        "    max_absolute_error = np.max(absolute_error)\n",
        "\n",
        "    # --- Step 4: Pass/Fail Assessment ---\n",
        "    # The validation passes if CI coverage is high and the average error is low.\n",
        "    # These thresholds define the acceptable margin of error for the MC engine.\n",
        "    passed = (ci_coverage >= coverage_threshold) and (mean_absolute_percentage_error <= mape_threshold)\n",
        "\n",
        "    # Issue a warning if the validation fails, providing context.\n",
        "    if not passed:\n",
        "        warnings.warn(\n",
        "            \"Monte Carlo engine cross-validation FAILED. \"\n",
        "            f\"CI Coverage: {ci_coverage:.2%} (Threshold: {coverage_threshold:.0%}). \"\n",
        "            f\"MAPE: {mean_absolute_percentage_error:.2%} (Threshold: {mape_threshold:.0%}). \"\n",
        "            \"This may indicate a systematic bias in the simulation or estimation logic.\",\n",
        "            UserWarning\n",
        "        )\n",
        "\n",
        "    # Package all metrics into the structured result object.\n",
        "    return CrossValidationResult(\n",
        "        passed=passed,\n",
        "        ci_coverage=ci_coverage,\n",
        "        mean_absolute_error=mean_absolute_error,\n",
        "        mean_absolute_percentage_error=mean_absolute_percentage_error,\n",
        "        max_absolute_error=max_absolute_error\n",
        "    )\n"
      ],
      "metadata": {
        "id": "NIaeMhuXLUGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19 — Optional: implement fixed-point solver for T(W)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 19: Optional: implement fixed-point solver for T(W)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class FixedPointResult:\n",
        "    \"\"\"\n",
        "    Immutable container for the results of the fixed-point iteration solver.\n",
        "\n",
        "    This class encapsulates the complete output of the `solve_vy_fixed_point`\n",
        "    function. It provides not only the final computed value function but also\n",
        "    critical diagnostic information about the convergence and performance of\n",
        "    the iterative numerical method.\n",
        "\n",
        "    Attributes:\n",
        "        value_function_result: The final `ValueFunctionResult` object, which\n",
        "                               contains the computed value function V_y(x) on\n",
        "                               the desired output grid.\n",
        "        converged: A boolean flag that is `True` if the fixed-point iteration\n",
        "                   converged to the specified tolerance within the maximum\n",
        "                   number of iterations, and `False` otherwise.\n",
        "        iterations: The total number of iterations performed by the solver\n",
        "                    before termination (either by convergence or by reaching\n",
        "                    the maximum limit).\n",
        "        final_error: The final supremum norm error, calculated as the maximum\n",
        "                     absolute difference between the last two iterates\n",
        "                     (max|W_k+1 - W_k|). This value quantifies the precision\n",
        "                     of the converged solution.\n",
        "    \"\"\"\n",
        "    # The final `ValueFunctionResult` object containing the converged value function.\n",
        "    value_function_result: 'ValueFunctionResult'\n",
        "\n",
        "    # A boolean flag indicating if the iteration converged successfully.\n",
        "    converged: bool\n",
        "\n",
        "    # The number of iterations the solver performed.\n",
        "    iterations: int\n",
        "\n",
        "    # The final supremum norm error, indicating the precision of the solution.\n",
        "    final_error: float\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Step 2: Helper function to apply the T(W) operator\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _apply_operator_T(\n",
        "    x: float,\n",
        "    y: float,\n",
        "    W_k: np.ndarray,\n",
        "    x_grid_fp: np.ndarray,\n",
        "    active_params: 'ActiveParameters',\n",
        "    shock_pdf: Callable[[float], float],\n",
        "    lambda_: float,\n",
        "    delta: float\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies the fixed-point operator T(W) at a single point x.\n",
        "\n",
        "    This function computes the value of T(W)(x) by performing a nested numerical\n",
        "    integration over the time to the next shock (τ₁) and the shock size (Z₁).\n",
        "\n",
        "    Args:\n",
        "        x: The capital level at which to apply the operator.\n",
        "        y: The transfer threshold.\n",
        "        W_k: The current estimate of the value function on the discrete grid.\n",
        "        x_grid_fp: The discrete grid corresponding to W_k.\n",
        "        active_params: The active model parameters.\n",
        "        shock_pdf: The PDF of the active shock distribution.\n",
        "        lambda_: The shock arrival rate.\n",
        "        delta: The discount rate.\n",
        "\n",
        "    Returns:\n",
        "        The value of T(W_k)(x).\n",
        "    \"\"\"\n",
        "    # The value at the boundary, W(y), is found by interpolation.\n",
        "    # `left` and `right` handle extrapolation if y is outside the grid.\n",
        "    W_at_y = np.interp(y, x_grid_fp, W_k, left=W_k[0], right=W_k[-1])\n",
        "\n",
        "    # Define the inner integrand over the shock distribution z.\n",
        "    def inner_integrand(z: float, capital_after_flow: float) -> float:\n",
        "        # Apply the shock to get the capital level at the jump time.\n",
        "        capital_after_shock = capital_after_flow * z\n",
        "\n",
        "        # Determine the cost based on whether the threshold is breached.\n",
        "        if capital_after_shock < y:\n",
        "            # If breached, cost is the injection plus the value at y.\n",
        "            cost = (y - capital_after_shock) + W_at_y\n",
        "        else:\n",
        "            # If not breached, cost is the continuation value W(X_τ₁).\n",
        "            # We must interpolate the value of W at the new capital level.\n",
        "            cost = np.interp(capital_after_shock, x_grid_fp, W_k, left=W_k[0], right=W_k[-1])\n",
        "\n",
        "        # Return the cost weighted by its probability density.\n",
        "        return cost * shock_pdf(z)\n",
        "\n",
        "    # Define the outer integrand over the time to the next shock, t = τ₁.\n",
        "    def outer_integrand(t: float) -> float:\n",
        "        # First, apply the deterministic flow for duration t.\n",
        "        capital_after_flow = _apply_deterministic_flow(\n",
        "            capital=x,\n",
        "            time_delta=t,\n",
        "            active_poverty_line=active_params.active_poverty_line,\n",
        "            active_growth_rate=active_params.active_growth_rate\n",
        "        )\n",
        "\n",
        "        # Compute the inner integral (expectation over z) for this t.\n",
        "        inner_integral, _ = scipy.integrate.quad(\n",
        "            lambda z: inner_integrand(z, capital_after_flow), 0, 1\n",
        "        )\n",
        "\n",
        "        # The outer integrand is the discounted inner integral, weighted by the\n",
        "        # density of the first jump time (which is λ*exp(-λt)).\n",
        "        # The full term is λ * exp(-(λ+δ)t) * E[Cost | t].\n",
        "        return lambda_ * np.exp(-(lambda_ + delta) * t) * inner_integral\n",
        "\n",
        "    # Compute the final value by integrating the outer integrand over all time.\n",
        "    # The integral is from 0 to infinity.\n",
        "    result, _ = scipy.integrate.quad(outer_integrand, 0, np.inf)\n",
        "\n",
        "    return result\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 19, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def solve_vy_fixed_point(\n",
        "    y: float,\n",
        "    active_params: 'ActiveParameters',\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    shock_pdf: Callable[[float], float],\n",
        "    max_iter: int = 1000,\n",
        "    tolerance: float = 1e-7,\n",
        "    grid_size: int = 200\n",
        ") -> FixedPointResult:\n",
        "    \"\"\"\n",
        "    Solves for the value function V_y(x) using fixed-point iteration.\n",
        "\n",
        "    This function implements the iterative numerical scheme from Proposition 4.3,\n",
        "    repeatedly applying the operator T(W) until the value function converges.\n",
        "    This method is an alternative to Monte Carlo simulation.\n",
        "\n",
        "    Args:\n",
        "        y: The transfer threshold.\n",
        "        active_params: The active model parameters.\n",
        "        parsed_config: The fully parsed configuration object.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        shock_pdf: The PDF of the active shock distribution.\n",
        "        max_iter: The maximum number of iterations to perform.\n",
        "        tolerance: The convergence tolerance for the supremum norm.\n",
        "        grid_size: The number of points to use for the discrete value function grid.\n",
        "\n",
        "    Returns:\n",
        "        A `FixedPointResult` object containing the converged value function,\n",
        "        and diagnostics about the iteration process.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Discretize the domain and initialize ---\n",
        "    # The grid for the fixed-point iteration starts at y and extends outwards.\n",
        "    x_max = y * 10 # A heuristic for a sufficiently large upper bound.\n",
        "    x_grid_fp = np.linspace(y, x_max, grid_size)\n",
        "\n",
        "    # Initialize the value function estimate W_k to zero.\n",
        "    W_k = np.zeros(grid_size)\n",
        "\n",
        "    # Extract parameters needed for the iteration.\n",
        "    lambda_ = parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda\n",
        "    delta = parsed_config.model_parameters.social_planner_parameters.discount_rate_delta\n",
        "\n",
        "    # --- Step 2: Iterate the operator T ---\n",
        "    converged = False\n",
        "    final_error = np.inf\n",
        "    for k in range(max_iter):\n",
        "        # Create the next iterate W_{k+1}\n",
        "        W_k_plus_1 = np.zeros(grid_size)\n",
        "\n",
        "        # Apply the operator T at each point on the grid.\n",
        "        for i, x_val in enumerate(x_grid_fp):\n",
        "            W_k_plus_1[i] = _apply_operator_T(\n",
        "                x=x_val, y=y, W_k=W_k, x_grid_fp=x_grid_fp,\n",
        "                active_params=active_params, shock_pdf=shock_pdf,\n",
        "                lambda_=lambda_, delta=delta\n",
        "            )\n",
        "\n",
        "        # --- Step 3: Check for convergence ---\n",
        "        # Calculate the supremum norm of the difference between iterates.\n",
        "        final_error = np.max(np.abs(W_k_plus_1 - W_k))\n",
        "\n",
        "        # Update the current iterate.\n",
        "        W_k = W_k_plus_1\n",
        "\n",
        "        # If the error is below the tolerance, convergence is achieved.\n",
        "        if final_error < tolerance:\n",
        "            converged = True\n",
        "            break\n",
        "\n",
        "    if not converged:\n",
        "        warnings.warn(\n",
        "            f\"Fixed-point iteration did not converge within {max_iter} iterations. \"\n",
        "            f\"Final error: {final_error:.2e}\",\n",
        "            RuntimeWarning\n",
        "        )\n",
        "\n",
        "    # --- Final Output Assembly ---\n",
        "    # Create the final output grid from the configuration.\n",
        "    output_grid_cfg = parsed_config.output_parameters.capital_grid_for_plots\n",
        "    output_grid = np.linspace(output_grid_cfg.start_value, output_grid_cfg.stop_value, output_grid_cfg.num_points)\n",
        "\n",
        "    # Interpolate the converged solution W_k onto the final output grid.\n",
        "    # For x > y, the value is W(x).\n",
        "    vy_above_y = np.interp(output_grid, x_grid_fp, W_k, left=W_k[0], right=W_k[-1])\n",
        "\n",
        "    # The value at the threshold, V_y(y), is the first point of the converged solution.\n",
        "    vy_at_y = W_k[0]\n",
        "\n",
        "    # For x <= y, the value is (y - x) + V_y(y).\n",
        "    vy_below_y = (y - output_grid) + vy_at_y\n",
        "\n",
        "    # Combine the two pieces.\n",
        "    final_vy_values = np.where(output_grid <= y, vy_below_y, vy_above_y)\n",
        "\n",
        "    # Package into a ValueFunctionResult.\n",
        "    value_function_result = ValueFunctionResult(\n",
        "        capital_grid=output_grid,\n",
        "        point_estimates=final_vy_values,\n",
        "        ci_lower=None, ci_upper=None, confidence_level=None\n",
        "    )\n",
        "\n",
        "    # Return the final, comprehensive result object.\n",
        "    return FixedPointResult(\n",
        "        value_function_result=value_function_result,\n",
        "        converged=converged,\n",
        "        iterations=k + 1,\n",
        "        final_error=final_error\n",
        "    )\n"
      ],
      "metadata": {
        "id": "4hHnsLG6NIcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 20 — Assemble orchestrator function (end-to-end pipeline)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 20: Implementation of the End-to-End Pipeline\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Provenance:\n",
        "    \"\"\"\n",
        "    Immutable container for all intermediate results and diagnostics.\n",
        "\n",
        "    This object serves as a complete and transparent audit trail for a single\n",
        "    execution of the end-to-end pipeline. It captures all key computed\n",
        "    parameters, strategic decisions, and verification outcomes, making any\n",
        "    given result fully reproducible and debuggable. It is a critical component\n",
        "    for maintaining high standards of scientific rigor.\n",
        "\n",
        "    Attributes:\n",
        "        base_quantities: The computed parameters for the baseline (uninsured)\n",
        "                         model, including `r` and `μ`.\n",
        "        insurance_transforms: The computed insurance-induced transformations\n",
        "                              (`p_R`, `x*^R`, `r^R`, etc.), or `None` if\n",
        "                              insurance was inactive.\n",
        "        active_params: The final, consolidated parameters (`active_poverty_line`,\n",
        "                       `final_algorithm_choice`, etc.) that were actually used\n",
        "                       by the core computational engine.\n",
        "        optimization_result: The detailed results from the one-dimensional\n",
        "                             search for the optimal threshold `y*`.\n",
        "        cross_validation_result: The results of the Monte Carlo vs. closed-form\n",
        "                                 validation check, or `None` if the check was\n",
        "                                 not applicable for the scenario.\n",
        "        warnings: A list of all warning messages generated during the pipeline\n",
        "                  run, captured to flag potential issues with numerical\n",
        "                  stability, convergence, or configuration choices.\n",
        "    \"\"\"\n",
        "    # The computed parameters for the baseline (uninsured) model.\n",
        "    base_quantities: 'BaseDerivedQuantities'\n",
        "\n",
        "    # The computed insurance-induced transformations, if applicable.\n",
        "    insurance_transforms: Optional['InsuranceTransforms']\n",
        "\n",
        "    # The final, consolidated parameters used by the core computational engine.\n",
        "    active_params: 'ActiveParameters'\n",
        "\n",
        "    # The detailed results from the optimization of the threshold y*.\n",
        "    optimization_result: 'OptimizationResult'\n",
        "\n",
        "    # The results of the MC vs. closed-form validation, if applicable.\n",
        "    cross_validation_result: Optional['CrossValidationResult']\n",
        "\n",
        "    # A list of any warnings generated during the run for diagnostic purposes.\n",
        "    warnings: List[str] = field(default_factory=list)\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class EndToEndResult:\n",
        "    \"\"\"\n",
        "    The final, comprehensive output of the entire research pipeline.\n",
        "\n",
        "    This top-level object encapsulates all artifacts produced by a successful\n",
        "    run of the `run_end_to_end_analysis` function. It is designed to be a\n",
        "    self-contained, complete record of the analysis, providing all necessary\n",
        "    data for plotting, reporting, and further robustness checks.\n",
        "\n",
        "    Attributes:\n",
        "        parsed_config: The fully parsed, validated, and cleansed configuration\n",
        "                       object that governed the entire run.\n",
        "        provenance: A nested `Provenance` object containing the complete audit\n",
        "                    trail of the run, including all key intermediate results\n",
        "                    and diagnostic checks.\n",
        "        all_value_functions: A dictionary mapping the canonical names of the\n",
        "                             value function series requested in the configuration\n",
        "                             (e.g., \"V_y_star_x_optimal_threshold\", \"C_x_..._line\")\n",
        "                             to their fully computed `ValueFunctionResult`\n",
        "                             objects. This is the primary data output of the\n",
        "                             analysis.\n",
        "    \"\"\"\n",
        "    # The fully parsed, validated, and cleansed configuration object.\n",
        "    parsed_config: 'ParsedStudyConfig'\n",
        "\n",
        "    # An object containing the complete audit trail of the run.\n",
        "    provenance: Provenance\n",
        "\n",
        "    # A dictionary mapping series names to their computed `ValueFunctionResult` objects.\n",
        "    all_value_functions: Dict[str, 'ValueFunctionResult']\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Orchestrator's Helper Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _is_cross_validation_eligible(\n",
        "    parsed_config: 'ParsedStudyConfig'\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Checks if the configuration is eligible for MC vs. Closed-Form cross-validation.\n",
        "\n",
        "    This is a critical gatekeeping function that determines if a meaningful\n",
        "    comparison between the Monte Carlo engine and the analytical solution is\n",
        "    possible. Eligibility is strictly defined by the conditions under which the\n",
        "    closed-form solution for C(x) was derived in the source paper (Section 5).\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The fully parsed, validated, and cleansed configuration object.\n",
        "\n",
        "    Returns:\n",
        "        True if the configuration is eligible for cross-validation (i.e., it\n",
        "        represents the specific baseline case with an analytical solution),\n",
        "        and False otherwise.\n",
        "    \"\"\"\n",
        "    # Extract the relevant parameter blocks from the configuration for the check.\n",
        "    insurance_params = parsed_config.model_parameters.microinsurance_parameters\n",
        "    loss_dist = parsed_config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z\n",
        "\n",
        "    # The conditions for eligibility are threefold, matching the paper's specific case:\n",
        "    # 1. Microinsurance must be inactive.\n",
        "    # 2. The loss distribution must be 'Beta'.\n",
        "    # 3. The 'beta' parameter of the Beta distribution must be exactly 1.0.\n",
        "    return (\n",
        "        not insurance_params.is_active and\n",
        "        loss_dist.name == \"Beta\" and\n",
        "        math.isclose(loss_dist.parameters.get(\"beta\", 0.0), 1.0)\n",
        "    )\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 20, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_end_to_end_analysis(\n",
        "    full_study_configuration: Dict[str, Any]\n",
        ") -> 'EndToEndResult':\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for a given configuration.\n",
        "\n",
        "    This top-level orchestrator is the single entry point for a full analysis.\n",
        "    This version represents the pinnacle of the design, featuring a\n",
        "    purely compositional structure that delegates all major computational stages\n",
        "    to dedicated, robust sub-orchestrators. It is methodologically pure,\n",
        "    efficient, and highly maintainable.\n",
        "\n",
        "    Pipeline Stages:\n",
        "    1.  **Setup & Validation:** Ingests, validates, and cleanses the raw config.\n",
        "    2.  **Parameter Computation:** Computes all derived parameters, selects active\n",
        "        parameters, and constructs the necessary random number samplers.\n",
        "    3.  **Core Computation:** Finds the optimal transfer threshold `y*`.\n",
        "    4.  **Artifact Generation:** Delegates the computation of ALL requested value\n",
        "        function series (V_{y*}, C(x), D(x), etc.) to a specialized engine.\n",
        "    5.  **Verification:** Performs critical sanity checks on the generated artifacts.\n",
        "    6.  **Result Aggregation:** Packages all results into a comprehensive object.\n",
        "\n",
        "    Args:\n",
        "        full_study_configuration: The raw dictionary with the study configuration.\n",
        "\n",
        "    Returns:\n",
        "        An `EndToEndResult` object containing all outputs and provenance of the analysis.\n",
        "    \"\"\"\n",
        "    # Use a context manager to capture any warnings for the final manifest.\n",
        "    with warnings.catch_warnings(record=True) as caught_warnings:\n",
        "        # --- Stage 1: Setup & Validation ---\n",
        "        # This stage transforms the raw input dict into a safe, validated, and\n",
        "        # canonical configuration object.\n",
        "        parsed_config = ingest_and_parse_config(full_study_configuration)\n",
        "        validate_parameters(parsed_config, full_study_configuration)\n",
        "        parsed_config = cleanse_and_normalize_config(parsed_config)\n",
        "\n",
        "        # --- Stage 2: Parameter Computation ---\n",
        "        # This stage computes all necessary derived parameters and objects that\n",
        "        # define the specific model instance to be analyzed.\n",
        "        base_quantities = compute_base_derived_quantities(parsed_config, full_study_configuration)\n",
        "\n",
        "        temp_rng = np.random.default_rng(parsed_config.computation_parameters.monte_carlo_settings.random_seed)\n",
        "        z_sampler = _create_z_sampler(\n",
        "            loss_dist=parsed_config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z,\n",
        "            rng=temp_rng\n",
        "        )\n",
        "\n",
        "        insurance_transforms = compute_insurance_transforms(parsed_config, base_quantities, z_sampler)\n",
        "        active_params = select_method_and_define_active_parameters(\n",
        "            parsed_config, base_quantities, insurance_transforms\n",
        "        )\n",
        "        active_sampler = construct_active_sampler(parsed_config, base_quantities, insurance_transforms)\n",
        "        main_rng = np.random.default_rng(parsed_config.computation_parameters.monte_carlo_settings.random_seed)\n",
        "\n",
        "        # --- Stage 3: Core Computation ---\n",
        "        print(\"--- Finding optimal threshold y* ---\")\n",
        "        # This is the central optimization step of the analysis.\n",
        "        optimization_result = find_optimal_threshold(\n",
        "            active_params=active_params, parsed_config=parsed_config,\n",
        "            base_quantities=base_quantities, shock_sampler=active_sampler, rng=main_rng\n",
        "        )\n",
        "\n",
        "        # --- Stage 4: Artifact Generation ---\n",
        "        print(\"--- Generating all requested output series ---\")\n",
        "        # All value function computations are now delegated to the robust Task 24\n",
        "        # orchestrator, eliminating redundancy and ensuring methodological consistency.\n",
        "        all_value_functions = generate_output_series(\n",
        "            parsed_config=parsed_config, active_params=active_params,\n",
        "            base_quantities=base_quantities, optimization_result=optimization_result,\n",
        "            shock_sampler=active_sampler, rng=main_rng\n",
        "        )\n",
        "\n",
        "        # --- Stage 5: Verification ---\n",
        "        print(\"--- Running final verification checks ---\")\n",
        "        # Perform tail limit checks on the key computed value functions.\n",
        "        verify_tail_limits([\n",
        "            (all_value_functions.get(\"V_y_star_x_optimal_threshold\"), \"V_y_star(x)\"),\n",
        "            (all_value_functions.get(\"C_x_inject_to_poverty_line\"), \"C(x)\")\n",
        "        ])\n",
        "\n",
        "        # Initialize cross-validation result.\n",
        "        cross_validation_res = None\n",
        "        # Check for eligibility and run the cross-validation if appropriate.\n",
        "        if _is_cross_validation_eligible(parsed_config) and active_params.final_algorithm_choice == \"MonteCarlo\":\n",
        "            print(\"--- Running MC vs. Closed-Form Cross-Validation ---\")\n",
        "            # Extract the MC result for C(x) from the generated artifacts.\n",
        "            mc_c_function = all_value_functions.get(\"C_x_inject_to_poverty_line\")\n",
        "\n",
        "            if mc_c_function:\n",
        "                # Compute the corresponding closed-form result for C(x).\n",
        "                cf_c_values = evaluate_c_closed_form(\n",
        "                    x=mc_c_function.capital_grid,\n",
        "                    x_star=active_params.active_poverty_line,\n",
        "                    delta=parsed_config.model_parameters.social_planner_parameters.discount_rate_delta,\n",
        "                    lambda_=parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "                    alpha=parsed_config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z.parameters['alpha'],\n",
        "                    r=active_params.active_growth_rate\n",
        "                )\n",
        "                cf_result = ValueFunctionResult(capital_grid=mc_c_function.capital_grid, point_estimates=cf_c_values, ci_lower=None,\n",
        "                                                ci_upper=None, confidence_level=None, truncation_fraction=None)\n",
        "\n",
        "                # Perform the comparison.\n",
        "                cross_validation_res = cross_validate_mc_vs_closed_form(mc_c_function, cf_result)\n",
        "                print(f\"--- Cross-Validation {'PASSED' if cross_validation_res.passed else 'FAILED'} ---\")\n",
        "\n",
        "        # --- Stage 6: Result Aggregation ---\n",
        "        # Assemble the complete provenance record for the run.\n",
        "        provenance = Provenance(\n",
        "            base_quantities=base_quantities,\n",
        "            insurance_transforms=insurance_transforms,\n",
        "            active_params=active_params,\n",
        "            optimization_result=optimization_result,\n",
        "            cross_validation_result=cross_validation_res,\n",
        "            warnings=[str(w.message) for w in caught_warnings]\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- End-to-End Analysis Complete ---\")\n",
        "        # Assemble the final, comprehensive result object.\n",
        "        return EndToEndResult(\n",
        "            parsed_config=parsed_config,\n",
        "            provenance=provenance,\n",
        "            all_value_functions=all_value_functions\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "id": "rjT1RGZHQZdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 21 — Robustness analysis: parameter sweeps\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 21: Robustness analysis: parameter sweeps\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Helper function to update nested dictionaries\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _update_nested_dict(d: Dict[str, Any], path: str, value: Any) -> None:\n",
        "    \"\"\"\n",
        "    Updates a value in a nested dictionary using a dot-separated path.\n",
        "\n",
        "    This utility function safely navigates and modifies a nested dictionary,\n",
        "    which is essential for creating the different configurations for each run\n",
        "    in a parameter sweep.\n",
        "\n",
        "    Args:\n",
        "        d: The dictionary to modify.\n",
        "        path: A dot-separated string (e.g., \"a.b.c\").\n",
        "        value: The new value to set at the specified path.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If an intermediate key in the path does not exist.\n",
        "    \"\"\"\n",
        "    # Split the path into keys.\n",
        "    keys = path.split('.')\n",
        "    # Navigate to the second-to-last dictionary.\n",
        "    for key in keys[:-1]:\n",
        "        d = d.setdefault(key, {})\n",
        "    # Set the value at the final key.\n",
        "    d[keys[-1]] = value\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Step 1: Helper function to generate parameter combinations\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _generate_parameter_combinations(\n",
        "    sweep_params: Dict[str, List[Any]]\n",
        ") -> Iterator[Tuple[Tuple[Any, ...], Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Generates all combinations of parameters for a sweep.\n",
        "\n",
        "    Args:\n",
        "        sweep_params: A dictionary where keys are dot-separated paths and\n",
        "                      values are lists of values to sweep over.\n",
        "\n",
        "    Yields:\n",
        "        A tuple containing:\n",
        "        - A tuple of the parameter values for the current combination.\n",
        "        - A dictionary mapping the parameter paths to their values.\n",
        "    \"\"\"\n",
        "    # Get the names (paths) of the parameters being swept.\n",
        "    param_names = list(sweep_params.keys())\n",
        "    # Get the lists of values for each parameter.\n",
        "    param_values = list(sweep_params.values())\n",
        "\n",
        "    # Use itertools.product to create the Cartesian product of all value lists.\n",
        "    for combination in itertools.product(*param_values):\n",
        "        # Yield the tuple of values (for use as a dictionary key) and a\n",
        "        # dictionary mapping the parameter names to these values.\n",
        "        yield combination, dict(zip(param_names, combination))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 21, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_parameter_sweep(\n",
        "    base_config: Dict[str, Any],\n",
        "    sweep_params: Dict[str, List[Any]]\n",
        ") -> Dict[Tuple[Any, ...], 'EndToEndResult']:\n",
        "    \"\"\"\n",
        "    Executes the end-to-end analysis over a grid of parameter values.\n",
        "\n",
        "    This function automates robustness analysis by systematically varying one or\n",
        "    more model parameters, running the full pipeline for each combination, and\n",
        "    collecting the results. It is designed to be robust, continuing even if\n",
        "    some parameter combinations are invalid or fail during execution.\n",
        "\n",
        "    Process:\n",
        "    1.  **Generate Combinations:** Creates the Cartesian product of all parameter\n",
        "        values specified in `sweep_params`.\n",
        "    2.  **Iterate and Execute:** For each parameter combination:\n",
        "        a.  Creates a deep copy of the `base_config`.\n",
        "        b.  Updates the copied configuration with the specific parameter values\n",
        "            for the current run.\n",
        "        c.  Calls the main `run_end_to_end_analysis` orchestrator.\n",
        "        d.  Catches any exceptions during the run, prints a warning, and\n",
        "            continues to the next combination.\n",
        "    3.  **Collect Results:** Stores the `EndToEndResult` object for each\n",
        "        successful run in a dictionary, keyed by the tuple of parameter values.\n",
        "\n",
        "    Args:\n",
        "        base_config: The base `full_study_configuration` dictionary.\n",
        "        sweep_params: A dictionary where keys are dot-separated paths to the\n",
        "                      parameters to be swept, and values are lists of the\n",
        "                      values to use.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are tuples of the swept parameter values and\n",
        "        values are the corresponding `EndToEndResult` objects for each\n",
        "        successful run.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Generate all parameter combinations for the sweep ---\n",
        "    param_combinations = list(_generate_parameter_combinations(sweep_params))\n",
        "\n",
        "    # Initialize a dictionary to store the results of the sweep.\n",
        "    sweep_results: Dict[Tuple[Any, ...], 'EndToEndResult'] = {}\n",
        "\n",
        "    # --- Step 2: Iterate through combinations and execute the pipeline ---\n",
        "    print(f\"Starting parameter sweep with {len(param_combinations)} combinations...\")\n",
        "\n",
        "    # Loop through each generated parameter combination.\n",
        "    for i, (param_tuple, param_dict) in enumerate(param_combinations):\n",
        "        print(f\"\\n--- Running combination {i+1}/{len(param_combinations)}: {param_dict} ---\")\n",
        "\n",
        "        try:\n",
        "            # Create a deep copy of the base configuration to ensure that each\n",
        "            # run is independent and does not modify the base state.\n",
        "            run_config = copy.deepcopy(base_config)\n",
        "\n",
        "            # Update the configuration for the current run of the sweep.\n",
        "            for path, value in param_dict.items():\n",
        "                _update_nested_dict(run_config, path, value)\n",
        "\n",
        "            # Execute the full end-to-end analysis pipeline.\n",
        "            result = run_end_to_end_analysis(run_config)\n",
        "\n",
        "            # --- Step 3: Store the result ---\n",
        "            # Use the tuple of parameter values as the key for easy lookup.\n",
        "            sweep_results[param_tuple] = result\n",
        "\n",
        "            print(f\"--- Combination {i+1} completed successfully. ---\")\n",
        "\n",
        "        except (ValueError, KeyError, NotImplementedError, RuntimeError) as e:\n",
        "            # If any run fails (e.g., due to an invalid parameter combination),\n",
        "            # catch the exception, print a warning, and continue with the sweep.\n",
        "            # This makes the overall sweep process robust to individual failures.\n",
        "            print(f\"--- WARNING: Combination {i+1} failed and was skipped. ---\")\n",
        "            print(f\"    Parameters: {param_dict}\")\n",
        "            print(f\"    Error: {e}\")\n",
        "\n",
        "    print(f\"\\nParameter sweep completed. {len(sweep_results)}/{len(param_combinations)} runs were successful.\")\n",
        "\n",
        "    # Return the dictionary containing all successful results.\n",
        "    return sweep_results\n"
      ],
      "metadata": {
        "id": "DJiPAXnV19bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 22: Robustness analysis: boundary mapping (Proposition 2.2)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 22: Robustness analysis: boundary mapping (Proposition 2.2)\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Step 2: Helper to compute the boundary diagnostic on a grid\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _compute_boundary_grid(\n",
        "    lambda_grid: np.ndarray,\n",
        "    mu_grid: np.ndarray,\n",
        "    b: float,\n",
        "    delta: float\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the policy preference boundary diagnostic over a 2D parameter grid.\n",
        "\n",
        "    Args:\n",
        "        lambda_grid: A 2D NumPy array of λ (shock frequency) values.\n",
        "        mu_grid: A 2D NumPy array of μ (expected remaining capital) values.\n",
        "        b: The scalar income generation rate.\n",
        "        delta: The scalar discount rate.\n",
        "\n",
        "    Returns:\n",
        "        A 2D NumPy array containing the value of b - (δ + λ(1 - μ)) at each\n",
        "        point on the grid.\n",
        "    \"\"\"\n",
        "    # Equation from Proposition 2.2: Boundary = b - (δ + λ(1 - μ))\n",
        "    # This calculation is fully vectorized for maximum efficiency.\n",
        "    boundary_values = b - (delta + lambda_grid * (1 - mu_grid))\n",
        "\n",
        "    return boundary_values\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 22, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_and_plot_policy_boundary(\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    lambda_range: Tuple[float, float] = (0.1, 3.0),\n",
        "    mu_range: Tuple[float, float] = (0.1, 0.99),\n",
        "    grid_resolution: int = 100\n",
        ") -> Figure:\n",
        "    \"\"\"\n",
        "    Analyzes and visualizes the policy preference boundary from Proposition 2.2.\n",
        "\n",
        "    This function creates a 2D plot in the (λ, μ) parameter space, showing the\n",
        "    regions where lump-sum transfers are theoretically more cost-effective than\n",
        "    perpetual regular transfers, and vice-versa. It provides a powerful visual\n",
        "    diagnostic for understanding the model's sensitivity to shock frequency (λ)\n",
        "    and severity (via μ).\n",
        "\n",
        "    Process:\n",
        "    1.  **Define Grid:** Creates a 2D grid of `λ` and `μ` values.\n",
        "    2.  **Compute Boundary:** Evaluates the diagnostic `b - (δ + λ(1 - μ))`\n",
        "        at every point on the grid.\n",
        "    3.  **Visualize:**\n",
        "        a.  Draws the boundary curve where the diagnostic is zero.\n",
        "        b.  Shades the two preference regions (lump-sum vs. perpetual).\n",
        "        c.  Plots a marker indicating the position of the current base\n",
        "            configuration on the map.\n",
        "\n",
        "    Args:\n",
        "        parsed_config: The fully parsed configuration object.\n",
        "        base_quantities: The computed derived quantities for the base model.\n",
        "        lambda_range: The (min, max) range for the λ axis.\n",
        "        mu_range: The (min, max) range for the μ axis.\n",
        "        grid_resolution: The number of points along each axis of the grid.\n",
        "\n",
        "    Returns:\n",
        "        A `matplotlib.figure.Figure` object containing the generated plot.\n",
        "    \"\"\"\n",
        "    # --- Extract necessary parameters from the configuration ---\n",
        "    b = parsed_config.model_parameters.household_economic_parameters.primitive_params_for_r.income_generation_rate_b\n",
        "    delta = parsed_config.model_parameters.social_planner_parameters.discount_rate_delta\n",
        "\n",
        "    # Get the specific (λ, μ) point for the current configuration.\n",
        "    current_lambda = parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda\n",
        "    current_mu = base_quantities.expected_remaining_capital_mu\n",
        "\n",
        "    # --- Step 1: Define the boundary scan grid ---\n",
        "    # Create the 1D axes for lambda and mu.\n",
        "    lambda_axis = np.linspace(lambda_range[0], lambda_range[1], grid_resolution)\n",
        "    mu_axis = np.linspace(mu_range[0], mu_range[1], grid_resolution)\n",
        "    # Create the 2D coordinate grids.\n",
        "    lambda_grid, mu_grid = np.meshgrid(lambda_axis, mu_axis)\n",
        "\n",
        "    # --- Step 2: Compute the boundary diagnostic for each grid point ---\n",
        "    # This is a single, efficient vectorized calculation.\n",
        "    boundary_grid = _compute_boundary_grid(lambda_grid, mu_grid, b, delta)\n",
        "\n",
        "    # --- Step 3: Visualize the boundary curve and preference regions ---\n",
        "    # Create a new figure and axes for the plot.\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Use contourf to shade the preference regions.\n",
        "    # Levels are set to create two regions: below 0 and above 0.\n",
        "    contour = ax.contourf(\n",
        "        lambda_grid, mu_grid, boundary_grid,\n",
        "        levels=[-np.inf, 0, np.inf],\n",
        "        colors=['#d1e5f0', '#fddbc7'], # Light blue for perpetual, light red for lump-sum\n",
        "        alpha=0.6\n",
        "    )\n",
        "\n",
        "    # Use contour to draw the exact boundary line (where the value is 0).\n",
        "    ax.contour(\n",
        "        lambda_grid, mu_grid, boundary_grid,\n",
        "        levels=[0],\n",
        "        colors='black',\n",
        "        linewidths=2\n",
        "    )\n",
        "\n",
        "    # Plot a distinct marker for the current configuration's parameters.\n",
        "    ax.plot(\n",
        "        current_lambda, current_mu,\n",
        "        marker='*',\n",
        "        markersize=15,\n",
        "        color='red',\n",
        "        markeredgecolor='black',\n",
        "        label=f'Current Config (λ={current_lambda}, μ={current_mu:.2f})'\n",
        "    )\n",
        "\n",
        "    # --- Add professional labels, title, and legend ---\n",
        "    ax.set_title(\n",
        "        'Policy Preference Boundary (Proposition 2.2)\\n'\n",
        "        f'Lump-Sum vs. Perpetual Transfers (for b={b}, δ={delta})',\n",
        "        fontsize=16\n",
        "    )\n",
        "    ax.set_xlabel('Shock Frequency (λ)', fontsize=12)\n",
        "    ax.set_ylabel('Expected Remaining Capital Fraction (μ)', fontsize=12)\n",
        "\n",
        "    # Create custom legend handles for the shaded regions.\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='#d1e5f0', alpha=0.6, label='Perpetual Transfers Preferred (D(x*) < C(x*))'),\n",
        "        Patch(facecolor='#fddbc7', alpha=0.6, label='Lump-Sum Transfers Preferred (C(x*) <= D(x*))'),\n",
        "        ax.get_lines()[0] # Get the handle for the current config marker\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='best')\n",
        "\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "CdKIsqAD4U9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 23 — Robustness analysis: convergence diagnostics (Monte Carlo)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 23: Robustness analysis: convergence diagnostics (Monte Carlo)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class ConvergenceDiagnostics:\n",
        "    \"\"\"\n",
        "    Immutable container for the results of Monte Carlo convergence diagnostic tests.\n",
        "\n",
        "    This object aggregates all artifacts produced by the `run_convergence_diagnostics`\n",
        "    function. It provides both the raw numerical data from the parameter sweeps\n",
        "    and the generated plots, offering a comprehensive overview of the Monte\n",
        "    Carlo engine's stability and convergence properties.\n",
        "\n",
        "    Attributes:\n",
        "        n_convergence_results: A dictionary mapping the number of simulation\n",
        "                               paths (N) to key outputs. Each entry contains\n",
        "                               the computed optimal threshold `y*`, the value\n",
        "                               at that threshold `V(y*)`, and the confidence\n",
        "                               interval width. This data is used to assess\n",
        "                               statistical convergence.\n",
        "        t_convergence_results: A dictionary mapping the simulation time\n",
        "                               horizon (T) to key outputs. Each entry contains\n",
        "                               the computed `y*`, `V(y*)`, and the observed\n",
        "                               truncation fraction. This data is used to assess\n",
        "                               truncation bias.\n",
        "        ci_scaling_slope: The estimated slope from a log-log regression of the\n",
        "                          confidence interval width against the sample size N.\n",
        "                          Theoretically, this value should be close to -0.5,\n",
        "                          and deviation from this indicates potential issues\n",
        "                          with the estimator's variance.\n",
        "        n_convergence_plot: A `matplotlib.figure.Figure` object containing the\n",
        "                            log-log plot of CI width versus N, providing a\n",
        "                            visual check of the estimator's convergence rate.\n",
        "        t_convergence_plot: A `matplotlib.figure.Figure` object containing the\n",
        "                            plot of `y*` and truncation fraction versus T,\n",
        "                            providing a visual check for truncation bias.\n",
        "    \"\"\"\n",
        "    # A dictionary mapping sample sizes (N) to key outputs (y*, V(y*), CI width).\n",
        "    n_convergence_results: Dict[int, Dict[str, float]]\n",
        "\n",
        "    # A dictionary mapping time horizons (T) to key outputs (y*, V(y*), truncation fraction).\n",
        "    t_convergence_results: Dict[float, Dict[str, float]]\n",
        "\n",
        "    # The estimated slope from a log-log regression of CI width against N.\n",
        "    ci_scaling_slope: float\n",
        "\n",
        "    # A matplotlib Figure visualizing CI width vs. N.\n",
        "    n_convergence_plot: Figure\n",
        "\n",
        "    # A matplotlib Figure visualizing the impact of the time horizon T.\n",
        "    t_convergence_plot: Figure\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 23, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_convergence_diagnostics(\n",
        "    base_config: Dict[str, Any],\n",
        "    n_values: List[int] = [10_000, 50_000, 100_000, 250_000],\n",
        "    t_values: List[float] = [50.0, 100.0, 200.0]\n",
        ") -> ConvergenceDiagnostics:\n",
        "    \"\"\"\n",
        "    Performs and visualizes convergence diagnostics for the Monte Carlo engine.\n",
        "\n",
        "    This function executes two critical robustness checks by\n",
        "    systematically varying the number of simulation paths (N) and the time\n",
        "    horizon (T). It now correctly extracts all necessary diagnostics, including\n",
        "    the truncation fraction, for a complete and accurate analysis.\n",
        "\n",
        "    Process:\n",
        "    1.  **N-Convergence Sweep:** Verifies that the confidence interval width\n",
        "        scales correctly (proportional to 1/sqrt(N)) by running the pipeline\n",
        "        for a range of `N` values.\n",
        "    2.  **T-Convergence Sweep:** Verifies that truncation bias is controlled by\n",
        "        running the pipeline for a range of `T` values and observing the\n",
        "        truncation fraction and the stability of the optimal threshold `y*`.\n",
        "    3.  **Visualization:** Generates publication-quality plots for both analyses.\n",
        "\n",
        "    Args:\n",
        "        base_config: The base `full_study_configuration` dictionary.\n",
        "        n_values: A list of sample sizes (N) to test.\n",
        "        t_values: A list of time horizons (T) to test.\n",
        "\n",
        "    Returns:\n",
        "        A `ConvergenceDiagnostics` object containing the raw data from the\n",
        "        sweeps and the generated diagnostic plots.\n",
        "    \"\"\"\n",
        "    # --- Step 1: N-Convergence Analysis ---\n",
        "    print(\"--- Starting N-Convergence Diagnostic ---\")\n",
        "    # Define the sweep over the number of simulation paths.\n",
        "    n_sweep_params = {'computation_parameters.monte_carlo_settings.num_simulation_paths_N': n_values}\n",
        "    # Execute the sweep using the Task 21 orchestrator.\n",
        "    n_sweep_results = run_parameter_sweep(base_config, n_sweep_params)\n",
        "\n",
        "    # Process the results of the N-sweep.\n",
        "    n_convergence_data = {}\n",
        "    for n_val_tuple, result in n_sweep_results.items():\n",
        "        n_val = n_val_tuple[0]\n",
        "        opt_res = result.provenance.optimization_result\n",
        "        v_star_res = result.all_value_functions['V_y_star_x_optimal_threshold']\n",
        "        # CI width is calculated at a fixed point (e.g., x=0) for consistency.\n",
        "        width = v_star_res.ci_upper[0] - v_star_res.ci_lower[0]\n",
        "        n_convergence_data[n_val] = {\n",
        "            'y_star': opt_res.optimal_threshold_y_star,\n",
        "            'V_at_y_star': opt_res.min_value_vy_at_y_star,\n",
        "            'ci_width': width\n",
        "        }\n",
        "\n",
        "    # --- Step 2: Check CI width scaling and visualize ---\n",
        "    # Filter out any runs that might have failed or had zero width.\n",
        "    valid_n_data = {n: res for n, res in n_convergence_data.items() if res['ci_width'] > 1e-12}\n",
        "    log_n = np.log(list(valid_n_data.keys()))\n",
        "    log_width = np.log([res['ci_width'] for res in valid_n_data.values()])\n",
        "\n",
        "    slope = -0.5 # Default to the theoretical slope.\n",
        "    if len(log_n) > 1:\n",
        "        # Perform a linear regression on the log-log data to estimate the scaling exponent.\n",
        "        lin_reg_result = linregress(log_n, log_width)\n",
        "        slope = lin_reg_result.slope\n",
        "\n",
        "    # Create the N-convergence plot.\n",
        "    fig_n, ax_n = plt.subplots(figsize=(10, 6))\n",
        "    ax_n.plot(list(valid_n_data.keys()), [res['ci_width'] for res in valid_n_data.values()], 'o-', label='Empirical CI Width')\n",
        "    ax_n.set_xscale('log'); ax_n.set_yscale('log')\n",
        "    ax_n.set_title('MC Convergence: CI Width vs. Sample Size (N)', fontsize=16)\n",
        "    ax_n.set_xlabel('Number of Simulation Paths (N)', fontsize=12)\n",
        "    ax_n.set_ylabel('Confidence Interval Width (log scale)', fontsize=12)\n",
        "    ax_n.grid(True, which='both', linestyle='--', alpha=0.6)\n",
        "    ax_n.text(0.05, 0.1, f'Log-log regression slope: {slope:.3f}\\n(Theoretical: -0.5)',\n",
        "              transform=ax_n.transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
        "    fig_n.tight_layout()\n",
        "\n",
        "    # --- Step 3: T-Convergence Analysis ---\n",
        "    print(\"\\n--- Starting T-Convergence Diagnostic ---\")\n",
        "    # Define the sweep over the simulation time horizon.\n",
        "    t_sweep_params = {'computation_parameters.monte_carlo_settings.simulation_time_horizon_T': t_values}\n",
        "    # Execute the sweep.\n",
        "    t_sweep_results = run_parameter_sweep(base_config, t_sweep_params)\n",
        "\n",
        "    t_convergence_data = {}\n",
        "    for t_val_tuple, result in t_sweep_results.items():\n",
        "        t_val = t_val_tuple[0]\n",
        "        opt_res = result.provenance.optimization_result\n",
        "\n",
        "        # Correctly extract the truncation fraction from the result object.\n",
        "        v_star_res = result.all_value_functions.get('V_y_star_x_optimal_threshold')\n",
        "\n",
        "        max_trunc_frac = np.nan\n",
        "        if v_star_res and v_star_res.truncation_fraction is not None:\n",
        "            # For a summary metric, we take the maximum truncation fraction\n",
        "            # observed across the grid, as this represents the worst-case bias.\n",
        "            max_trunc_frac = np.nanmax(v_star_res.truncation_fraction)\n",
        "\n",
        "        t_convergence_data[t_val] = {\n",
        "            'y_star': opt_res.optimal_threshold_y_star,\n",
        "            'V_at_y_star': opt_res.min_value_vy_at_y_star,\n",
        "            'max_truncation_fraction': max_trunc_frac\n",
        "        }\n",
        "\n",
        "    # --- Step 4: Visualize T-Convergence ---\n",
        "    fig_t, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Plot Optimal Threshold (y*) vs. T on the primary y-axis.\n",
        "    color1 = 'tab:blue'\n",
        "    ax1.set_xlabel('Simulation Time Horizon (T)', fontsize=12)\n",
        "    ax1.set_ylabel('Optimal Threshold (y*)', color=color1, fontsize=12)\n",
        "    ax1.plot(list(t_convergence_data.keys()), [res['y_star'] for res in t_convergence_data.values()], 'o-', color=color1)\n",
        "    ax1.tick_params(axis='y', labelcolor=color1)\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Plot Max Truncation Fraction vs. T on the secondary y-axis.\n",
        "    ax2 = ax1.twinx()\n",
        "    color2 = 'tab:red'\n",
        "    ax2.set_ylabel('Max Truncation Fraction', color=color2, fontsize=12)\n",
        "    ax2.plot(list(t_convergence_data.keys()), [res['max_truncation_fraction'] for res in t_convergence_data.values()], 's--', color=color2)\n",
        "    ax2.tick_params(axis='y', labelcolor=color2)\n",
        "    ax2.set_ylim(bottom=0)\n",
        "\n",
        "    fig_t.suptitle('MC Convergence: Impact of Time Horizon (T)', fontsize=16)\n",
        "    fig_t.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "\n",
        "    # --- Final Assembly ---\n",
        "    return ConvergenceDiagnostics(\n",
        "        n_convergence_results=n_convergence_data,\n",
        "        t_convergence_results=t_convergence_data,\n",
        "        ci_scaling_slope=slope,\n",
        "        n_convergence_plot=fig_n,\n",
        "        t_convergence_plot=fig_t\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "lYcyHVo78GXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 24 — Output generation: compute and store all series\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 24: Output Series Generation\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Step 1, 2, 3: universal helper to evaluate any threshold strategy\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def evaluate_threshold_strategy(\n",
        "    y: float,\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    active_params: 'ActiveParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator\n",
        ") -> 'ValueFunctionResult':\n",
        "    \"\"\"\n",
        "    Evaluates a specific threshold strategy V_y(x) for a given fixed threshold y.\n",
        "\n",
        "    This is a powerful, universal engine for analyzing any threshold policy. It\n",
        "    determines the value at the boundary, V_y(y), and then evaluates the function\n",
        "    across the entire capital grid, reusing the robust grid evaluation logic.\n",
        "\n",
        "    Args:\n",
        "        y: The fixed transfer threshold to evaluate.\n",
        "        (All other args are the standard pipeline objects).\n",
        "\n",
        "    Returns:\n",
        "        A complete `ValueFunctionResult` object for the specified strategy.\n",
        "    \"\"\"\n",
        "    # First, determine the value at the boundary, V_y(y).\n",
        "    vy_at_y: float\n",
        "    if active_params.final_algorithm_choice == \"ClosedForm\":\n",
        "        # Use the closed-form solution if applicable.\n",
        "        vy_at_y = evaluate_vy_closed_form(\n",
        "            x=y, y=y,\n",
        "            x_star=active_params.active_poverty_line,\n",
        "            delta=parsed_config.model_parameters.social_planner_parameters.discount_rate_delta,\n",
        "            lambda_=parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "            alpha=parsed_config.model_parameters.stochastic_shock_parameters.loss_distribution_G_Z.parameters['alpha'],\n",
        "            r=active_params.active_growth_rate\n",
        "        )\n",
        "    else:\n",
        "        # Use the Monte Carlo estimator if not.\n",
        "        vy_at_y = estimate_vy_at_threshold(\n",
        "            threshold_y=y,\n",
        "            active_params=active_params, shock_sampler=shock_sampler, rng=rng,\n",
        "            shock_arrival_rate=parsed_config.model_parameters.stochastic_shock_parameters.shock_frequency_lambda,\n",
        "            discount_rate=parsed_config.model_parameters.social_planner_parameters.discount_rate_delta,\n",
        "            num_paths=parsed_config.computation_parameters.monte_carlo_settings.num_simulation_paths_N,\n",
        "            time_horizon=parsed_config.computation_parameters.monte_carlo_settings.simulation_time_horizon_T\n",
        "        )\n",
        "\n",
        "    # Create a temporary OptimizationResult object to hold this fixed-policy data.\n",
        "    fixed_y_opt_result = OptimizationResult(\n",
        "        optimal_threshold_y_star=y,\n",
        "        min_value_vy_at_y_star=vy_at_y,\n",
        "        optimizer_result=None  # No optimization was performed\n",
        "    )\n",
        "\n",
        "    # Reuse the Task 16 grid evaluator with the fixed result.\n",
        "    return evaluate_vy_star_on_grid(\n",
        "        opt_result=fixed_y_opt_result,\n",
        "        active_params=active_params, parsed_config=parsed_config,\n",
        "        base_quantities=base_quantities, shock_sampler=shock_sampler, rng=rng\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 24, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_output_series(\n",
        "    parsed_config: 'ParsedStudyConfig',\n",
        "    active_params: 'ActiveParameters',\n",
        "    base_quantities: 'BaseDerivedQuantities',\n",
        "    optimization_result: 'OptimizationResult',\n",
        "    shock_sampler: Callable[[int], np.ndarray],\n",
        "    rng: np.random.Generator\n",
        ") -> Dict[str, 'ValueFunctionResult']:\n",
        "    \"\"\"\n",
        "    Generates all requested value function series for output and visualization.\n",
        "\n",
        "    This orchestrator is a high-level dispatcher that uses a universal\n",
        "    evaluation engine (`evaluate_threshold_strategy`) to compute all requested\n",
        "    analytical artifacts. It is fully configuration-driven and methodologically\n",
        "    consistent across all computed series.\n",
        "\n",
        "    Args:\n",
        "        (All arguments are the standard pipeline objects).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping series names to their `ValueFunctionResult` objects.\n",
        "    \"\"\"\n",
        "    # Initialize the dictionary to hold all computed series.\n",
        "    all_value_functions: Dict[str, 'ValueFunctionResult'] = {}\n",
        "\n",
        "    # Get the list of series that the user has requested to compute.\n",
        "    series_to_compute = parsed_config.output_parameters.series_to_plot\n",
        "\n",
        "    # Create the capital grid once, to be used for all evaluations.\n",
        "    grid_cfg = parsed_config.output_parameters.capital_grid_for_plots\n",
        "    x_grid = np.linspace(grid_cfg.start_value, grid_cfg.stop_value, grid_cfg.num_points)\n",
        "\n",
        "    # --- Iterate through the requested series and dispatch to the correct evaluator ---\n",
        "    for series_name in series_to_compute:\n",
        "        print(f\"--- Generating series: {series_name} ---\")\n",
        "\n",
        "        result: Optional['ValueFunctionResult'] = None\n",
        "\n",
        "        if series_name == \"V_y_star_x_optimal_threshold\":\n",
        "            # This is the main result. Evaluate the strategy for the optimal y*.\n",
        "            result = evaluate_threshold_strategy(\n",
        "                y=optimization_result.optimal_threshold_y_star,\n",
        "                parsed_config=parsed_config, active_params=active_params,\n",
        "                base_quantities=base_quantities, shock_sampler=shock_sampler, rng=rng\n",
        "            )\n",
        "\n",
        "        elif series_name in [\"C_x_inject_to_poverty_line\", \"V_y_x_at_y_equals_x_star\"]:\n",
        "            # This is the \"inject-to-poverty-line\" strategy, C(x).\n",
        "            # It's a threshold strategy with y fixed at the active poverty line.\n",
        "            result = evaluate_threshold_strategy(\n",
        "                y=active_params.active_poverty_line,\n",
        "                parsed_config=parsed_config, active_params=active_params,\n",
        "                base_quantities=base_quantities, shock_sampler=shock_sampler, rng=rng\n",
        "            )\n",
        "\n",
        "        elif series_name == \"V_y_x_at_y_equals_40\":\n",
        "            # This is an example of another fixed-threshold strategy.\n",
        "            # Ensure the fixed y is not below the active poverty line.\n",
        "            fixed_y = 40.0\n",
        "            if fixed_y < active_params.active_poverty_line:\n",
        "                warnings.warn(\n",
        "                    f\"Requested fixed threshold y=40 is below the active poverty \"\n",
        "                    f\"line ({active_params.active_poverty_line:.2f}). Skipping series.\",\n",
        "                    UserWarning\n",
        "                )\n",
        "                continue\n",
        "            result = evaluate_threshold_strategy(\n",
        "                y=fixed_y,\n",
        "                parsed_config=parsed_config, active_params=active_params,\n",
        "                base_quantities=base_quantities, shock_sampler=shock_sampler, rng=rng\n",
        "            )\n",
        "\n",
        "        elif series_name == \"D_x_perpetual_transfers\":\n",
        "            # This is the perpetual transfers baseline, which has a different structure.\n",
        "            # Call its dedicated evaluator from Task 14.\n",
        "            d_values = evaluate_d_comparator(\n",
        "                x_grid=x_grid,\n",
        "                active_params=active_params, base_quantities=base_quantities,\n",
        "                parsed_config=parsed_config, shock_sampler=shock_sampler, rng=rng\n",
        "            )\n",
        "            result = ValueFunctionResult(\n",
        "                capital_grid=x_grid, point_estimates=d_values,\n",
        "                ci_lower=None, ci_upper=None, confidence_level=None\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Issue a warning for any unrecognized series names in the config.\n",
        "            warnings.warn(\n",
        "                f\"The series name '{series_name}' in 'series_to_plot' is not \"\n",
        "                \"recognized and will be skipped.\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "        # If a result was successfully computed, store it.\n",
        "        if result:\n",
        "            all_value_functions[series_name] = result\n",
        "\n",
        "    return all_value_functions\n"
      ],
      "metadata": {
        "id": "djMTCvKc91i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 25 — Output generation: report microinsurance metrics (if active)\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 25: Output generation: report microinsurance metrics (if active)\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class InsuranceImpactResult:\n",
        "    \"\"\"\n",
        "    Immutable container for the results of the microinsurance impact analysis.\n",
        "\n",
        "    This object provides a comprehensive summary of the effect of microinsurance\n",
        "    by comparing an insured scenario to its uninsured counterfactual.\n",
        "\n",
        "    Attributes:\n",
        "        insured_run_result: The complete `EndToEndResult` for the insured scenario.\n",
        "        uninsured_run_result: The complete `EndToEndResult` for the uninsured scenario.\n",
        "        premium_p_R: The computed insurance premium.\n",
        "        transformed_poverty_line_x_star_R: The adjusted poverty line under insurance.\n",
        "        transformed_growth_rate_r_R: The adjusted growth rate under insurance.\n",
        "        mean_post_insurance_loss_E_W: The mean of the post-insurance shock dist.\n",
        "        optimal_threshold_shift: The absolute change in the optimal threshold\n",
        "                                 (y*_insured - y*_uninsured).\n",
        "        cost_reduction_on_grid: A NumPy array representing the cost reduction\n",
        "                                (V_uninsured(x) - V_insured(x)) on the grid.\n",
        "    \"\"\"\n",
        "    # The complete `EndToEndResult` for the insured scenario.\n",
        "    insured_run_result: 'EndToEndResult'\n",
        "\n",
        "    # The complete `EndToEndResult` for the uninsured scenario.\n",
        "    uninsured_run_result: 'EndToEndResult'\n",
        "\n",
        "    # The computed insurance premium.\n",
        "    premium_p_R: float\n",
        "\n",
        "    # The adjusted poverty line under insurance.\n",
        "    transformed_poverty_line_x_star_R: float\n",
        "\n",
        "    # The adjusted growth rate under insurance.\n",
        "    transformed_growth_rate_r_R: float\n",
        "\n",
        "    # The mean of the post-insurance shock dist.\n",
        "    mean_post_insurance_loss_E_W: float\n",
        "\n",
        "    # The absolute change in the optimal threshold (y*_insured - y*_uninsured).\n",
        "    optimal_threshold_shift: float\n",
        "\n",
        "    # A NumPy array of the cost reduction on the grid.\n",
        "    cost_reduction_on_grid: np.ndarray\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 25, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def analyze_insurance_impact(\n",
        "    insured_config: Dict[str, Any]\n",
        ") -> InsuranceImpactResult:\n",
        "    \"\"\"\n",
        "    Performs a comparative analysis of an insured vs. uninsured scenario.\n",
        "\n",
        "    This function quantifies the impact of a microinsurance policy by running\n",
        "    the full end-to-end pipeline twice: once with the provided insured\n",
        "    configuration, and once with a programmatically generated uninsured\n",
        "    counterfactual. It then computes and returns key comparative metrics.\n",
        "\n",
        "    Process:\n",
        "    1.  **Validate Input:** Ensures the provided configuration has insurance active.\n",
        "    2.  **Run Insured Scenario:** Executes `run_end_to_end_analysis` on the\n",
        "        provided `insured_config`.\n",
        "    3.  **Create & Run Uninsured Scenario:** Creates a deep copy of the config,\n",
        "        sets `is_active` to `False`, and runs the pipeline again.\n",
        "    4.  **Extract Metrics (Step 1):** Extracts the key transformed parameters\n",
        "        (p_R, x*^R, r^R, E[W]) from the insured run's provenance.\n",
        "    5.  **Compute Cost Reduction (Step 2):** Calculates the point-wise difference\n",
        "        between the optimal value functions of the two scenarios.\n",
        "    6.  **Compute Threshold Shift (Step 3):** Calculates the difference between\n",
        "        the optimal thresholds (y*) found in each scenario.\n",
        "    7.  **Aggregate Results:** Packages all metrics into a comprehensive\n",
        "        `InsuranceImpactResult` object.\n",
        "\n",
        "    Args:\n",
        "        insured_config: The raw `full_study_configuration` dictionary for the\n",
        "                        scenario with microinsurance active.\n",
        "\n",
        "    Returns:\n",
        "        An `InsuranceImpactResult` object containing the full comparison.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the provided configuration does not have insurance active.\n",
        "    \"\"\"\n",
        "    # --- Step 0: Input Validation ---\n",
        "    # This analysis is only meaningful if the base scenario has insurance.\n",
        "    if not insured_config.get(\"model_parameters\", {}).get(\"microinsurance_parameters\", {}).get(\"is_active\", False):\n",
        "        raise ValueError(\n",
        "            \"The provided configuration must have microinsurance active to run \"\n",
        "            \"an impact analysis.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2 (part 1): Run the full pipeline for the insured scenario ---\n",
        "    print(\"--- Running analysis for INSURED scenario ---\")\n",
        "    insured_result = run_end_to_end_analysis(insured_config)\n",
        "    print(\"--- Insured scenario analysis complete. ---\")\n",
        "\n",
        "    # --- Step 2 (part 2): Create and run the uninsured counterfactual ---\n",
        "    print(\"\\n--- Running analysis for UNINSURED counterfactual scenario ---\")\n",
        "    # Create a deep copy to avoid modifying the original input config.\n",
        "    uninsured_config = copy.deepcopy(insured_config)\n",
        "    # Programmatically disable insurance in the copied configuration.\n",
        "    _update_nested_dict(\n",
        "        uninsured_config,\n",
        "        \"model_parameters.microinsurance_parameters.is_active\",\n",
        "        False\n",
        "    )\n",
        "    uninsured_result = run_end_to_end_analysis(uninsured_config)\n",
        "    print(\"--- Uninsured scenario analysis complete. ---\")\n",
        "\n",
        "    # --- Step 1: Extract transformed parameters from the insured run ---\n",
        "    insurance_transforms = insured_result.provenance.insurance_transforms\n",
        "    if insurance_transforms is None:\n",
        "        # This should not happen due to the initial validation, but it's a robust safeguard.\n",
        "        raise RuntimeError(\"Insurance transforms were not computed for the insured scenario.\")\n",
        "\n",
        "    # --- Step 3: Report optimal threshold shift ---\n",
        "    y_star_insured = insured_result.provenance.optimization_result.optimal_threshold_y_star\n",
        "    y_star_uninsured = uninsured_result.provenance.optimization_result.optimal_threshold_y_star\n",
        "    threshold_shift = y_star_insured - y_star_uninsured\n",
        "\n",
        "    # --- Step 2 (part 3): Compute cost reduction over the grid ---\n",
        "    v_insured = insured_result.all_value_functions['V_y_star_x_optimal_threshold']\n",
        "    v_uninsured = uninsured_result.all_value_functions['V_y_star_x_optimal_threshold']\n",
        "\n",
        "    # Ensure the grids are aligned before taking the difference.\n",
        "    if not np.array_equal(v_insured.capital_grid, v_uninsured.capital_grid):\n",
        "        raise RuntimeError(\"Grid alignment error during comparative analysis.\")\n",
        "\n",
        "    # Cost Reduction ΔV(x) = V_uninsured(x) - V_insured(x)\n",
        "    cost_reduction = v_uninsured.point_estimates - v_insured.point_estimates\n",
        "\n",
        "    # --- Final Assembly: Package all results into the output object ---\n",
        "    return InsuranceImpactResult(\n",
        "        insured_run_result=insured_result,\n",
        "        uninsured_run_result=uninsured_result,\n",
        "        premium_p_R=insurance_transforms.premium_p_R,\n",
        "        transformed_poverty_line_x_star_R=insurance_transforms.transformed_poverty_line_x_star_R,\n",
        "        transformed_growth_rate_r_R=insurance_transforms.transformed_growth_rate_r_R,\n",
        "        mean_post_insurance_loss_E_W=insurance_transforms.mean_post_insurance_loss_E_W,\n",
        "        optimal_threshold_shift=threshold_shift,\n",
        "        cost_reduction_on_grid=cost_reduction\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OE4cJKMLDMw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 26 — Visualization: generate plots\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 26: Visualization: generate plots\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 1: Plot value and comparator functions\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_value_functions(\n",
        "    end_to_end_result: 'EndToEndResult'\n",
        ") -> Figure:\n",
        "    \"\"\"\n",
        "    Generates a comprehensive plot of all computed value functions.\n",
        "\n",
        "    This function visualizes the primary outputs of the analysis, plotting the\n",
        "    optimal value function (V_{y*}) against key baseline comparators (C(x), D(x))\n",
        "    on a single set of axes for easy comparison.\n",
        "\n",
        "    Args:\n",
        "        end_to_end_result: The complete result object from the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        A `matplotlib.figure.Figure` object containing the generated plot.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    # Extract the necessary data from the result object.\n",
        "    results_dict = end_to_end_result.all_value_functions\n",
        "    y_star = end_to_end_result.provenance.optimization_result.optimal_threshold_y_star\n",
        "\n",
        "    # Create a figure and axes for the plot.\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Define a style mapping for consistent and professional plotting.\n",
        "    STYLE_MAP = {\n",
        "        \"V_y_star_x_optimal_threshold\": {\"color\": \"red\", \"linestyle\": \"-\", \"label\": \"$V_{y^*}(x)$ (Optimal Policy)\", \"zorder\": 10},\n",
        "        \"C_x_inject_to_poverty_line\": {\"color\": \"blue\", \"linestyle\": \"--\", \"label\": \"$C(x)$ (Inject-to-Poverty-Line)\", \"zorder\": 8},\n",
        "        \"D_x_perpetual_transfers\": {\"color\": \"green\", \"linestyle\": \":\", \"label\": \"$D(x)$ (Perpetual Transfers)\", \"zorder\": 7},\n",
        "        \"V_y_x_at_y_equals_40\": {\"color\": \"purple\", \"linestyle\": \"-.\", \"label\": \"$V_{y=40}(x)$ (Fixed Threshold)\", \"zorder\": 6},\n",
        "    }\n",
        "\n",
        "    # --- Plot each series requested in the configuration ---\n",
        "    for series_name, result in results_dict.items():\n",
        "        style = STYLE_MAP.get(series_name, {})\n",
        "        if not style:\n",
        "            continue # Skip series not in the style map\n",
        "\n",
        "        # Plot the main line for the point estimates.\n",
        "        ax.plot(\n",
        "            result.capital_grid,\n",
        "            result.point_estimates,\n",
        "            color=style[\"color\"],\n",
        "            linestyle=style[\"linestyle\"],\n",
        "            label=style[\"label\"],\n",
        "            linewidth=2,\n",
        "            zorder=style[\"zorder\"]\n",
        "        )\n",
        "\n",
        "        # If confidence intervals are available, plot the shaded region.\n",
        "        if result.ci_lower is not None and result.ci_upper is not None:\n",
        "            ax.fill_between(\n",
        "                result.capital_grid,\n",
        "                result.ci_lower,\n",
        "                result.ci_upper,\n",
        "                color=style[\"color\"],\n",
        "                alpha=0.2,\n",
        "                zorder=style[\"zorder\"] - 1\n",
        "            )\n",
        "\n",
        "    # --- Add annotations and formatting ---\n",
        "    # Add a vertical line to mark the optimal threshold y*.\n",
        "    ax.axvline(\n",
        "        x=y_star,\n",
        "        color='black',\n",
        "        linestyle='--',\n",
        "        linewidth=1.5,\n",
        "        label=f'Optimal Threshold $y^* = {y_star:.2f}$'\n",
        "    )\n",
        "\n",
        "    # Add a vertical line for the active poverty line.\n",
        "    active_x_star = end_to_end_result.provenance.active_params.active_poverty_line\n",
        "    ax.axvline(\n",
        "        x=active_x_star,\n",
        "        color='grey',\n",
        "        linestyle=':',\n",
        "        linewidth=1.5,\n",
        "        label=f'Poverty Line $\\\\bar{{x}}^* = {active_x_star:.2f}$'\n",
        "    )\n",
        "\n",
        "    # Set professional labels, title, and grid.\n",
        "    ax.set_title('Optimal Cost vs. Baseline Policies', fontsize=18)\n",
        "    ax.set_xlabel('Initial Capital (x)', fontsize=14)\n",
        "    ax.set_ylabel('Expected Discounted Cost', fontsize=14)\n",
        "    ax.legend(fontsize=12)\n",
        "    ax.grid(True, which='both', linestyle='--', alpha=0.6)\n",
        "    ax.set_xlim(left=0)\n",
        "    ax.set_ylim(bottom=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 2: Plot microinsurance sensitivity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def plot_sensitivity_sweep(\n",
        "    sweep_results: Dict[Tuple[Any, ...], 'EndToEndResult'],\n",
        "    swept_param_name: str,\n",
        "    swept_param_path: str\n",
        ") -> Figure:\n",
        "    \"\"\"\n",
        "    Generates a sensitivity analysis plot from a parameter sweep.\n",
        "\n",
        "    This function visualizes how the optimal value function V_{y*}(x) changes\n",
        "    as a single model parameter is varied.\n",
        "\n",
        "    Args:\n",
        "        sweep_results: The dictionary of results from `run_parameter_sweep`.\n",
        "        swept_param_name: The display name of the parameter (e.g., \"η\").\n",
        "        swept_param_path: The dot-separated path to the parameter in the config.\n",
        "\n",
        "    Returns:\n",
        "        A `matplotlib.figure.Figure` object containing the sensitivity plot.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Get the list of unique parameter values from the sweep results keys.\n",
        "    param_values = sorted([key[0] for key in sweep_results.keys()])\n",
        "\n",
        "    # Create a colormap to assign a unique color to each parameter value.\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(param_values)))\n",
        "\n",
        "    # --- Plot the results for each parameter value ---\n",
        "    for i, param_val in enumerate(param_values):\n",
        "        result = sweep_results[(param_val,)]\n",
        "        v_star_result = result.all_value_functions['V_y_star_x_optimal_threshold']\n",
        "        y_star = result.provenance.optimization_result.optimal_threshold_y_star\n",
        "\n",
        "        # Plot the optimal value function for this parameter value.\n",
        "        ax.plot(\n",
        "            v_star_result.capital_grid,\n",
        "            v_star_result.point_estimates,\n",
        "            color=colors[i],\n",
        "            label=f'{swept_param_name} = {param_val:.2f} (y*={y_star:.2f})'\n",
        "        )\n",
        "\n",
        "        # Add a vertical line for the corresponding optimal threshold.\n",
        "        ax.axvline(x=y_star, color=colors[i], linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Add formatting ---\n",
        "    ax.set_title(f'Sensitivity of Optimal Cost $V_{{y^*}}(x)$ to {swept_param_name}', fontsize=18)\n",
        "    ax.set_xlabel('Initial Capital (x)', fontsize=14)\n",
        "    ax.set_ylabel('Expected Discounted Cost', fontsize=14)\n",
        "    ax.legend(title=f'Parameter Values ({swept_param_name})', fontsize=10)\n",
        "    ax.grid(True, which='both', linestyle='--', alpha=0.6)\n",
        "    ax.set_xlim(left=0)\n",
        "    ax.set_ylim(bottom=0)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 26, Step 3: Plot boundary diagnostic (Re-use of Task 22)\n",
        "# ------------------------------------------------------------------------------\n",
        "# The function `analyze_and_plot_policy_boundary` from Task 22 already\n",
        "# performs this step completely and correctly. It should be called directly.\n",
        "# No new implementation is needed.\n"
      ],
      "metadata": {
        "id": "Qeq-ycCqJlyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 27: Implementation of Audit Trail and Manifest Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 27: Implementation of Audit Trail and Manifest Generation\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Step 1 & 2: Helper for recursive serialization\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _to_serializable_dict(obj: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Recursively converts a nested object to a JSON-serializable representation.\n",
        "\n",
        "    This robust utility handles the conversion of custom dataclasses, NumPy\n",
        "    arrays, and other non-standard JSON types into a clean, serializable\n",
        "    dictionary and list structure.\n",
        "\n",
        "    Args:\n",
        "        obj: The object to convert.\n",
        "\n",
        "    Returns:\n",
        "        A JSON-serializable representation of the object.\n",
        "    \"\"\"\n",
        "    # If the object is a dataclass, convert it to a dict and recurse on its values.\n",
        "    if dataclasses.is_dataclass(obj):\n",
        "        return {f.name: _to_serializable_dict(getattr(obj, f.name)) for f in dataclasses.fields(obj)}\n",
        "    # If the object is a dictionary, recurse on its values.\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: _to_serializable_dict(v) for k, v in obj.items()}\n",
        "    # If the object is a list or tuple, recurse on its elements.\n",
        "    elif isinstance(obj, (list, tuple)):\n",
        "        return [_to_serializable_dict(i) for i in obj]\n",
        "    # Convert NumPy arrays to nested Python lists.\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    # Convert NumPy numeric types (like np.float64) to standard Python types.\n",
        "    elif isinstance(obj, np.number):\n",
        "        return obj.item()\n",
        "    # Convert SciPy's OptimizeResult to a clean dictionary.\n",
        "    elif isinstance(obj, OptimizeResult):\n",
        "        # Only include key attributes for a clean manifest.\n",
        "        return {\n",
        "            \"success\": obj.success,\n",
        "            \"message\": obj.message,\n",
        "            \"optimal_x\": _to_serializable_dict(obj.x),\n",
        "            \"optimal_fun\": _to_serializable_dict(obj.fun),\n",
        "            \"num_evaluations\": obj.nfev,\n",
        "        }\n",
        "    # For basic types that are already JSON-serializable, return them directly.\n",
        "    elif isinstance(obj, (int, float, str, bool)) or obj is None:\n",
        "        return obj\n",
        "    # For any other unhandled type, return its string representation as a fallback.\n",
        "    else:\n",
        "        return f\"<Unserializable type: {type(obj).__name__}>\"\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 27, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def generate_run_manifest(\n",
        "    end_to_end_result: 'EndToEndResult',\n",
        "    raw_config: Dict[str, Any]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Assembles a complete, verifiable audit trail (run manifest) in JSON format.\n",
        "\n",
        "    This function creates a comprehensive, self-contained, and human-readable\n",
        "    record of a single pipeline execution. The manifest is designed for maximum\n",
        "    reproducibility, including environment metadata, an input integrity hash,\n",
        "    all derived parameters, diagnostics, and final results.\n",
        "\n",
        "    Process:\n",
        "    1.  **Gather Metadata:** Collects execution metadata like timestamp and library versions.\n",
        "    2.  **Create Integrity Hash:** Calculates a SHA-256 hash of the canonical\n",
        "        (sorted, compact) JSON representation of the raw input configuration.\n",
        "        This provides a verifiable fingerprint of the inputs.\n",
        "    3.  **Map to Figures:** Adds a descriptive string that heuristically maps\n",
        "        the run's scenario ID to a specific figure in the source paper.\n",
        "    4.  **Serialize All Data:** Recursively converts the entire `EndToEndResult`\n",
        "        object, including all nested dataclasses and NumPy arrays, into a\n",
        "        JSON-serializable dictionary structure.\n",
        "    5.  **Assemble Manifest:** Combines all the above components into a single,\n",
        "        logically structured dictionary.\n",
        "    6.  **Format and Return:** Returns the final manifest as a pretty-printed\n",
        "        JSON string, suitable for saving to a file and manual inspection.\n",
        "\n",
        "    Args:\n",
        "        end_to_end_result: The complete result object from the main pipeline.\n",
        "        raw_config: The original raw configuration dictionary that initiated the run.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the complete run manifest in pretty-printed JSON format.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Assemble Execution Metadata ---\n",
        "    execution_metadata = {\n",
        "        \"timestamp_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"python_version\": sys.version,\n",
        "        \"numpy_version\": np.__version__,\n",
        "        \"scipy_version\": scipy.__version__,\n",
        "    }\n",
        "\n",
        "    # --- Step 2: Create Integrity Hash of Inputs ---\n",
        "    # Create a canonical JSON string: sorted keys, no whitespace. This ensures\n",
        "    # the hash is consistent regardless of dictionary key order.\n",
        "    canonical_input_str = json.dumps(raw_config, sort_keys=True, separators=(',', ':'))\n",
        "    # Calculate the SHA-256 hash to serve as a unique fingerprint for the inputs.\n",
        "    input_hash = hashlib.sha256(canonical_input_str.encode('utf-8')).hexdigest()\n",
        "\n",
        "    # --- Step 3: Map Outputs to Figures (Heuristic) ---\n",
        "    scenario_id = end_to_end_result.parsed_config.study_metadata.scenario_id.lower()\n",
        "    figure_mapping = \"No direct figure mapping identified.\"\n",
        "    if \"figure7a\" in scenario_id:\n",
        "        figure_mapping = \"This scenario's parameters correspond to Figure 7a in the paper (sensitivity to proportional insurance parameters).\"\n",
        "    elif \"figure2\" in scenario_id:\n",
        "        figure_mapping = \"This scenario's parameters correspond to Figure 2 in the paper (sensitivity of C(x) to δ or α).\"\n",
        "    # (Add more rules for other figures as needed)\n",
        "\n",
        "    # --- Step 4: Serialize All Data ---\n",
        "    # Convert the entire, complex EndToEndResult object into a clean, serializable dictionary.\n",
        "    serializable_result = _to_serializable_dict(end_to_end_result)\n",
        "\n",
        "    # --- Step 5: Assemble the Final Manifest ---\n",
        "    # The manifest is structured with clear top-level keys for clarity.\n",
        "    manifest = {\n",
        "        \"manifest_header\": {\n",
        "            \"manifest_version\": \"1.1\",\n",
        "            \"description\": \"Complete audit trail for a single run of the social protection cost optimization pipeline.\",\n",
        "            \"figure_mapping_heuristic\": figure_mapping,\n",
        "        },\n",
        "        \"execution_context\": execution_metadata,\n",
        "        \"input_integrity\": {\n",
        "            \"raw_input_configuration\": raw_config,\n",
        "            \"input_hash_sha256\": input_hash,\n",
        "        },\n",
        "        # The full results, including provenance and outputs, are nested here.\n",
        "        \"results\": serializable_result\n",
        "    }\n",
        "\n",
        "    # --- Step 6: Format and Return ---\n",
        "    # Serialize the final manifest dictionary to a pretty-printed JSON string.\n",
        "    # `indent=4` makes the output human-readable.\n",
        "    return json.dumps(manifest, indent=4)\n"
      ],
      "metadata": {
        "id": "RcUaDiAxPngi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 28 — Final validation: end-to-end sanity checks\n",
        "\n",
        "# ==============================================================================\n",
        "# Task 28: Final validation: end-to-end sanity checks\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 1: Helper to verify value function monotonicity\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _verify_monotonicity(\n",
        "    result: 'ValueFunctionResult',\n",
        "    function_name: str,\n",
        "    tolerance: float = 1e-9\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Verifies that a computed value function is non-increasing.\n",
        "\n",
        "    Args:\n",
        "        result: The `ValueFunctionResult` object to check.\n",
        "        function_name: The name of the function for error messages.\n",
        "        tolerance: A small positive tolerance to allow for floating-point noise.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the function is found to be systematically increasing.\n",
        "    \"\"\"\n",
        "    # Calculate the first differences of the point estimates.\n",
        "    diffs = np.diff(result.point_estimates)\n",
        "\n",
        "    # Check if all differences are less than or equal to the tolerance.\n",
        "    # A positive difference means the function is increasing at that point.\n",
        "    if not np.all(diffs <= tolerance):\n",
        "        # Find where the violations occur for a more detailed error message.\n",
        "        violation_indices = np.where(diffs > tolerance)[0]\n",
        "        num_violations = len(violation_indices)\n",
        "        max_violation = np.max(diffs[violation_indices])\n",
        "\n",
        "        raise AssertionError(\n",
        "            f\"Monotonicity check failed for '{function_name}'. The function \"\n",
        "            f\"was found to be increasing at {num_violations} grid points. \"\n",
        "            f\"The maximum positive difference was {max_violation:.2e}. \"\n",
        "            f\"First violation at index {violation_indices[0]}.\"\n",
        "        )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 2: Helper to verify boundary condition consistency\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _verify_boundary_consistency(\n",
        "    end_to_end_result: 'EndToEndResult'\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Verifies consistency with the theoretical boundary from Proposition 2.2.\n",
        "\n",
        "    Args:\n",
        "        end_to_end_result: The complete result object from the main pipeline.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the numerical results for C(x*) and D(x*) contradict\n",
        "                        the theoretical prediction.\n",
        "    \"\"\"\n",
        "    # Extract the theoretical prediction and the numerical results.\n",
        "    provenance = end_to_end_result.provenance\n",
        "    lump_sum_preferred_theory = provenance.policy_boundary.lump_sum_preferred\n",
        "\n",
        "    c_result = end_to_end_result.all_value_functions.get(\"C_x_inject_to_poverty_line\")\n",
        "    d_result = end_to_end_result.all_value_functions.get(\"D_x_perpetual_transfers\")\n",
        "\n",
        "    if c_result is None or d_result is None:\n",
        "        warnings.warn(\"Skipping boundary consistency check: C(x) or D(x) not computed.\", UserWarning)\n",
        "        return\n",
        "\n",
        "    # Interpolate the values of C and D at the precise active poverty line.\n",
        "    active_x_star = provenance.active_params.active_poverty_line\n",
        "    c_at_x_star = np.interp(active_x_star, c_result.capital_grid, c_result.point_estimates)\n",
        "    d_at_x_star = np.interp(active_x_star, d_result.capital_grid, d_result.point_estimates)\n",
        "\n",
        "    # Check if the numerical result matches the theoretical prediction.\n",
        "    if lump_sum_preferred_theory:\n",
        "        # Theory predicts C(x*) <= D(x*).\n",
        "        if not (c_at_x_star <= d_at_x_star + 1e-9): # Add tolerance\n",
        "            raise AssertionError(\n",
        "                \"Boundary consistency check failed. Theory predicted C(x*) <= D(x*), \"\n",
        "                f\"but numerically C({active_x_star:.2f}) = {c_at_x_star:.4f} and \"\n",
        "                f\"D({active_x_star:.2f}) = {d_at_x_star:.4f}.\"\n",
        "            )\n",
        "    else:\n",
        "        # Theory predicts C(x*) > D(x*).\n",
        "        if not (c_at_x_star > d_at_x_star):\n",
        "            raise AssertionError(\n",
        "                \"Boundary consistency check failed. Theory predicted C(x*) > D(x*), \"\n",
        "                f\"but numerically C({active_x_star:.2f}) = {c_at_x_star:.4f} and \"\n",
        "                f\"D({active_x_star:.2f}) = {d_at_x_star:.4f}.\"\n",
        "            )\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Step 3: Helper to confirm reproducibility\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def _confirm_reproducibility(\n",
        "    base_config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Confirms that the pipeline is deterministic for a given seed.\n",
        "\n",
        "    Args:\n",
        "        base_config: The configuration dictionary to use for the runs.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the results of two identical runs are not the same.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Reproducibility Check ---\")\n",
        "    # Run the pipeline the first time.\n",
        "    print(\"Running first pass...\")\n",
        "    result1 = run_end_to_end_analysis(base_config)\n",
        "\n",
        "    # Run the pipeline a second time with the exact same configuration.\n",
        "    print(\"Running second pass...\")\n",
        "    result2 = run_end_to_end_analysis(base_config)\n",
        "\n",
        "    # Convert results to serializable dictionaries for comparison.\n",
        "    dict1 = _to_serializable_dict(result1)\n",
        "    dict2 = _to_serializable_dict(result2)\n",
        "\n",
        "    # Compare the canonical JSON strings of the two results.\n",
        "    # This is a robust way to check for deep equality.\n",
        "    json1 = json.dumps(dict1, sort_keys=True)\n",
        "    json2 = json.dumps(dict2, sort_keys=True)\n",
        "\n",
        "    if json1 != json2:\n",
        "        raise AssertionError(\n",
        "            \"Reproducibility check FAILED. Two runs with the identical \"\n",
        "            \"configuration and seed produced different results. This indicates \"\n",
        "            \"an uncontrolled source of randomness in the pipeline.\"\n",
        "        )\n",
        "    print(\"--- Reproducibility Check PASSED ---\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Task 28, Orchestrator Function\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "def run_final_validation_checks(\n",
        "    end_to_end_result: 'EndToEndResult',\n",
        "    base_config_for_repro_check: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates a suite of final, end-to-end sanity checks on the results.\n",
        "\n",
        "    This function serves as the final quality gate, verifying that the computed\n",
        "    results adhere to fundamental theoretical properties of the model and that\n",
        "    the pipeline itself is robust and reproducible.\n",
        "\n",
        "    Process:\n",
        "    1.  **Verify Monotonicity:** Checks that all computed value functions are\n",
        "        non-increasing with respect to initial capital.\n",
        "    2.  **Verify Boundary Consistency:** Checks if the numerical comparison of\n",
        "        C(x*) and D(x*) matches the theoretical prediction from Proposition 2.2.\n",
        "    3.  **Confirm Reproducibility:** Runs the entire pipeline twice with an\n",
        "        identical configuration to ensure the results are deterministic.\n",
        "\n",
        "    Args:\n",
        "        end_to_end_result: The complete result object from a pipeline run.\n",
        "        base_config_for_repro_check: The raw configuration dictionary, which\n",
        "                                     will be used to perform the reproducibility check.\n",
        "\n",
        "    Returns:\n",
        "        None. The function returns successfully if all checks pass.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any of the critical sanity checks fail.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Final Validation Checks ---\")\n",
        "\n",
        "    # --- Step 1: Verify monotonicity of all computed value functions ---\n",
        "    print(\"Checking monotonicity of value functions...\")\n",
        "    for name, result in end_to_end_result.all_value_functions.items():\n",
        "        _verify_monotonicity(result, name)\n",
        "    print(\"Monotonicity check PASSED.\")\n",
        "\n",
        "    # --- Step 2: Verify lump-sum dominance boundary consistency ---\n",
        "    print(\"Checking policy boundary consistency...\")\n",
        "    _verify_boundary_consistency(end_to_end_result)\n",
        "    print(\"Boundary consistency check PASSED.\")\n",
        "\n",
        "    # --- Step 3: Confirm reproducibility across runs ---\n",
        "    # This is a computationally expensive check and should be used in testing.\n",
        "    # A deepcopy is essential to ensure the config isn't mutated.\n",
        "    _confirm_reproducibility(copy.deepcopy(base_config_for_repro_check))\n",
        "\n",
        "    print(\"\\n--- All Final Validation Checks Passed Successfully ---\")\n"
      ],
      "metadata": {
        "id": "n1o0s3DCQTJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Master Orchestrator for the Complete Study\n",
        "# ==============================================================================\n",
        "\n",
        "def run_complete_study(\n",
        "    full_study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire, multi-stage research and analysis pipeline.\n",
        "\n",
        "    This master orchestrator is the single, definitive entry point for the entire\n",
        "    project. It is fully configuration-driven, composing all major sub-orchestrators\n",
        "    to perform a complete analysis, from initial data validation to core computation,\n",
        "    robustness checks, visualization, and final manifest generation.\n",
        "\n",
        "    The execution flow is controlled by an `analysis_stages` block within the\n",
        "    input configuration, allowing for flexible and reproducible execution of\n",
        "    different analytical components.\n",
        "\n",
        "    Args:\n",
        "        full_study_configuration: The raw, nested dictionary containing the\n",
        "                                  complete study configuration, including an\n",
        "                                  `analysis_stages` block to control the flow.\n",
        "\n",
        "    Returns:\n",
        "        A comprehensive dictionary containing all major artifacts produced by the\n",
        "        pipeline, including the main analysis result, robustness check results,\n",
        "        plots, and the final JSON manifest. If an error occurs, the dictionary\n",
        "        will contain an 'error' key with the exception details.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to hold all artifacts from the complete study.\n",
        "    master_results: Dict[str, Any] = {}\n",
        "\n",
        "    # Extract the run control configuration from the input dictionary.\n",
        "    # If not present, default to an empty dictionary, which will result in\n",
        "    # default behavior (e.g., running the main analysis).\n",
        "    run_control = full_study_configuration.get(\"analysis_stages\", {})\n",
        "\n",
        "    # Print a header to mark the beginning of the pipeline execution.\n",
        "    print(\"==============================================================================\")\n",
        "    print(\"= STARTING COMPLETE STUDY PIPELINE                                         =\")\n",
        "    print(f\"= Run controlled by `analysis_stages` configuration.                       =\")\n",
        "    print(\"==============================================================================\")\n",
        "\n",
        "    try:\n",
        "        # --- Stage 1: Main End-to-End Analysis (Task 20) ---\n",
        "        # This is the core computation and is typically a prerequisite for other stages.\n",
        "        # It is executed by default unless explicitly disabled.\n",
        "        if run_control.get(\"run_main_analysis\", True):\n",
        "            # Announce the start of the stage.\n",
        "            print(\"\\n[STAGE 1/5] EXECUTING MAIN ANALYSIS...\")\n",
        "            # Call the main analysis orchestrator.\n",
        "            main_analysis_result = run_end_to_end_analysis(full_study_configuration)\n",
        "            # Store the comprehensive result object.\n",
        "            master_results[\"main_analysis_result\"] = main_analysis_result\n",
        "            # Announce the completion of the stage.\n",
        "            print(\"[STAGE 1/5] MAIN ANALYSIS COMPLETE.\")\n",
        "        else:\n",
        "            # If the main analysis is skipped, log this and terminate gracefully.\n",
        "            print(\"\\n[STAGE 1/5] SKIPPING MAIN ANALYSIS as per configuration.\")\n",
        "            print(\"\\n==============================================================================\")\n",
        "            print(\"= COMPLETE STUDY PIPELINE FINISHED (Main analysis was skipped)             =\")\n",
        "            print(\"==============================================================================\")\n",
        "            return master_results\n",
        "\n",
        "        # --- Stage 2: Robustness & Sensitivity Analysis ---\n",
        "        print(\"\\n[STAGE 2/5] EXECUTING ROBUSTNESS AND SENSITIVITY ANALYSES...\")\n",
        "\n",
        "        # Conditionally run convergence diagnostics based on the config flag.\n",
        "        if run_control.get(\"run_convergence_diagnostics\", False):\n",
        "            print(\"\\n--- Running Convergence Diagnostics (Task 23) ---\")\n",
        "            convergence_diagnostics = run_convergence_diagnostics(full_study_configuration)\n",
        "            master_results[\"convergence_diagnostics_result\"] = convergence_diagnostics\n",
        "            print(\"--- Convergence Diagnostics Complete ---\")\n",
        "\n",
        "        # Conditionally run insurance impact analysis based on the config flag.\n",
        "        if run_control.get(\"run_insurance_impact_analysis\", False):\n",
        "            print(\"\\n--- Running Insurance Impact Analysis (Task 25) ---\")\n",
        "            insurance_impact_result = analyze_insurance_impact(full_study_configuration)\n",
        "            master_results[\"insurance_impact_analysis_result\"] = insurance_impact_result\n",
        "            print(\"--- Insurance Impact Analysis Complete ---\")\n",
        "\n",
        "        # Conditionally run parameter sweeps based on the config block.\n",
        "        if run_control.get(\"run_parameter_sweeps\"):\n",
        "            print(\"\\n--- Running Parameter Sweeps (Task 21) ---\")\n",
        "            all_sweep_results = {}\n",
        "            # Iterate through the list of sweep definitions in the config.\n",
        "            for sweep_config in run_control[\"run_parameter_sweeps\"]:\n",
        "                sweep_name = sweep_config.get(\"sweep_name\", \"unnamed_sweep\")\n",
        "                print(f\"  - Running sweep: {sweep_name}\")\n",
        "                sweep_params = sweep_config.get(\"sweep_params\", {})\n",
        "                if sweep_params:\n",
        "                    # Execute the sweep and store the result.\n",
        "                    sweep_result = run_parameter_sweep(full_study_configuration, sweep_params)\n",
        "                    all_sweep_results[sweep_name] = sweep_result\n",
        "            master_results[\"parameter_sweep_results\"] = all_sweep_results\n",
        "            print(\"--- Parameter Sweeps Complete ---\")\n",
        "\n",
        "        print(\"[STAGE 2/5] ROBUSTNESS AND SENSITIVITY ANALYSES COMPLETE.\")\n",
        "\n",
        "        # --- Stage 3: Visualization ---\n",
        "        print(\"\\n[STAGE 3/5] GENERATING PLOTS...\")\n",
        "        if run_control.get(\"generate_plots\", False):\n",
        "            plots = {}\n",
        "            print(\"\\n--- Generating Main Value Function Plot (Task 26.1) ---\")\n",
        "            # Generate the primary plot comparing all computed value functions.\n",
        "            plots[\"main_value_function_plot\"] = plot_value_functions(main_analysis_result)\n",
        "\n",
        "            print(\"--- Generating Policy Boundary Plot (Task 26.3) ---\")\n",
        "            # Generate the 2D diagnostic plot for policy preference.\n",
        "            plots[\"policy_boundary_plot\"] = analyze_and_plot_policy_boundary(\n",
        "                parsed_config=main_analysis_result.parsed_config,\n",
        "                base_quantities=main_analysis_result.provenance.base_quantities\n",
        "            )\n",
        "            master_results[\"plots\"] = plots\n",
        "            print(\"--- Plot Generation Complete ---\")\n",
        "        else:\n",
        "            print(\"--- Plot Generation Skipped ---\")\n",
        "        print(\"[STAGE 3/5] PLOT GENERATION COMPLETE.\")\n",
        "\n",
        "        # --- Stage 4: Final Validation ---\n",
        "        print(\"\\n[STAGE 4/5] RUNNING FINAL VALIDATION CHECKS...\")\n",
        "        if run_control.get(\"run_final_validation\", False):\n",
        "            # Execute the suite of end-to-end sanity checks.\n",
        "            run_final_validation_checks(\n",
        "                end_to_end_result=main_analysis_result,\n",
        "                base_config_for_repro_check=full_study_configuration\n",
        "            )\n",
        "        else:\n",
        "            print(\"--- Final Validation Skipped ---\")\n",
        "        print(\"[STAGE 4/5] FINAL VALIDATION COMPLETE.\")\n",
        "\n",
        "        # --- Stage 5: Manifest Generation ---\n",
        "        print(\"\\n[STAGE 5/5] GENERATING FINAL RUN MANIFEST...\")\n",
        "        # Generate the final audit trail by default unless explicitly disabled.\n",
        "        if run_control.get(\"generate_manifest\", True):\n",
        "            # Create the comprehensive JSON manifest for the run.\n",
        "            run_manifest_json = generate_run_manifest(\n",
        "                end_to_end_result=main_analysis_result,\n",
        "                raw_config=full_study_configuration\n",
        "            )\n",
        "            master_results[\"run_manifest_json\"] = run_manifest_json\n",
        "            print(\"--- Manifest Generation Complete ---\")\n",
        "        else:\n",
        "            print(\"--- Manifest Generation Skipped ---\")\n",
        "        print(\"[STAGE 5/5] FINAL ARTIFACT GENERATION COMPLETE.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # A global exception handler to ensure the pipeline fails gracefully.\n",
        "        print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(f\"!! PIPELINE EXECUTION FAILED: An unhandled error occurred.\")\n",
        "        print(f\"!! Error Type: {type(e).__name__}\")\n",
        "        print(f\"!! Error Details: {e}\")\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        # Store the error message in the results dictionary for inspection.\n",
        "        master_results[\"error\"] = str(e)\n",
        "        # Re-raise the exception to halt execution and provide a traceback.\n",
        "        raise\n",
        "\n",
        "    # Print a final success message.\n",
        "    print(\"\\n==============================================================================\")\n",
        "    print(\"= COMPLETE STUDY PIPELINE FINISHED SUCCESSFULLY                            =\")\n",
        "    print(\"==============================================================================\")\n",
        "\n",
        "    # Return the dictionary containing all generated artifacts.\n",
        "    return master_results\n"
      ],
      "metadata": {
        "id": "cXwBbHuq8mKm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}